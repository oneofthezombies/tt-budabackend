# git checkout 4359732fb
# pytest pybuda/test/model_demos/high_prio/cnn/pytorch/test_hardnet.py::test_hardnet_pytorch[Wormhole_B0-hardnet85]

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                                                   {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 392], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x20]]}

  # output
  pt_hardnet85.output_add_1230:                                                              {input: add_1230, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0x18e260]]}

  # parameter
  base.0.conv.weight:                                                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f6f3660], [4, 0x3f701000]]}
  base.0.conv.weight_fork_clone4676:                                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7499e0], [5, 0x3f761200]]}
  base.1.conv.weight_fork_clone3935:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [2, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f706aa0], [1, 0x7f715640], [2, 0x3f725540]]}
  base.1.conv.weight:                                                                        {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [2, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7afca0], [0, 0x2f6fc860], [0, 0x7f709ba0]]}
  base.1.conv.weight_fork_clone3933:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [2, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f750860], [4, 0x7f798f20], [5, 0x3f7b04e0]]}
  base.3.layers.0.conv.weight_fork_clone3954:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f72fdc0]]}
  base.3.layers.0.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f73f180]]}
  base.3.layers.0.conv.weight_fork_clone3952:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f746340]]}
  base.3.layers.1.conv.weight_fork_clone3962:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [12, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7cd680]]}
  base.3.layers.1.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [12, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7cce60]]}
  base.3.layers.1.conv.weight_fork_clone3960:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [12, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7c2be0]]}
  base.3.layers.2.conv.weight_fork_clone3970:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f709b80]]}
  base.3.layers.2.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f72f7e0]]}
  base.3.layers.2.conv.weight_fork_clone3968:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7acbc0]]}
  base.3.layers.3.conv.weight_fork_clone3978:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [15, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f85a2a0]]}
  base.3.layers.3.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [15, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f868e40]]}
  base.3.layers.3.conv.weight_fork_clone3976:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [15, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f86d760]]}
  base.3.layers.4.conv.weight_fork_clone3986:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [3, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f76d4a0], [5, 0x7f7700a0], [0, 0x2f6bcc60]]}
  base.3.layers.4.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [3, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f6c15e0], [0, 0x7f6ce920], [1, 0x3f6d19e0]]}
  base.3.layers.4.conv.weight_fork_clone3984:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [3, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f75a600], [5, 0x3f771e20], [5, 0x7f774a20]]}
  base.3.layers.5.conv.weight_fork_clone3994:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f791700]]}
  base.3.layers.5.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f805bc0]]}
  base.3.layers.5.conv.weight_fork_clone3992:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f78fec0]]}
  base.3.layers.6.conv.weight_fork_clone4002:                                                {input: HOST, type: ram, entries: 1, grid_size: [6, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f6b8680], [3, 0x7f6b9200], [4, 0x3f6c6ba0], [4, 0x7f70f580], [5, 0x3f726da0], [5, 0x7f729c60]]}
  base.3.layers.6.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [6, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f695cc0], [2, 0x3f6a5e80], [2, 0x7f6ad760], [3, 0x3f6b7e40], [3, 0x7f6b89c0], [4, 0x3f6c6360]]}
  base.3.layers.6.conv.weight_fork_clone4000:                                                {input: HOST, type: ram, entries: 1, grid_size: [6, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f726560], [5, 0x7f729420], [0, 0x2f675fe0], [0, 0x7f6834a0], [1, 0x3f686560], [1, 0x7f695480]]}
  base.3.layers.7.conv.weight_fork_clone4010:                                                {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 1], ublock: [12, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f793820], [1, 0x7f7a1380], [2, 0x3f7b0740], [2, 0x7f7b68c0]]}
  base.3.layers.7.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [24, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f88e7a0], [4, 0x3f89a280]]}
  base.3.layers.7.conv.weight_fork_clone4008:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [24, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f8df380], [5, 0x3f8f3880]]}
  base.4.conv.weight:                                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [7, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7968c0], [2, 0x3f7a5c80]]}
  base.6.layers.0.conv.weight_fork_clone4042:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f73d6a0], [4, 0x3f74a6a0], [4, 0x7f792d60]]}
  base.6.layers.0.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f74d780], [4, 0x7f795e40], [5, 0x3f7ad400]]}
  base.6.layers.0.conv.weight_fork_clone4040:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f70fd40], [1, 0x7f71e8e0], [2, 0x3f72e7e0]]}
  base.6.layers.1.conv.weight_fork_clone4050:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f844d40]]}
  base.6.layers.1.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f8538e0]]}
  base.6.layers.1.conv.weight_fork_clone4048:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f753f20], [2, 0x7f75b0e0], [3, 0x3f765360]]}
  base.6.layers.2.conv.weight_fork_clone4058:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f718720]]}
  base.6.layers.2.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f739a60]]}
  base.6.layers.2.conv.weight_fork_clone4056:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f740780]]}
  base.6.layers.3.conv.weight_fork_clone4066:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [24, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f999aa0]]}
  base.6.layers.3.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [24, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f98bf40]]}
  base.6.layers.3.conv.weight_fork_clone4064:                                                {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 3], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f81a100], [0, 0x2f765c80], [0, 0x7f771f80], [1, 0x3f775040]]}
  base.6.layers.4.conv.weight_fork_clone4074:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f70c4e0]]}
  base.6.layers.4.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f75d900]]}
  base.6.layers.4.conv.weight_fork_clone4072:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7bf920]]}
  base.6.layers.5.conv.weight_fork_clone4082:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7e53a0]]}
  base.6.layers.5.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f759920]]}
  base.6.layers.5.conv.weight_fork_clone4080:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7b06e0]]}
  base.6.layers.6.conv.weight_fork_clone4090:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7aa320]]}
  base.6.layers.6.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7475c0]]}
  base.6.layers.6.conv.weight_fork_clone4088:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f743860]]}
  base.6.layers.7.conv.weight_fork_clone4098:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 2], ublock: [11, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fa09e40], [5, 0x7fa09e40]]}
  base.6.layers.7.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 2], ublock: [11, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f9b0840], [4, 0x7f9f5940]]}
  base.6.layers.7.conv.weight_fork_clone4096:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [11, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f8779e0], [3, 0x7f878200], [4, 0x3f883ce0]]}
  base.6.layers.8.conv.weight_fork_clone4106:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f75f1c0]]}
  base.6.layers.8.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f74dd80]]}
  base.6.layers.8.conv.weight_fork_clone4104:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7ba860]]}
  base.6.layers.9.conv.weight_fork_clone4114:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f808e40]]}
  base.6.layers.9.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f8234c0]]}
  base.6.layers.9.conv.weight_fork_clone4112:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f832880]]}
  base.6.layers.10.conv.weight_fork_clone4122:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7359a0]]}
  base.6.layers.10.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f70cc80]]}
  base.6.layers.10.conv.weight_fork_clone4120:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7b2d80]]}
  base.6.layers.11.conv.weight_fork_clone4130:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f8d0020]]}
  base.6.layers.11.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f93eb60]]}
  base.6.layers.11.conv.weight_fork_clone4128:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f8d9a80]]}
  base.6.layers.12.conv.weight_fork_clone4138:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f72b480]]}
  base.6.layers.12.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f71c8e0]]}
  base.6.layers.12.conv.weight_fork_clone4136:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f719820]]}
  base.6.layers.13.conv.weight_fork_clone4146:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f786c60]]}
  base.6.layers.13.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7ee600]]}
  base.6.layers.13.conv.weight_fork_clone4144:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f79a960]]}
  base.6.layers.14.conv.weight_fork_clone4154:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f706ac0]]}
  base.6.layers.14.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f70cc60]]}
  base.6.layers.14.conv.weight_fork_clone4152:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f6ff940]]}
  base.6.layers.15.conv.weight_fork_clone4162:                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [14, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fa18220], [3, 0x3fa1fc00], [3, 0x7fa1fc00]]}
  base.6.layers.15.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [14, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fa51860], [3, 0x7fa51860], [4, 0x3fa579e0]]}
  base.6.layers.15.conv.weight_fork_clone4160:                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [14, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fa25d80], [4, 0x7fa67dc0], [5, 0x3fa7aa60]]}
  base.7.conv.weight:                                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [13, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f8a3d20], [2, 0x3f8b20a0]]}
  base.8.layers.0.conv.weight_fork_clone4183:                                                {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f71b800], [2, 0x3f72b700], [2, 0x7f7328c0], [3, 0x3f73cb40]]}
  base.8.layers.0.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7bca60]]}
  base.8.layers.0.conv.weight_fork_clone4181:                                                {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f78fc80], [5, 0x3f7a7240], [5, 0x7f7a9ae0], [0, 0x2f6f66a0]]}
  base.8.layers.1.conv.weight_fork_clone4191:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f92a660]]}
  base.8.layers.1.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f8969e0]]}
  base.8.layers.1.conv.weight_fork_clone4189:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [9, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f76bde0], [1, 0x7f77a980], [2, 0x3f789d40]]}
  base.8.layers.2.conv.weight_fork_clone4199:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f728620]]}
  base.8.layers.2.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7039e0]]}
  base.8.layers.2.conv.weight_fork_clone4197:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f73fc20]]}
  base.8.layers.3.conv.weight_fork_clone4207:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [11, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fa38a40]]}
  base.8.layers.3.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 3], ublock: [11, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fa30840]]}
  base.8.layers.3.conv.weight_fork_clone4205:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [11, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f85f3a0], [4, 0x7f8a54e0], [5, 0x3f8bb240]]}
  base.8.layers.4.conv.weight_fork_clone4215:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7bd080]]}
  base.8.layers.4.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7a5ac0]]}
  base.8.layers.4.conv.weight_fork_clone4213:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7505c0]]}
  base.8.layers.5.conv.weight_fork_clone4223:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f784460]]}
  base.8.layers.5.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f790760]]}
  base.8.layers.5.conv.weight_fork_clone4221:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7cde80]]}
  base.8.layers.6.conv.weight_fork_clone4231:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f73a5c0]]}
  base.8.layers.6.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f6f9780]]}
  base.8.layers.6.conv.weight_fork_clone4229:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f742d00]]}
  base.8.layers.7.conv.weight_fork_clone4239:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [13, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f98f800], [4, 0x7f9d4900], [5, 0x3f9e8e00]]}
  base.8.layers.7.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 5], t: 1, mblock: [1, 1], ublock: [39, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f859a60], [3, 0x3f863ce0], [3, 0x7f864500], [4, 0x3f86ffe0], [4, 0x7f8b6120]]}
  base.8.layers.7.conv.weight_fork_clone4237:                                                {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [13, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f97c340], [3, 0x3f985da0], [3, 0x7f985da0]]}
  base.8.layers.8.conv.weight_fork_clone4247:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7f6fe0]]}
  base.8.layers.8.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [3, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f6c9fa0], [1, 0x3f6cd060], [1, 0x7f6dbf40], [2, 0x3f6ebe40], [2, 0x7f6f3620]]}
  base.8.layers.8.conv.weight_fork_clone4245:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f742b60]]}
  base.8.layers.9.conv.weight_fork_clone4255:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f8cbe80]]}
  base.8.layers.9.conv.weight:                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f8cd6e0]]}
  base.8.layers.9.conv.weight_fork_clone4253:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f821c80]]}
  base.8.layers.10.conv.weight_fork_clone4263:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f756a20]]}
  base.8.layers.10.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f79f0e0]]}
  base.8.layers.10.conv.weight_fork_clone4261:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7b66a0]]}
  base.8.layers.11.conv.weight_fork_clone4271:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [21, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f934980]]}
  base.8.layers.11.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [21, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f9e9e40]]}
  base.8.layers.11.conv.weight_fork_clone4269:                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f82bda0], [5, 0x7f82de20], [0, 0x2f7799a0]]}
  base.8.layers.12.conv.weight_fork_clone4279:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f721220]]}
  base.8.layers.12.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f71e160]]}
  base.8.layers.12.conv.weight_fork_clone4277:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f750de0]]}
  base.8.layers.13.conv.weight_fork_clone4287:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7c0b40]]}
  base.8.layers.13.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7c1360]]}
  base.8.layers.13.conv.weight_fork_clone4285:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f82ce20]]}
  base.8.layers.14.conv.weight_fork_clone4295:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f705b00]]}
  base.8.layers.14.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f712e40]]}
  base.8.layers.14.conv.weight_fork_clone4293:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7b8f40]]}
  base.8.layers.15.conv.weight_fork_clone4303:                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [18, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7faeb6a0], [2, 0x3faf89e0], [2, 0x7faf89e0]]}
  base.8.layers.15.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [18, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fb44480], [5, 0x3fb57120], [5, 0x7fb57120]]}
  base.8.layers.15.conv.weight_fork_clone4301:                                               {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [18, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fb003c0], [3, 0x7fb003c0], [4, 0x3fb02440]]}
  base.9.conv.weight:                                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 5], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f896100], [5, 0x3f8abe60], [5, 0x7f8adee0], [0, 0x2f7f9a60], [0, 0x7f8034c0]]}
  base.11.layers.0.conv.weight_fork_clone4335:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f948e60]]}
  base.11.layers.0.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f948e60]]}
  base.11.layers.0.conv.weight_fork_clone4333:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f940460]]}
  base.11.layers.1.conv.weight_fork_clone4343:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f99d380]]}
  base.11.layers.1.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f9a6de0]]}
  base.11.layers.1.conv.weight_fork_clone4341:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [9, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f9a6de0]]}
  base.11.layers.2.conv.weight_fork_clone4351:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f76c500]]}
  base.11.layers.2.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7b46c0]]}
  base.11.layers.2.conv.weight_fork_clone4349:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f72fe20]]}
  base.11.layers.3.conv.weight_fork_clone4359:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [21, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fb8ae20]]}
  base.11.layers.3.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [21, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fb8ae20]]}
  base.11.layers.3.conv.weight_fork_clone4357:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [21, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fb7dae0]]}
  base.11.layers.4.conv.weight_fork_clone4367:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7ad6a0]]}
  base.11.layers.4.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f79fb40]]}
  base.11.layers.4.conv.weight_fork_clone4365:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f79ca80]]}
  base.11.layers.5.conv.weight_fork_clone4375:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f815960]]}
  base.11.layers.5.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f838a00]]}
  base.11.layers.5.conv.weight_fork_clone4373:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f84ffc0]]}
  base.11.layers.6.conv.weight_fork_clone4383:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f72cd60]]}
  base.11.layers.6.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f720240]]}
  base.11.layers.6.conv.weight_fork_clone4381:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7d46c0]]}
  base.11.layers.7.conv.weight_fork_clone4391:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 1], ublock: [17, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fb92800], [3, 0x7fb92800]]}
  base.11.layers.7.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 1], ublock: [17, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fb2c6c0], [1, 0x3fb2c6c0]]}
  base.11.layers.7.conv.weight_fork_clone4389:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [3, 1], ublock: [17, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fbe9560], [5, 0x7fbe9560]]}
  base.11.layers.8.conv.weight_fork_clone4399:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f824d40]]}
  base.11.layers.8.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f8328a0]]}
  base.11.layers.8.conv.weight_fork_clone4397:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f818220]]}
  base.11.layers.9.conv.weight_fork_clone4407:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f834120]]}
  base.11.layers.9.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f8de320]]}
  base.11.layers.9.conv.weight_fork_clone4405:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f8dfb80]]}
  base.11.layers.10.conv.weight_fork_clone4415:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7d1e20]]}
  base.11.layers.10.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f754f40]]}
  base.11.layers.10.conv.weight_fork_clone4413:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f73e9c0]]}
  base.11.layers.11.conv.weight_fork_clone4423:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [27, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fa7bae0]]}
  base.11.layers.11.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [27, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fa834c0]]}
  base.11.layers.11.conv.weight_fork_clone4421:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [27, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fa834c0]]}
  base.11.layers.12.conv.weight_fork_clone4431:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f842b80]]}
  base.11.layers.12.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f844c00]]}
  base.11.layers.12.conv.weight_fork_clone4429:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f790780]]}
  base.11.layers.13.conv.weight_fork_clone4439:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f8128a0]]}
  base.11.layers.13.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f842c80]]}
  base.11.layers.13.conv.weight_fork_clone4437:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f8434a0]]}
  base.11.layers.14.conv.weight_fork_clone4447:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7cbc80]]}
  base.11.layers.14.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7ce520]]}
  base.11.layers.14.conv.weight_fork_clone4445:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [6, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f75f9e0]]}
  base.11.layers.15.conv.weight_fork_clone4455:                                              {input: HOST, type: ram, entries: 1, grid_size: [6, 2], t: 1, mblock: [1, 1], ublock: [11, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f8e7e20], [2, 0x7f8eb700], [3, 0x3f8f5160], [3, 0x7f8f5160], [4, 0x3f900c40], [4, 0x7f945d40], [5, 0x3f95a240], [5, 0x7f95baa0], [0, 0x2f8a65e0], [0, 0x7f8b0040], [1, 0x3f8b20c0], [1, 0x7f8be3c0]]}
  base.11.layers.15.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [6, 2], t: 1, mblock: [1, 1], ublock: [11, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f903d20], [2, 0x7f907600], [3, 0x3f911060], [3, 0x7f911060], [4, 0x3f91cb40], [4, 0x7f961c40], [5, 0x3f976140], [5, 0x7f9779a0], [0, 0x2f8c24e0], [0, 0x7f8cbf40], [1, 0x3f8cdfc0], [1, 0x7f8da2c0]]}
  base.11.layers.15.conv.weight_fork_clone4453:                                              {input: HOST, type: ram, entries: 1, grid_size: [6, 2], t: 1, mblock: [1, 1], ublock: [11, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f91fc20], [2, 0x7f923500], [3, 0x3f92cf60], [3, 0x7f92cf60], [4, 0x3f938a40], [4, 0x7f97db40], [5, 0x3f992040], [5, 0x7f9938a0], [0, 0x2f8de3e0], [0, 0x7f8e7e40], [1, 0x3f8e9ec0], [1, 0x7f8f61c0]]}
  base.12.conv.weight:                                                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [19, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f9d69e0], [2, 0x3f9e4540], [2, 0x7f9e7e20]]}
  base.13.layers.0.conv.weight_fork_clone4476:                                               {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [9, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f783be0], [2, 0x3f792fa0], [2, 0x7f799120], [3, 0x3f7a33a0], [3, 0x7f7a3bc0]]}
  base.13.layers.0.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [9, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7f7860], [5, 0x3f80ee20], [5, 0x7f810ea0], [0, 0x2f75ca20], [0, 0x7f768d20]]}
  base.13.layers.0.conv.weight_fork_clone4474:                                               {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [9, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7537c0], [0, 0x7f75fac0], [1, 0x3f762b80], [1, 0x7f771720], [2, 0x3f780ae0]]}
  base.13.layers.1.conv.weight_fork_clone4484:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [17, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fb24ce0]]}
  base.13.layers.1.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [17, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fbd68c0]]}
  base.13.layers.1.conv.weight_fork_clone4482:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [17, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fb94880]]}
  base.13.layers.2.conv.weight_fork_clone4492:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7684c0]]}
  base.13.layers.2.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f74a560]]}
  base.13.layers.2.conv.weight_fork_clone4490:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7fe9e0]]}
  base.13.layers.3.conv.weight_fork_clone4500:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 5], t: 1, mblock: [15, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f91fc40], [1, 0x3f921cc0], [1, 0x7f92dfc0], [2, 0x3f93bb20], [2, 0x7f93f400]]}
  base.13.layers.3.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 5], t: 1, mblock: [15, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f971060], [4, 0x7f9b6160], [5, 0x3f9ca660], [5, 0x7f9cb6a0], [0, 0x2f9161e0]]}
  base.13.layers.3.conv.weight_fork_clone4498:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 5], t: 1, mblock: [15, 1], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f94c760], [2, 0x3f95a2c0], [2, 0x7f95dba0], [3, 0x3f967600], [3, 0x7f967600]]}
  base.13.layers.4.conv.weight_fork_clone4508:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7ea680]]}
  base.13.layers.4.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f89eb00]]}
  base.13.layers.4.conv.weight_fork_clone4506:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f829620]]}
  base.13.layers.5.conv.weight_fork_clone4516:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f8cc740]]}
  base.13.layers.5.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f8e5560]]}
  base.13.layers.5.conv.weight_fork_clone4514:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f8d9a80]]}
  base.13.layers.6.conv.weight_fork_clone4524:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7fc960]]}
  base.13.layers.6.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f79e220]]}
  base.13.layers.6.conv.weight_fork_clone4522:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f818080]]}
  base.13.layers.7.conv.weight_fork_clone4532:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [18, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fba0340], [0, 0x2fadbac0], [0, 0x7fae34a0], [1, 0x3fae34a0]]}
  base.13.layers.7.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [18, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fb348c0], [2, 0x3fb41c00], [2, 0x7fb41c00], [3, 0x3fb495e0]]}
  base.13.layers.7.conv.weight_fork_clone4530:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [18, 1], ublock: [4, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7fb495e0], [4, 0x3fb4b660], [4, 0x7fb8d6a0], [5, 0x3fba0340]]}
  base.13.layers.8.conv.weight_fork_clone4540:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f8f50e0]]}
  base.13.layers.8.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f83fc20]]}
  base.13.layers.8.conv.weight_fork_clone4538:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [4, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f849680]]}
  base.13.layers.9.conv.weight_fork_clone4548:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [27, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f99bb40]]}
  base.13.layers.9.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [27, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fa51820]]}
  base.13.layers.9.conv.weight_fork_clone4546:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [27, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fa51820]]}
  base.13.layers.10.conv.weight_fork_clone4556:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f79a140]]}
  base.13.layers.10.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7a7480]]}
  base.13.layers.10.conv.weight_fork_clone4554:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f807c40]]}
  base.13.layers.11.conv.weight_fork_clone4564:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [18, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fb72820]]}
  base.13.layers.11.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [18, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fc370a0]]}
  base.13.layers.11.conv.weight_fork_clone4562:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [18, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fc370a0]]}
  base.13.layers.12.conv.weight_fork_clone4572:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f8234a0]]}
  base.13.layers.12.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f8140e0]]}
  base.13.layers.12.conv.weight_fork_clone4570:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f806580]]}
  base.13.layers.13.conv.weight_fork_clone4580:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f894960]]}
  base.13.layers.13.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f88af00]]}
  base.13.layers.13.conv.weight_fork_clone4578:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [18, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f9403c0]]}
  base.13.layers.14.conv.weight_fork_clone4588:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f790ee0]]}
  base.13.layers.14.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f756860]]}
  base.13.layers.14.conv.weight_fork_clone4586:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [9, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f777880]]}
  base.13.layers.15.conv.weight_fork_clone4596:                                              {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [31, 13], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3fd98000], [3, 0x7fd98000], [4, 0x3fd98000]]}
  base.13.layers.15.conv.weight:                                                             {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [31, 13], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fd98000], [2, 0x3fd98000], [2, 0x7fd98000]]}
  base.13.layers.15.conv.weight_fork_clone4594:                                              {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [31, 13], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fd98000], [0, 0x7fd98000], [1, 0x3fd98000]]}
  base.14.conv.weight:                                                                       {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 23], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fa5b2e0], [0, 0x7fa62cc0], [1, 0x3fa62cc0], [1, 0x7fa6aec0], [2, 0x3fa78200]]}
  base.16.layers.0.conv.weight_fork_clone4628:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [3, 1], ublock: [23, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fab2880], [2, 0x7fab2880], [3, 0x3faba260], [3, 0x7faba260]]}
  base.16.layers.0.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [3, 1], ublock: [23, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fa95960], [0, 0x7fa9d340], [1, 0x3fa9d340], [1, 0x7faa5540]]}
  base.16.layers.0.conv.weight_fork_clone4626:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [3, 1], ublock: [23, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fabc2e0], [4, 0x7fafe320], [5, 0x3fb10fc0], [5, 0x7fb10fc0]]}
  base.16.layers.1.conv.weight_fork_clone4636:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 7], t: 1, mblock: [3, 1], ublock: [31, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fc31760], [1, 0x3fc31760], [1, 0x7fc31760], [2, 0x3fc3eaa0], [2, 0x7fc3eaa0], [3, 0x3fc3eaa0], [3, 0x7fc3eaa0]]}
  base.16.layers.1.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 7], t: 1, mblock: [3, 1], ublock: [31, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fc40b20], [4, 0x7fc82b60], [5, 0x3fc92740], [5, 0x7fc92740], [0, 0x2fbcdec0], [0, 0x7fbd3000], [1, 0x3fbd3000]]}
  base.16.layers.1.conv.weight_fork_clone4634:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 7], t: 1, mblock: [3, 1], ublock: [31, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fbd3000], [2, 0x3fbe0340], [2, 0x7fbe0340], [3, 0x3fbe0340], [3, 0x7fbe0340], [4, 0x3fbe23c0], [4, 0x7fc24400]]}
  base.16.layers.2.conv.weight_fork_clone4644:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [7, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fdba060]]}
  base.16.layers.2.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [7, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fdba060]]}
  base.16.layers.2.conv.weight_fork_clone4642:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [7, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fdba060]]}
  base.16.layers.3.conv.weight_fork_clone4652:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [15, 1], ublock: [9, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fe64a80], [2, 0x7fe64a80], [3, 0x3fe64a80], [3, 0x7fe64a80], [4, 0x3fe64a80], [4, 0x7fe64a80], [5, 0x3fe64a80], [5, 0x7fe64a80]]}
  base.16.layers.3.conv.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [15, 1], ublock: [9, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3ff32540], [4, 0x7ff32540], [5, 0x3ff32540], [5, 0x7ff32540], [0, 0x2fe64a80], [0, 0x7fe64a80], [1, 0x3fe64a80], [1, 0x7fe64a80]]}
  base.16.layers.3.conv.weight_fork_clone4650:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [15, 1], ublock: [9, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2ff32540], [0, 0x7ff32540], [1, 0x3ff32540], [1, 0x7ff32540], [2, 0x3ff32540], [2, 0x7ff32540], [3, 0x3ff32540], [3, 0x7ff32540]]}
  base.18.conv.weight:                                                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [4, 2], ublock: [5, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fc8fec0], [0, 0x7fc8fec0], [1, 0x3fc8fec0], [1, 0x7fc8fec0], [2, 0x3fc9d200], [2, 0x7fc9d200], [3, 0x3fc9d200], [3, 0x7fc9d200]]}
  base.19.3.weight:                                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [10, 1], ublock: [4, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fcf57e0], [0, 0x7fcf57e0], [1, 0x3fcf57e0], [1, 0x7fcf57e0]]}
  base.19.3.bias:                                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f8bd2c0]]}

  # constant
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6af040], [3, 0x3f6b9720], [3, 0x7f6ba2a0], [4, 0x3f6c7c40]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6ac8a0], [0, 0x7f6b9d40], [1, 0x3f6bce00], [1, 0x7f6cbd20]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f682da0], [1, 0x3f685e60], [1, 0x7f694d80], [2, 0x3f6a5080]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6ae920], [0, 0x7f6bbdc0], [1, 0x3f6bee80], [1, 0x7f6cdda0]]}
  input_1_add_1_fork_clone1886:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f683ce0], [1, 0x3f686da0]]}
  lc.input_tensor.conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6c0dc0], [4, 0x3f6ce760], [4, 0x7f717140], [5, 0x3f72e960]]}
  lc.input_tensor.conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f75a020], [5, 0x7f75cc20], [0, 0x2f6a97e0], [0, 0x7f6b6c80]]}
  lc.input_tensor.conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6d1520], [4, 0x7f719f00], [5, 0x3f731720], [5, 0x7f734320]]}
  lc.input_tensor.conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6e1380], [3, 0x3f6eb900], [3, 0x7f6ec480], [4, 0x3f6f9e20]]}
  lc.input_tensor.conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6b2ac0], [3, 0x3f6bd1a0], [3, 0x7f6bdd20], [4, 0x3f6cb6c0]]}
  lc.input_tensor.conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6cace0], [2, 0x3f6dabe0], [2, 0x7f6e23c0], [3, 0x3f6ec940]]}
  input_1_add_15_fork_clone1830:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f701700]]}
  lc.input_tensor.max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 39], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f749a20]]}
  lc.input_tensor.max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f77a520]]}
  lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f753040]]}
  lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6d9ba0]]}
  lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f76d460]]}
  lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f75e120]]}
  lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f7030c0]]}
  lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f760d20]]}
  input_1_add_30_fork_clone1718:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f676820]]}
  lc.input_tensor.concatenate_43.dc.sparse_matmul.4.0:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f70c020], [5, 0x3f7239a0], [5, 0x7f726860], [0, 0x2f673420]]}
  lc.input_tensor.concatenate_43.dc.sparse_matmul.4.1:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6dcc60]]}
  lc.input_tensor.conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f707ce0]]}
  lc.input_tensor.conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6be9e0]]}
  lc.input_tensor.conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6fa600]]}
  lc.input_tensor.conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f69f560]]}
  lc.input_tensor.conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6b2a80]]}
  lc.input_tensor.conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6f1c20]]}
  input_1_add_45_fork_clone1811:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f6d1f00]]}
  lc.input_tensor.conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6e7d40]]}
  lc.input_tensor.conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d8140]]}
  lc.input_tensor.conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6ba020]]}
  lc.input_tensor.conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6cb820]]}
  lc.input_tensor.conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f74baa0]]}
  lc.input_tensor.conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6e0180]]}
  input_1_add_59_fork_clone1723:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f70ed40]]}
  lc.input_tensor.concatenate_72.dc.sparse_matmul.5.0:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6a2ea0], [2, 0x7f6aa780], [3, 0x3f6b4e60], [3, 0x7f6b59e0], [4, 0x3f6c3640]]}
  lc.input_tensor.concatenate_72.dc.sparse_matmul.5.1:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6c2ae0]]}
  lc.input_tensor.conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f7506c0]]}
  lc.input_tensor.conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f73e700]]}
  lc.input_tensor.conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f766a20]]}
  lc.input_tensor.conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f758fe0]]}
  lc.input_tensor.conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f765ec0]]}
  lc.input_tensor.conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6e4280]]}
  input_1_add_74_fork_clone1817:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f684520], [1, 0x3f6875e0], [1, 0x7f696500]]}
  lc.input_tensor.conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6bf5c0], [3, 0x7f6c0140], [4, 0x3f6cdae0], [4, 0x7f7164c0], [5, 0x3f72dce0], [5, 0x7f730a40], [0, 0x2f67d600]]}
  lc.input_tensor.conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6f2c60], [4, 0x7f73b640], [5, 0x3f752e60], [5, 0x7f755a60], [0, 0x2f6a2620], [0, 0x7f6afac0], [1, 0x3f6b2b80]]}
  lc.input_tensor.conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f69ca80], [2, 0x3f6ac980], [2, 0x7f6b4260], [3, 0x3f6be940], [3, 0x7f6bf4c0], [4, 0x3f6cce60], [4, 0x7f715840]]}
  lc.input_tensor.conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6b5c40], [1, 0x7f6c4b60], [2, 0x3f6d4a60], [2, 0x7f6dc240], [3, 0x3f6e67c0], [3, 0x7f6e7340], [4, 0x3f6f4ce0]]}
  lc.input_tensor.conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6b2100], [3, 0x3f6bc7e0], [3, 0x7f6bd360], [4, 0x3f6cad00], [4, 0x7f7136e0], [5, 0x3f72af00], [5, 0x7f72dc60]]}
  lc.input_tensor.conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6c3b20], [2, 0x3f6d3a20], [2, 0x7f6db200], [3, 0x3f6e5780], [3, 0x7f6e6300], [4, 0x3f6f3ca0], [4, 0x7f73c680]]}
  input_1_add_88_fork_clone1728:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f6a66c0]]}
  lc.input_tensor.concatenate_101.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f724680], [5, 0x7f727540], [0, 0x2f674100]]}
  lc.input_tensor.concatenate_101.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6a3660], [0, 0x7f6b0b00], [1, 0x3f6b3bc0]]}
  lc.input_tensor.conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f769620], [0, 0x2f6b61e0]]}
  lc.input_tensor.conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6a56e0], [0, 0x7f6b2b80]]}
  lc.input_tensor.conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f706820], [4, 0x7f74f200]]}
  lc.input_tensor.conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6da1c0], [3, 0x3f6e4740]]}
  lc.input_tensor.conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6bff20], [1, 0x3f6c2fe0]]}
  lc.input_tensor.conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d9180], [3, 0x3f6e3700]]}
  input_1_add_103_fork_clone1823:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f762da0]]}
  lc.input_tensor.conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f67c980], [0, 0x7f689e20], [1, 0x3f68cee0], [1, 0x7f69be00], [2, 0x3f6abd00], [2, 0x7f6b35e0], [3, 0x3f6bdcc0]]}
  lc.input_tensor.conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6c7c20], [2, 0x3f6d7b20], [2, 0x7f6df300], [3, 0x3f6e9880], [3, 0x7f6ea400], [4, 0x3f6f7da0], [4, 0x7f740780]]}
  lc.input_tensor.conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6cc1e0], [4, 0x7f714bc0], [5, 0x3f72c3e0], [5, 0x7f72f140], [0, 0x2f67bd00], [0, 0x7f6891a0], [1, 0x3f68c260]]}
  lc.input_tensor.conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6a7760], [0, 0x7f6b4c00], [1, 0x3f6b7cc0], [1, 0x7f6c6be0], [2, 0x3f6d6ae0], [2, 0x7f6de2c0], [3, 0x3f6e8840]]}
  lc.input_tensor.conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f728800], [5, 0x7f72b560], [0, 0x2f678120], [0, 0x7f6855c0], [1, 0x3f688680], [1, 0x7f6975a0], [2, 0x3f6a7760]]}
  lc.input_tensor.conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f759b60], [0, 0x2f6a6720], [0, 0x7f6b3bc0], [1, 0x3f6b6c80], [1, 0x7f6c5ba0], [2, 0x3f6d5aa0], [2, 0x7f6dd280]]}
  input_1_add_117_fork_clone1733:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f6adfa0]]}
  lc.input_tensor.concatenate_130.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f673880], [0, 0x7f680d40], [1, 0x3f683e00], [1, 0x7f692d20], [2, 0x3f6a3020], [2, 0x7f6aa900], [3, 0x3f6b4fe0], [3, 0x7f6b5b60]]}
  lc.input_tensor.concatenate_130.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d60c0], [3, 0x3f6e0640], [3, 0x7f6e11c0], [4, 0x3f6eeb60], [4, 0x7f737540], [5, 0x3f74ed60], [5, 0x7f751960], [0, 0x2f69e520]]}
  lc.input_tensor.conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6ef520], [3, 0x3f6f9aa0]]}
  lc.input_tensor.conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6bd9a0], [2, 0x3f6cd8a0]]}
  lc.input_tensor.conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f76a860]]}
  lc.input_tensor.conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f750920]]}
  lc.input_tensor.conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7632c0]]}
  lc.input_tensor.conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6df140]]}
  input_1_add_132_fork_clone1738:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f70d0c0]]}
  lc.input_tensor.concatenate_145.dc.sparse_matmul.8.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f72fdc0]]}
  lc.input_tensor.concatenate_145.dc.sparse_matmul.8.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6e26c0]]}
  input_1_add_147_fork_clone1574:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f6f6720], [3, 0x3f700ba0]]}
  lc.input_tensor.max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                    {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 45], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f73b380], [2, 0x7f742540], [3, 0x3f74c7c0], [3, 0x7f74cfe0], [4, 0x3f759b00]]}
  lc.input_tensor.max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                    {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6e3240], [4, 0x3f6f0be0], [4, 0x7f7395c0], [5, 0x3f750de0], [5, 0x7f7539e0]]}
  lc.input_tensor.conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6e41c0], [2, 0x7f6eb9a0], [3, 0x3f6f5f20], [3, 0x7f6f6a80], [4, 0x3f704160]]}
  lc.input_tensor.conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6cf920], [2, 0x7f6d7100], [3, 0x3f6e1680], [3, 0x7f6e2200], [4, 0x3f6efba0]]}
  lc.input_tensor.conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6c3680], [1, 0x3f6c6740], [1, 0x7f6d5620], [2, 0x3f6e5520], [2, 0x7f6ecd00]]}
  lc.input_tensor.conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6ee9c0], [3, 0x7f6ef540], [4, 0x3f6fcee0], [4, 0x7f7458c0], [5, 0x3f75d0e0]]}
  lc.input_tensor.conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f718080], [5, 0x3f72f8a0], [5, 0x7f7324a0], [0, 0x2f67f060], [0, 0x7f68c500]]}
  lc.input_tensor.conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6e5480], [3, 0x3f6efa00], [3, 0x7f6f0580], [4, 0x3f6fdf20], [4, 0x7f746900]]}
  input_1_add_162:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7b99c0]]}
  input_1_add_162_fork_clone1348:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7abe60]]}
  lc.input_tensor.concatenate_175.dc.sparse_matmul.4.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6a3ce0]]}
  lc.input_tensor.concatenate_175.dc.sparse_matmul.4.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6ed980]]}
  lc.input_tensor.conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f700be0]]}
  lc.input_tensor.conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6ee500]]}
  lc.input_tensor.conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f712840]]}
  lc.input_tensor.conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6bad80]]}
  lc.input_tensor.conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7307e0], [5, 0x7f7333e0], [0, 0x2f67ffa0], [0, 0x7f68d440], [1, 0x3f690500]]}
  lc.input_tensor.conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6f15c0], [4, 0x3f6fef60], [4, 0x7f747940], [5, 0x3f75f160], [5, 0x7f761d60]]}
  input_1_add_177:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f926d60]]}
  input_1_add_177_fork_clone1531:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f925500]]}
  lc.input_tensor.conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f7154c0]]}
  lc.input_tensor.conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f73e4a0]]}
  lc.input_tensor.conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f702d00]]}
  lc.input_tensor.conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6da660]]}
  lc.input_tensor.conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f70a860]]}
  lc.input_tensor.conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6bfb40]]}
  input_1_add_191:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f850f20]]}
  input_1_add_191_fork_clone1353:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7e64c0]]}
  lc.input_tensor.concatenate_204.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6c5560]]}
  lc.input_tensor.concatenate_204.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f720fc0]]}
  lc.input_tensor.conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f7133a0]]}
  lc.input_tensor.conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f7387e0]]}
  lc.input_tensor.conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f7083c0]]}
  lc.input_tensor.conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6c9080]]}
  lc.input_tensor.conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6b7b40], [3, 0x3f6c20c0], [3, 0x7f6c2c40], [4, 0x3f6d05e0], [4, 0x7f718fc0]]}
  lc.input_tensor.conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f69d640], [1, 0x7f6ac560], [2, 0x3f6bc460], [2, 0x7f6c3c40], [3, 0x3f6ce1c0]]}
  input_1_add_206:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fa171a0]]}
  input_1_add_206_fork_clone1537:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fa2b6a0]]}
  lc.input_tensor.conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7828e0]]}
  lc.input_tensor.conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6cfd80]]}
  lc.input_tensor.conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f784a00]]}
  lc.input_tensor.conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6dd720]]}
  lc.input_tensor.conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6e8d00]]}
  lc.input_tensor.conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f740520]]}
  input_1_add_220:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7cef00]]}
  input_1_add_220_fork_clone1358:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7c8d80]]}
  lc.input_tensor.concatenate_233.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f70df40]]}
  lc.input_tensor.concatenate_233.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6de760]]}
  lc.input_tensor.conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f70e600]]}
  lc.input_tensor.conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6dc6e0]]}
  lc.input_tensor.conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f764da0]]}
  lc.input_tensor.conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6ca0c0]]}
  lc.input_tensor.conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f70b3c0]]}
  lc.input_tensor.conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6cd180]]}
  input_1_add_235:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f87d380]]}
  input_1_add_235_fork_clone1543:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f863d20]]}
  lc.input_tensor.conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f704180]]}
  lc.input_tensor.conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6c6b40]]}
  lc.input_tensor.conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6eebc0]]}
  lc.input_tensor.conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6d34a0]]}
  lc.input_tensor.conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f761000]]}
  lc.input_tensor.conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f7336a0]]}
  input_1_add_249:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f79caa0]]}
  input_1_add_249_fork_clone1363:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f86a5a0]]}
  lc.input_tensor.concatenate_262.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6c37c0], [4, 0x7f70c1a0], [5, 0x3f723b20], [5, 0x7f7269e0]]}
  lc.input_tensor.concatenate_262.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6c4ac0]]}
  lc.input_tensor.conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6ecaa0]]}
  lc.input_tensor.conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f735260]]}
  lc.input_tensor.conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6dae40]]}
  lc.input_tensor.conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6d2460]]}
  lc.input_tensor.conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6c7360], [1, 0x3f6ca420], [1, 0x7f6d9300], [2, 0x3f6e9200], [2, 0x7f6f09e0]]}
  lc.input_tensor.conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f681e20], [0, 0x7f68f2c0], [1, 0x3f692380], [1, 0x7f6a12a0], [2, 0x3f6b11a0]]}
  input_1_add_264:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7facb680]]}
  input_1_add_264_fork_clone1549:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fa89640]]}
  lc.input_tensor.conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6cfc20]]}
  lc.input_tensor.conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f732660]]}
  lc.input_tensor.conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f711280]]}
  lc.input_tensor.conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f71ef40]]}
  lc.input_tensor.conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f718940]]}
  lc.input_tensor.conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f739360]]}
  input_1_add_278:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7d99a0]]}
  input_1_add_278_fork_clone1368:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7d9180]]}
  lc.input_tensor.concatenate_291.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6b5fa0], [4, 0x3f6c3aa0], [4, 0x7f70c480], [5, 0x3f723e00], [5, 0x7f726cc0]]}
  lc.input_tensor.concatenate_291.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6c7000], [3, 0x7f6c7b80], [4, 0x3f6d5520], [4, 0x7f71df00], [5, 0x3f735720]]}
  lc.input_tensor.conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6dcf60]]}
  lc.input_tensor.conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6d6560]]}
  lc.input_tensor.conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6dbde0]]}
  lc.input_tensor.conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6d44e0]]}
  lc.input_tensor.conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f7003e0]]}
  lc.input_tensor.conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f7372e0]]}
  input_1_add_293:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f90d700]]}
  input_1_add_293_fork_clone1555:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f90bea0]]}
  lc.input_tensor.conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f77ee20]]}
  lc.input_tensor.conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f691340]]}
  lc.input_tensor.conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f70f160]]}
  lc.input_tensor.conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6baa00]]}
  lc.input_tensor.conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6d7020]]}
  lc.input_tensor.conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f7313c0]]}
  input_1_add_307:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f86be40]]}
  input_1_add_307_fork_clone1373:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f881ba0]]}
  lc.input_tensor.concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f710620]]}
  lc.input_tensor.concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.1:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6d9480]]}
  lc.input_tensor.concatenate_320.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f684d60]]}
  lc.input_tensor.concatenate_320.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f74a7a0]]}
  lc.input_tensor.conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6d3e60]]}
  lc.input_tensor.conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6a78c0]]}
  lc.input_tensor.conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f7872a0]]}
  lc.input_tensor.conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b67e0]]}
  lc.input_tensor.conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6f8c00]]}
  lc.input_tensor.conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6d8fc0]]}
  input_1_add_322:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f9cb700]]}
  input_1_add_322_fork_clone1561:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f9b0860]]}
  lc.input_tensor.conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6e2140]]}
  lc.input_tensor.conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f69b460]]}
  lc.input_tensor.conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6f0ce0]]}
  lc.input_tensor.conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6ab9c0]]}
  lc.input_tensor.conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6da0e0]]}
  lc.input_tensor.conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6ca7e0]]}
  input_1_add_336:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7e8580]]}
  input_1_add_336_fork_clone1378:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7e2400]]}
  lc.input_tensor.concatenate_349.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6815c0], [1, 0x3f684680], [1, 0x7f6935a0]]}
  lc.input_tensor.concatenate_349.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f74ac60], [5, 0x7f74d860], [0, 0x2f69a420]]}
  lc.input_tensor.conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7807c0]]}
  lc.input_tensor.conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c97a0]]}
  lc.input_tensor.conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f720920]]}
  lc.input_tensor.conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f74c820]]}
  lc.input_tensor.conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f77d120]]}
  lc.input_tensor.conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f730380]]}
  input_1_add_351:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f87b300]]}
  input_1_add_351_fork_clone1567:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f8718a0]]}
  lc.input_tensor.conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f76b100]]}
  lc.input_tensor.conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6a1740]]}
  lc.input_tensor.conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6e11a0]]}
  lc.input_tensor.conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6d2e40]]}
  lc.input_tensor.conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6c7fe0]]}
  lc.input_tensor.conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6bf520]]}
  input_1_add_365:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7d58c0]]}
  input_1_add_365_fork_clone1383:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7c2420]]}
  lc.input_tensor.concatenate_378.dc.sparse_matmul.7.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f68b740], [1, 0x7f69a660]]}
  lc.input_tensor.concatenate_378.dc.sparse_matmul.7.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6df7a0], [4, 0x7f728180]]}
  lc.input_tensor.conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6f7280], [3, 0x7f6f7de0], [4, 0x3f7054c0], [4, 0x7f74dea0], [5, 0x3f7656c0]]}
  lc.input_tensor.conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6ae5e0], [2, 0x3f6be4e0], [2, 0x7f6c5cc0], [3, 0x3f6d0240], [3, 0x7f6d0dc0]]}
  lc.input_tensor.conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f764360], [5, 0x7f766f60], [0, 0x2f6b3b20], [0, 0x7f6c0fc0], [1, 0x3f6c4080]]}
  lc.input_tensor.conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6cce80], [3, 0x3f6d7400], [3, 0x7f6d7f80], [4, 0x3f6e5920], [4, 0x7f72e300]]}
  lc.input_tensor.conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f68f5c0], [1, 0x7f69e4e0], [2, 0x3f6ae3e0], [2, 0x7f6b5cc0], [3, 0x3f6c0240]]}
  lc.input_tensor.conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f745b20], [5, 0x7f748720], [0, 0x2f6952e0], [0, 0x7f6a2780], [1, 0x3f6a5840]]}
  input_1_add_380:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 7], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fb7a200]]}
  input_1_add_380_fork_clone1388:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 7], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7fb7a200]]}
  lc.input_tensor.concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f727e40]]}
  lc.input_tensor.concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.1:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f72d2c0]]}
  input_1_add_395_fork_clone1146:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f71a640], [4, 0x7f762d00]]}
  lc.input_tensor.conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f7682c0], [0, 0x2f6b4e80], [0, 0x7f6c2320], [1, 0x3f6c53e0], [1, 0x7f6d42c0]]}
  lc.input_tensor.conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e6960], [4, 0x7f72f340], [5, 0x3f746b60], [5, 0x7f749760], [0, 0x2f696320]]}
  lc.input_tensor.conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6cb9e0]]}
  lc.input_tensor.conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6d4ec0]]}
  lc.input_tensor.conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f680ee0], [0, 0x7f68e380], [1, 0x3f691440], [1, 0x7f6a0360], [2, 0x3f6b0260]]}
  lc.input_tensor.conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b16a0], [2, 0x3f6c15a0], [2, 0x7f6c8d80], [3, 0x3f6d3300], [3, 0x7f6d3e80]]}
  input_1_add_409:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f845c80]]}
  input_1_add_409_fork_clone979:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f85b9e0]]}
  lc.input_tensor.concatenate_422.dc.sparse_matmul.4.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f724ac0], [5, 0x7f727980], [0, 0x2f674540]]}
  lc.input_tensor.concatenate_422.dc.sparse_matmul.4.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f745660], [0, 0x2f692220], [0, 0x7f69f6c0]]}
  lc.input_tensor.conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f710720]]}
  lc.input_tensor.conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c56a0]]}
  lc.input_tensor.conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6cdb00]]}
  lc.input_tensor.conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6d22c0]]}
  lc.input_tensor.conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f76bfa0], [0, 0x2f6b8b60], [0, 0x7f6c5ea0], [1, 0x3f6c8f60], [1, 0x7f6d7e40]]}
  lc.input_tensor.conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d1fc0], [3, 0x3f6dc540], [3, 0x7f6dd0c0], [4, 0x3f6eaa60], [4, 0x7f733440]]}
  input_1_add_424:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f89d360]]}
  input_1_add_424_fork_clone1152:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f898a40]]}
  lc.input_tensor.conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6ddf00]]}
  lc.input_tensor.conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6a8900]]}
  lc.input_tensor.conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f7062a0]]}
  lc.input_tensor.conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6bb420]]}
  lc.input_tensor.conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f776b20]]}
  lc.input_tensor.conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f72a200]]}
  input_1_add_438:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7dba40]]}
  input_1_add_438_fork_clone984:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7e5cc0]]}
  lc.input_tensor.concatenate_451.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6aa820]]}
  lc.input_tensor.concatenate_451.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f72b240]]}
  lc.input_tensor.conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f77c580]]}
  lc.input_tensor.conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b4760]]}
  lc.input_tensor.conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f71c6e0]]}
  lc.input_tensor.conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f69c600]]}
  lc.input_tensor.conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6faf60], [3, 0x7f6fbac0], [4, 0x3f7091a0], [4, 0x7f751b80], [5, 0x3f7693a0]]}
  lc.input_tensor.conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d0f80], [3, 0x3f6db500], [3, 0x7f6dc080], [4, 0x3f6e9a20], [4, 0x7f732400]]}
  input_1_add_453:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f97f420]]}
  input_1_add_453_fork_clone1158:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f9759c0]]}
  lc.input_tensor.conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f724b60]]}
  lc.input_tensor.conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6aa980]]}
  lc.input_tensor.conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6e4260]]}
  lc.input_tensor.conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f747ba0]]}
  lc.input_tensor.conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6d66e0]]}
  lc.input_tensor.conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e89e0]]}
  input_1_add_467:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7deb60]]}
  input_1_add_467_fork_clone989:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7ec6c0]]}
  lc.input_tensor.concatenate_480.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6c4320], [4, 0x7f70cd00]]}
  lc.input_tensor.concatenate_480.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6a3320], [2, 0x3f6b3220]]}
  lc.input_tensor.conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f783060]]}
  lc.input_tensor.conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f690300]]}
  lc.input_tensor.conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f785180]]}
  lc.input_tensor.conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f69a580]]}
  lc.input_tensor.conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f779720]]}
  lc.input_tensor.conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6bd4a0]]}
  input_1_add_482:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f858240]]}
  input_1_add_482_fork_clone1164:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f871060]]}
  lc.input_tensor.conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6df080]]}
  lc.input_tensor.conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6d85e0]]}
  lc.input_tensor.conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f714960]]}
  lc.input_tensor.conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6cccc0]]}
  lc.input_tensor.conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6c9ce0]]}
  lc.input_tensor.conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f73b8a0]]}
  input_1_add_496:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f868520]]}
  input_1_add_496_fork_clone994:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f8527c0]]}
  lc.input_tensor.concatenate_509.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6735a0], [0, 0x7f680a60], [1, 0x3f683b20], [1, 0x7f692a40]]}
  lc.input_tensor.concatenate_509.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6b93a0]]}
  lc.input_tensor.conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 17], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6ea6c0], [2, 0x7f6f1ea0], [3, 0x3f6fc420], [3, 0x7f6fcf80], [4, 0x3f70a660]]}
  lc.input_tensor.conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f688fe0], [0, 0x7f696480], [1, 0x3f699540], [1, 0x7f6a8460], [2, 0x3f6b8360]]}
  lc.input_tensor.conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6d1d40]]}
  lc.input_tensor.conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6974c0]]}
  lc.input_tensor.conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f767ee0], [5, 0x7f76aae0], [0, 0x2f6b76a0], [0, 0x7f6c49e0], [1, 0x3f6c7aa0]]}
  lc.input_tensor.conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6a63e0], [2, 0x3f6b62e0], [2, 0x7f6bdac0], [3, 0x3f6c8040], [3, 0x7f6c8bc0]]}
  input_1_add_511:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7eef40], [2, 0x7f7f50c0], [3, 0x3f7ff340], [3, 0x7f7ffb60], [4, 0x3f80c680]]}
  input_1_add_511_fork_clone1170:                                                            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f8770e0], [0, 0x2f7c2c60], [0, 0x7f7cef60], [1, 0x3f7d2020], [1, 0x7f7dfb80]]}
  lc.input_tensor.conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f768fe0]]}
  lc.input_tensor.conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6c5b00]]}
  lc.input_tensor.conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6d2f60], [2, 0x3f6e2e60], [2, 0x7f6ea640], [3, 0x3f6f4bc0], [3, 0x7f6f5720]]}
  lc.input_tensor.conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f695440], [1, 0x7f6a4360], [2, 0x3f6b4260], [2, 0x7f6bba40], [3, 0x3f6c5fc0]]}
  lc.input_tensor.conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f778820]]}
  lc.input_tensor.conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f683ea0]]}
  input_1_add_525:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7fba80]]}
  input_1_add_525_fork_clone999:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7dbaa0]]}
  lc.input_tensor.concatenate_538.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6ab5c0], [3, 0x3f6b5ca0], [3, 0x7f6b6820]]}
  lc.input_tensor.concatenate_538.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f74b7e0], [0, 0x2f6983a0], [0, 0x7f6a5840]]}
  lc.input_tensor.conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6f2e00]]}
  lc.input_tensor.conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b7820]]}
  lc.input_tensor.conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f76d220]]}
  lc.input_tensor.conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6cef00]]}
  lc.input_tensor.conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6d5320]]}
  lc.input_tensor.conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f74e8a0]]}
  input_1_add_540:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f8a6dc0]]}
  input_1_add_540_fork_clone1176:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f8a6dc0]]}
  lc.input_tensor.conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f70a4e0]]}
  lc.input_tensor.conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d3000]]}
  lc.input_tensor.conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f722a40]]}
  lc.input_tensor.conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6a6880]]}
  lc.input_tensor.conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6c62e0]]}
  lc.input_tensor.conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f7425a0]]}
  input_1_add_554:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f8191e0]]}
  input_1_add_554_fork_clone1004:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7e85e0]]}
  lc.input_tensor.concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.0:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f674ae0], [0, 0x7f681fa0], [1, 0x3f685060]]}
  lc.input_tensor.concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.1:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f741a20], [5, 0x7f744620], [0, 0x2f6911e0]]}
  lc.input_tensor.concatenate_567.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6ab1e0]]}
  lc.input_tensor.concatenate_567.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f69e680]]}
  lc.input_tensor.conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6d8d20]]}
  lc.input_tensor.conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6d8440]]}
  lc.input_tensor.conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f766ec0]]}
  lc.input_tensor.conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6cac40]]}
  lc.input_tensor.conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6af320], [2, 0x7f6b6c00], [3, 0x3f6c1180], [3, 0x7f6c1d00], [4, 0x3f6cf6a0]]}
  lc.input_tensor.conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f685f20], [0, 0x7f6933c0], [1, 0x3f696480], [1, 0x7f6a53a0], [2, 0x3f6b52a0]]}
  input_1_add_569:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f9cb700]]}
  input_1_add_569_fork_clone1182:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f9c1ca0]]}
  lc.input_tensor.conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f780f40]]}
  lc.input_tensor.conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6da000]]}
  lc.input_tensor.conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6fc9a0]]}
  lc.input_tensor.conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e79a0]]}
  lc.input_tensor.conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6d83e0]]}
  lc.input_tensor.conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6da4c0]]}
  input_1_add_583:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7b58e0]]}
  input_1_add_583_fork_clone1009:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7b6120]]}
  lc.input_tensor.concatenate_596.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6c48c0], [4, 0x7f70d2a0]]}
  lc.input_tensor.concatenate_596.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6c6d00], [3, 0x3f6d1280]]}
  lc.input_tensor.conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f77e6a0]]}
  lc.input_tensor.conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f73f9a0]]}
  lc.input_tensor.conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f71e800]]}
  lc.input_tensor.conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f7466a0]]}
  lc.input_tensor.conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f77b420]]}
  lc.input_tensor.conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e2860]]}
  input_1_add_598:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f8b28a0]]}
  input_1_add_598_fork_clone1188:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f861ca0]]}
  lc.input_tensor.conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6feac0]]}
  lc.input_tensor.conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e1820]]}
  lc.input_tensor.conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6e0020]]}
  lc.input_tensor.conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6a2780]]}
  lc.input_tensor.conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6d3620]]}
  lc.input_tensor.conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f742a60]]}
  input_1_add_612:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7ffb40]]}
  input_1_add_612_fork_clone1014:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7f3020]]}
  lc.input_tensor.concatenate_625.dc.sparse_matmul.7.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f7140a0], [5, 0x3f72b8c0], [5, 0x7f72e620]]}
  lc.input_tensor.concatenate_625.dc.sparse_matmul.7.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e38a0], [4, 0x7f72c280], [5, 0x3f743aa0]]}
  lc.input_tensor.conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 17], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f76e920], [0, 0x2f6bb4e0], [0, 0x7f6c8820], [1, 0x3f6cb8e0], [1, 0x7f6da7c0]]}
  lc.input_tensor.conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b3720], [2, 0x3f6c3620], [2, 0x7f6cae00], [3, 0x3f6d5380], [3, 0x7f6d5f00]]}
  lc.input_tensor.conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 17], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6fdba0], [3, 0x7f6fe700], [4, 0x3f70bde0], [4, 0x7f754500], [5, 0x3f76bd20]]}
  lc.input_tensor.conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f744ae0], [5, 0x7f7476e0], [0, 0x2f6942a0], [0, 0x7f6a1740], [1, 0x3f6a4800]]}
  lc.input_tensor.conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6d6980], [2, 0x3f6e6880], [2, 0x7f6ee060], [3, 0x3f6f85e0], [3, 0x7f6f9140]]}
  lc.input_tensor.conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c4660], [2, 0x7f6cbe40], [3, 0x3f6d63c0], [3, 0x7f6d6f40], [4, 0x3f6e48e0]]}
  input_1_add_627:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fd54740]]}
  input_1_add_627_fork_clone1019:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fd54740]]}
  lc.input_tensor.concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.0:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7275e0], [5, 0x7f72a4a0], [0, 0x2f677060]]}
  lc.input_tensor.concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.1:                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6a37c0], [1, 0x3f6a6880], [1, 0x7f6b57a0]]}
  input_1_add_642_fork_clone807:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f763e00], [0, 0x2f6b09c0], [0, 0x7f6bde60], [1, 0x3f6c0f20], [1, 0x7f6cfe40]]}
  lc.input_tensor.max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                    {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6e7000], [2, 0x3f6f6f00], [2, 0x7f6fe6e0], [3, 0x3f708b60], [3, 0x7f7096c0], [4, 0x3f716c40], [4, 0x7f75f300]]}
  lc.input_tensor.max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                    {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f693260], [0, 0x7f6a0700], [1, 0x3f6a37c0], [1, 0x7f6b26e0], [2, 0x3f6c25e0], [2, 0x7f6c9dc0], [3, 0x3f6d4340]]}
  lc.input_tensor.conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f72b9c0]]}
  lc.input_tensor.conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f69f6c0]]}
  lc.input_tensor.conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f71de60]]}
  lc.input_tensor.conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f68f160]]}
  lc.input_tensor.conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6f54c0]]}
  lc.input_tensor.conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6d1e00]]}
  input_1_add_657:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7da860]]}
  input_1_add_657_fork_clone581:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7263e0]]}
  lc.input_tensor.concatenate_670.dc.sparse_matmul.4.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f693f80], [2, 0x3f6a4280], [2, 0x7f6abb60], [3, 0x3f6b6240]]}
  lc.input_tensor.concatenate_670.dc.sparse_matmul.4.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f7435e0], [0, 0x2f6901a0], [0, 0x7f69d640], [1, 0x3f6a0700]]}
  lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f7719e0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f7409e0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f792e40]]}
  lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f7291c0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f7137c0]]}
  lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6e07e0]]}
  input_1_add_672:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f6dfd40], [2, 0x7f6e7520], [3, 0x3f6f1aa0], [3, 0x7f6f2600], [4, 0x3f6fffa0], [4, 0x7f748980], [5, 0x3f7601a0]]}
  input_1_add_672_fork_clone764:                                                             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f6b1a20], [0, 0x7f6beec0], [1, 0x3f6c1f80], [1, 0x7f6d0ea0], [2, 0x3f6e0da0], [2, 0x7f6e8580], [3, 0x3f6f2b00]]}
  lc.input_tensor.conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f721060]]}
  lc.input_tensor.conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6c7d40]]}
  lc.input_tensor.conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f776720]]}
  lc.input_tensor.conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c0560]]}
  lc.input_tensor.conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6f2040]]}
  lc.input_tensor.conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b0660]]}
  input_1_add_686:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7cedc0]]}
  input_1_add_686_fork_clone586:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7e8c20]]}
  lc.input_tensor.concatenate_699.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f67b1e0], [0, 0x7f688680]]}
  lc.input_tensor.concatenate_699.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c8760], [2, 0x7f6cff40]]}
  lc.input_tensor.conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6dd360]]}
  lc.input_tensor.conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b8860]]}
  lc.input_tensor.conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f70e8a0]]}
  lc.input_tensor.conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6a9940]]}
  lc.input_tensor.conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f706240]]}
  lc.input_tensor.conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f749c20]]}
  input_1_add_701:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7dc2e0]]}
  input_1_add_701_fork_clone770:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f890760]]}
  lc.input_tensor.conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6e5960]]}
  lc.input_tensor.conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6a78c0]]}
  lc.input_tensor.conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f70c200]]}
  lc.input_tensor.conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6ba8e0]]}
  lc.input_tensor.conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f704800]]}
  lc.input_tensor.conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6a8900]]}
  input_1_add_715:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f752f20]]}
  input_1_add_715_fork_clone591:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7622e0]]}
  lc.input_tensor.concatenate_728.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f70c8c0], [5, 0x3f724240], [5, 0x7f727100], [0, 0x2f673cc0], [0, 0x7f681180]]}
  lc.input_tensor.concatenate_728.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6dd580], [3, 0x7f6de100], [4, 0x3f6ebaa0], [4, 0x7f734480], [5, 0x3f74bca0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6f75c0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6b98a0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f7139e0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6cdec0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7750e0]]}
  lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c66e0]]}
  input_1_add_730:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f773f40]]}
  input_1_add_730_fork_clone776:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7c7be0]]}
  lc.input_tensor.conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f78b860]]}
  lc.input_tensor.conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6a4800]]}
  lc.input_tensor.conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6dacc0]]}
  lc.input_tensor.conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f697360]]}
  lc.input_tensor.conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6f3a80]]}
  lc.input_tensor.conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6db040]]}
  input_1_add_744:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7df1a0]]}
  input_1_add_744_fork_clone596:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7e1a40]]}
  lc.input_tensor.concatenate_757.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6b8a80]]}
  lc.input_tensor.concatenate_757.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6c7720]]}
  lc.input_tensor.conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f71b7c0]]}
  lc.input_tensor.conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f748be0]]}
  lc.input_tensor.conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f71c320]]}
  lc.input_tensor.conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6af620]]}
  lc.input_tensor.conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f7056e0]]}
  lc.input_tensor.conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6c2c00]]}
  input_1_add_759:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f8371e0]]}
  input_1_add_759_fork_clone782:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f8c9e20]]}
  lc.input_tensor.conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f78ba60]]}
  lc.input_tensor.conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f694400]]}
  lc.input_tensor.conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f720500]]}
  lc.input_tensor.conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f7346e0]]}
  lc.input_tensor.conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f75d8c0]]}
  lc.input_tensor.conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f71cec0]]}
  input_1_add_773:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f744b60]]}
  input_1_add_773_fork_clone601:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f74f680]]}
  lc.input_tensor.concatenate_786.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6b8ec0]]}
  lc.input_tensor.concatenate_786.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f692380]]}
  lc.input_tensor.conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6efe00]]}
  lc.input_tensor.conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f684ee0]]}
  lc.input_tensor.conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6fe9a0]]}
  lc.input_tensor.conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f738320]]}
  lc.input_tensor.conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6c48a0]]}
  lc.input_tensor.conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6bca80]]}
  input_1_add_788:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f773720]]}
  input_1_add_788_fork_clone788:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f787c40]]}
  lc.input_tensor.conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7905a0]]}
  lc.input_tensor.conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f736760]]}
  lc.input_tensor.conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f78df00]]}
  lc.input_tensor.conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6c3f40]]}
  lc.input_tensor.conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6fcca0]]}
  lc.input_tensor.conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f71ae40]]}
  input_1_add_802:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f76cd60]]}
  input_1_add_802_fork_clone606:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f76c540]]}
  lc.input_tensor.concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 18], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f711ee0]]}
  lc.input_tensor.concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.1:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6c4f80]]}
  lc.input_tensor.concatenate_815.dc.sparse_matmul.6.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6ac960], [3, 0x3f6b7040], [3, 0x7f6b7bc0]]}
  lc.input_tensor.concatenate_815.dc.sparse_matmul.6.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6a22e0], [2, 0x3f6b21e0], [2, 0x7f6b99c0]]}
  lc.input_tensor.conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6ed760]]}
  lc.input_tensor.conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6933c0]]}
  lc.input_tensor.conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f7074c0]]}
  lc.input_tensor.conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f682e60]]}
  lc.input_tensor.conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7736a0]]}
  lc.input_tensor.conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f7362a0]]}
  input_1_add_817:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f832840]]}
  input_1_add_817_fork_clone794:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f825d20]]}
  lc.input_tensor.conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f704e20]]}
  lc.input_tensor.conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f71be80]]}
  lc.input_tensor.conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6e32c0]]}
  lc.input_tensor.conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6db6a0]]}
  lc.input_tensor.conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6e2140]]}
  lc.input_tensor.conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6cdd00]]}
  input_1_add_831:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7694a0]]}
  input_1_add_831_fork_clone611:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f744380]]}
  lc.input_tensor.concatenate_844.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6a38a0], [2, 0x7f6ab180], [3, 0x3f6b5860], [3, 0x7f6b63e0], [4, 0x3f6c3ee0]]}
  lc.input_tensor.concatenate_844.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f73f4e0], [0, 0x2f68c0a0], [0, 0x7f699540], [1, 0x3f69c600], [1, 0x7f6ab520]]}
  lc.input_tensor.conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f786b20]]}
  lc.input_tensor.conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f73c8e0]]}
  lc.input_tensor.conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f726c80]]}
  lc.input_tensor.conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f7250c0]]}
  lc.input_tensor.conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6d01a0]]}
  lc.input_tensor.conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6ced40]]}
  input_1_add_846:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7412c0]]}
  input_1_add_846_fork_clone800:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7347a0]]}
  lc.input_tensor.conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f719120]]}
  lc.input_tensor.conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f68d0e0]]}
  lc.input_tensor.conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f76f340]]}
  lc.input_tensor.conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f73d920]]}
  lc.input_tensor.conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f715200]]}
  lc.input_tensor.conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f726100]]}
  input_1_add_860:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7e6380]]}
  input_1_add_860_fork_clone616:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f732f00]]}
  lc.input_tensor.concatenate_873.dc.sparse_matmul.7.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6c3000], [3, 0x7f6c3b80]]}
  lc.input_tensor.concatenate_873.dc.sparse_matmul.7.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6c4c80], [3, 0x3f6cf200]]}
  lc.input_tensor.conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f686940], [1, 0x3f689a00], [1, 0x7f698920], [2, 0x3f6a8ae0], [2, 0x7f6b03c0], [3, 0x3f6baaa0], [3, 0x7f6bb620]]}
  lc.input_tensor.conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f727140], [5, 0x3f73e960], [5, 0x7f741560], [0, 0x2f68e120], [0, 0x7f69b5c0], [1, 0x3f69e680], [1, 0x7f6ad5a0]]}
  lc.input_tensor.conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f67a820], [0, 0x7f687cc0], [1, 0x3f68ad80], [1, 0x7f699ca0], [2, 0x3f6a9e60], [2, 0x7f6b1740], [3, 0x3f6bbe20]]}
  lc.input_tensor.conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6c9c00], [4, 0x3f6d75a0], [4, 0x7f71ff80], [5, 0x3f7377a0], [5, 0x7f73a3a0], [0, 0x2f686f60], [0, 0x7f694400]]}
  lc.input_tensor.conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f725760], [5, 0x7f728620], [0, 0x2f6751e0], [0, 0x7f6826a0], [1, 0x3f685760], [1, 0x7f694680], [2, 0x3f6a4980]]}
  lc.input_tensor.conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f73b3e0], [0, 0x2f687fa0], [0, 0x7f695440], [1, 0x3f698500], [1, 0x7f6a7420], [2, 0x3f6b7320], [2, 0x7f6beb00]]}
  input_1_add_875:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 2], ublock: [1, 5], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f71b0e0], [0, 0x7f727c00], [1, 0x3f72acc0], [1, 0x7f739860], [2, 0x3f748c20], [2, 0x7f74fde0], [3, 0x3f75a060]]}
  input_1_add_875_fork_clone621:                                                             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 2], ublock: [1, 5], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f75a880], [4, 0x3f7673a0], [4, 0x7f7af560], [5, 0x3f7c6b20], [5, 0x7f7c93c0], [0, 0x2f715f80], [0, 0x7f722aa0]]}
  lc.input_tensor.concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f68aaa0]]}
  lc.input_tensor.concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.1:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f724080]]}
  input_1_add_890_fork_clone379:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f723700], [4, 0x3f730700], [4, 0x7f778dc0]]}
  lc.input_tensor.conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f697f60], [2, 0x3f6a8120], [2, 0x7f6afa00], [3, 0x3f6ba0e0], [3, 0x7f6bac60], [4, 0x3f6c8600], [4, 0x7f710fe0]]}
  lc.input_tensor.conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6c0b80], [3, 0x3f6cb100], [3, 0x7f6cbc80], [4, 0x3f6d9620], [4, 0x7f722000], [5, 0x3f739820], [5, 0x7f73c420]]}
  lc.input_tensor.conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6c8fc0], [4, 0x7f7119a0], [5, 0x3f7291c0], [5, 0x7f72bf20], [0, 0x2f678ae0], [0, 0x7f685f80], [1, 0x3f689040]]}
  lc.input_tensor.conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f723040], [5, 0x3f73a860], [5, 0x7f73d460], [0, 0x2f68a020], [0, 0x7f6974c0], [1, 0x3f69a580], [1, 0x7f6a94a0]]}
  lc.input_tensor.conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6b7740], [3, 0x7f6b82c0], [4, 0x3f6c5c60], [4, 0x7f70e640], [5, 0x3f725e60], [5, 0x7f728d20], [0, 0x2f6758e0]]}
  lc.input_tensor.conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f68b060], [0, 0x7f698500], [1, 0x3f69b5c0], [1, 0x7f6aa4e0], [2, 0x3f6ba3e0], [2, 0x7f6c1bc0], [3, 0x3f6cc140]]}
  input_1_add_904:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f779880]]}
  input_1_add_904_fork_clone212:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f735fc0]]}
  lc.input_tensor.concatenate_917.dc.sparse_matmul.4.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6ad060]]}
  lc.input_tensor.concatenate_917.dc.sparse_matmul.4.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6993e0]]}
  lc.input_tensor.conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f78e100]]}
  lc.input_tensor.conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f75fce0]]}
  lc.input_tensor.conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f70c600]]}
  lc.input_tensor.conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6ddca0]]}
  lc.input_tensor.conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f703ca0]]}
  lc.input_tensor.conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f75dc60]]}
  input_1_add_919:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f6dd7c0], [2, 0x3f6ed6c0], [2, 0x7f6f4ea0], [3, 0x3f6ff320], [3, 0x7f6ffe80], [4, 0x3f70d560], [4, 0x7f755c80]]}
  input_1_add_919_fork_clone385:                                                             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f70ede0], [4, 0x7f757500], [5, 0x3f76ed20], [5, 0x7f771920], [0, 0x2f6be4e0], [0, 0x7f6cb820], [1, 0x3f6ce8e0]]}
  lc.input_tensor.conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f7175e0]]}
  lc.input_tensor.conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6c9ca0]]}
  lc.input_tensor.conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6f4f20]]}
  lc.input_tensor.conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6fae60]]}
  lc.input_tensor.conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6f9820]]}
  lc.input_tensor.conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6bbdc0]]}
  input_1_add_933:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f765b80]]}
  input_1_add_933_fork_clone217:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7726a0]]}
  lc.input_tensor.concatenate_946.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6bc9a0], [4, 0x3f6ca340], [4, 0x7f712d20], [5, 0x3f72a540]]}
  lc.input_tensor.concatenate_946.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6fbea0], [4, 0x7f744880], [5, 0x3f75c0a0], [5, 0x7f75eca0]]}
  lc.input_tensor.conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6d5f80]]}
  lc.input_tensor.conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6b8d00]]}
  lc.input_tensor.conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f7893c0]]}
  lc.input_tensor.conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f743840]]}
  lc.input_tensor.conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6d1be0]]}
  lc.input_tensor.conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6b7cc0]]}
  input_1_add_948:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7348e0], [3, 0x7f735440], [4, 0x3f742440], [4, 0x7f78ab00], [5, 0x3f7a20c0], [5, 0x7f7a4960], [0, 0x2f6f1520]]}
  input_1_add_948_fork_clone391:                                                             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7371a0], [3, 0x7f737d00], [4, 0x3f744d00], [4, 0x7f78d3c0], [5, 0x3f7a4980], [5, 0x7f7a7220], [0, 0x2f6f3de0]]}
  lc.input_tensor.conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f70eca0]]}
  lc.input_tensor.conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6e0340]]}
  lc.input_tensor.conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f719c80]]}
  lc.input_tensor.conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6bde40]]}
  lc.input_tensor.conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f7762a0]]}
  lc.input_tensor.conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6e4440]]}
  input_1_add_962:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f780a60]]}
  input_1_add_962_fork_clone222:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f752740]]}
  lc.input_tensor.concatenate_975.dc.sparse_matmul.5.0:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6778c0]]}
  lc.input_tensor.concatenate_975.dc.sparse_matmul.5.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6afac0]]}
  lc.input_tensor.conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6e6380]]}
  lc.input_tensor.conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f7529a0]]}
  lc.input_tensor.conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f716a80]]}
  lc.input_tensor.conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6ada40]]}
  lc.input_tensor.conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f75be80]]}
  lc.input_tensor.conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6a05a0]]}
  input_1_add_977:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7b68a0]]}
  input_1_add_977_fork_clone397:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7abe00]]}
  lc.input_tensor.conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6ea6a0]]}
  lc.input_tensor.conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6d0960]]}
  lc.input_tensor.conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f7907a0]]}
  lc.input_tensor.conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6e3400]]}
  lc.input_tensor.conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f707c80]]}
  lc.input_tensor.conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6ecae0]]}
  input_1_add_991:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f73b980]]}
  input_1_add_991_fork_clone227:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f77b120]]}
  lc.input_tensor.concatenate_1004.dc.sparse_matmul.6.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f69f420]]}
  lc.input_tensor.concatenate_1004.dc.sparse_matmul.6.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f74dd20]]}
  lc.input_tensor.conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f716080]]}
  lc.input_tensor.conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6edb20]]}
  lc.input_tensor.conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6ecd40]]}
  lc.input_tensor.conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x7f6d5080]]}
  lc.input_tensor.conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6d4ca0]]}
  lc.input_tensor.conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6aea80]]}
  input_1_add_1006:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f954940]]}
  input_1_add_1006_fork_clone403:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f9adf40]]}
  lc.input_tensor.conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6e8a20]]}
  lc.input_tensor.conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f755f20]]}
  lc.input_tensor.conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f7891c0]]}
  lc.input_tensor.conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6f5d20]]}
  lc.input_tensor.conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6c2e60]]}
  lc.input_tensor.conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6e7800]]}
  input_1_add_1020:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f77a900]]}
  input_1_add_1020_fork_clone232:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f770680]]}
  lc.input_tensor.concatenate_1033.dc.sparse_matmul.5.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f72ad00]]}
  lc.input_tensor.concatenate_1033.dc.sparse_matmul.5.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f756f60]]}
  lc.input_tensor.conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f711340]]}
  lc.input_tensor.conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6e93c0]]}
  lc.input_tensor.conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f6e8000]]}
  lc.input_tensor.conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f75aba0]]}
  lc.input_tensor.conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6e3b80]]}
  lc.input_tensor.conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f757fa0]]}
  input_1_add_1035:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f6e08c0], [2, 0x3f6f07c0], [2, 0x7f6f7fa0], [3, 0x3f702420], [3, 0x7f702f80], [4, 0x3f710660], [4, 0x7f758d80]]}
  input_1_add_1035_fork_clone409:                                                            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7705a0], [5, 0x7f7731a0], [0, 0x2f6bfd60], [0, 0x7f6cd0a0], [1, 0x3f6d0160], [1, 0x7f6df040], [2, 0x3f6eef40]]}
  lc.input_tensor.conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f71e9c0]]}
  lc.input_tensor.conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f73f740]]}
  lc.input_tensor.conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6f9c60]]}
  lc.input_tensor.conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6c1aa0]]}
  lc.input_tensor.conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f777ce0]]}
  lc.input_tensor.conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6e52c0]]}
  input_1_add_1049:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7694c0]]}
  input_1_add_1049_fork_clone237:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f75a100]]}
  lc.input_tensor.concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6b6dc0]]}
  lc.input_tensor.concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.1:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f753ea0]]}
  lc.input_tensor.concatenate_1062.dc.sparse_matmul.6.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f687e20], [1, 0x7f696d40], [2, 0x3f6a6f00], [2, 0x7f6ae7e0]]}
  lc.input_tensor.concatenate_1062.dc.sparse_matmul.6.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f757ae0], [0, 0x2f6a46a0], [0, 0x7f6b1b40], [1, 0x3f6b4c00]]}
  lc.input_tensor.conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f72e060]]}
  lc.input_tensor.conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f73d6c0]]}
  lc.input_tensor.conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6fc300]]}
  lc.input_tensor.conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6bfa20]]}
  lc.input_tensor.conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6d3260]]}
  lc.input_tensor.conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f754ee0]]}
  input_1_add_1064:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f797dc0], [5, 0x7f79a660], [0, 0x2f6e7220], [0, 0x7f6f4560], [1, 0x3f6f7620], [1, 0x7f7061c0], [2, 0x3f7160c0]]}
  input_1_add_1064_fork_clone415:                                                            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f708a80], [2, 0x3f718980], [2, 0x7f720160], [3, 0x3f72a5e0], [3, 0x7f72b140], [4, 0x3f738140], [4, 0x7f780800]]}
  lc.input_tensor.conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6dfa00]]}
  lc.input_tensor.conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f756aa0]]}
  lc.input_tensor.conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f729320]]}
  lc.input_tensor.conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6d29e0]]}
  lc.input_tensor.conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f6e55c0]]}
  lc.input_tensor.conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6d19a0]]}
  input_1_add_1078:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7d5fa0]]}
  input_1_add_1078_fork_clone242:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f78ee20]]}
  lc.input_tensor.concatenate_1091.dc.sparse_matmul.5.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f681a00], [1, 0x3f684ac0], [1, 0x7f6939e0]]}
  lc.input_tensor.concatenate_1091.dc.sparse_matmul.5.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f75bbe0], [0, 0x2f6a87a0], [0, 0x7f6b5c40]]}
  lc.input_tensor.conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f774080]]}
  lc.input_tensor.conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6b8d00]]}
  lc.input_tensor.conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f6eb0c0]]}
  lc.input_tensor.conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x3f6f6d60]]}
  lc.input_tensor.conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f707120]]}
  lc.input_tensor.conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6e8380]]}
  input_1_add_1093:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7b6080]]}
  input_1_add_1093_fork_clone421:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f77e2a0]]}
  lc.input_tensor.conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f6d8620]]}
  lc.input_tensor.conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x7f758b20]]}
  lc.input_tensor.conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f709b60]]}
  lc.input_tensor.conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6ab9c0]]}
  lc.input_tensor.conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6fb260]]}
  lc.input_tensor.conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6df600]]}
  input_1_add_1107:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f782300]]}
  input_1_add_1107_fork_clone247:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f781ae0]]}
  lc.input_tensor.concatenate_1120.dc.sparse_matmul.7.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x7f74cb40]]}
  lc.input_tensor.concatenate_1120.dc.sparse_matmul.7.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6ce8e0]]}
  lc.input_tensor.conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x3f6bb460], [3, 0x7f6bbfe0], [4, 0x3f6c9980], [4, 0x7f712360], [5, 0x3f729b80], [5, 0x7f72c8e0], [0, 0x2f6794a0]]}
  lc.input_tensor.conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f69d4e0], [0, 0x7f6aa980], [1, 0x3f6ada40], [1, 0x7f6bc960], [2, 0x3f6cc860], [2, 0x7f6d4040], [3, 0x3f6de5c0]]}
  lc.input_tensor.conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f72d2a0], [0, 0x2f679e60], [0, 0x7f687300], [1, 0x3f68a3c0], [1, 0x7f6992e0], [2, 0x3f6a94a0], [2, 0x7f6b0d80]]}
  lc.input_tensor.conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f7354c0], [5, 0x3f74cce0], [5, 0x7f74f8e0], [0, 0x2f69c4a0], [0, 0x7f6a9940], [1, 0x3f6aca00], [1, 0x7f6bb920]]}
  lc.input_tensor.conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6ac260], [3, 0x3f6b6940], [3, 0x7f6b74c0], [4, 0x3f6c4e60], [4, 0x7f70d840], [5, 0x3f725060], [5, 0x7f727f20]]}
  lc.input_tensor.conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f73a600], [5, 0x3f751e20], [5, 0x7f754a20], [0, 0x2f6a15e0], [0, 0x7f6aea80], [1, 0x3f6b1b40], [1, 0x7f6c0a60]]}
  input_1_add_1122:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 13], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f9f1880]]}
  input_1_add_1122_fork_clone252:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 13], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f9f1880]]}
  lc.input_tensor.concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6a5780]]}
  lc.input_tensor.concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.1:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6b0b00]]}
  input_1_add_1137_fork_clone118:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 23], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f815000]]}
  lc.input_tensor.max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 31], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f72cd20]]}
  lc.input_tensor.max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f738580]]}
  lc.input_tensor.conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2f67e280]]}
  lc.input_tensor.conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f74fda0]]}
  lc.input_tensor.conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x7f7316c0]]}
  lc.input_tensor.conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6aca00]]}
  lc.input_tensor.conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6be840]]}
  lc.input_tensor.conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f736500]]}
  input_1_add_1152:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7dd180]]}
  input_1_add_1152_fork_clone58:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7f4740]]}
  lc.input_tensor.concatenate_1165.dc.sparse_matmul.4.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f684240]]}
  lc.input_tensor.concatenate_1165.dc.sparse_matmul.4.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x7f6ccd60]]}
  lc.input_tensor.conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x7f68b720]]}
  lc.input_tensor.conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x7f6bad80]]}
  lc.input_tensor.conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x7f6b4ee0]]}
  lc.input_tensor.conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6ad8e0]]}
  lc.input_tensor.conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f68db60]]}
  lc.input_tensor.conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6d8b60]]}
  input_1_add_1167:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f805d40]]}
  input_1_add_1167_fork_clone111:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f815100]]}
  lc.input_tensor.conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x3f6ad600]]}
  lc.input_tensor.conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x7f742800]]}
  lc.input_tensor.conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f69d700]]}
  lc.input_tensor.conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6aa820]]}
  lc.input_tensor.conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f69b180]]}
  lc.input_tensor.conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x7f6ed4c0]]}
  input_1_add_1181:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f796000]]}
  input_1_add_1181_fork_clone63:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7894e0]]}
  lc.input_tensor.concatenate_1194.dc.sparse_matmul.5.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x7f6b9a40]]}
  lc.input_tensor.concatenate_1194.dc.sparse_matmul.5.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x2f6ab860]]}
  lc.input_tensor.conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x3f6c73e0], [4, 0x7f70fdc0]]}
  lc.input_tensor.conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x3f6b9d40], [1, 0x7f6c8c60]]}
  lc.input_tensor.conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3f68e7e0]]}
  lc.input_tensor.conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3f75b060]]}
  lc.input_tensor.conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x3f72d060]]}
  lc.input_tensor.conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x3f6dbc20]]}
  input_1_add_1196:                                                                          {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f836860], [5, 0x7f8388e0]]}
  input_1_add_1196_fork_clone68:                                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 3], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7da1a0], [4, 0x7f820b00]]}
  lc.input_tensor.concatenate_1209.dc.sparse_matmul.4.0:                                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x7f693160], [2, 0x3f6a3460], [2, 0x7f6aad40], [3, 0x3f6b5420]]}
  lc.input_tensor.concatenate_1209.dc.sparse_matmul.4.1:                                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x3f6ea8c0], [3, 0x7f6eb440], [4, 0x3f6f8de0], [4, 0x7f7417c0]]}
  input_1_add_1212_fork_clone20:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7aa400], [5, 0x3f7c19c0], [5, 0x7f7c4260], [0, 0x2f710e20]]}
  lc.input_tensor.avg_pool2d_1225.dc.reduce_avg.2.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f764e60]]}

  # epoch_to_epoch
  e2e__fused_op_1_0:                                                                         {input: _fused_op_1, type: queue, entries: 1, grid_size: [4, 1], t: 49, mblock: [1, 1], ublock: [2, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fd02b20], [3, 0x3fd02b20], [3, 0x7fd02b20], [4, 0x3fd02b20]]}
  e2e__fused_op_5_0:                                                                         {input: _fused_op_5, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [14, 1], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f9c4d80], [0, 0x7f9cc760], [1, 0x3f9ccf80]]}
  e2e__fused_op_7_0:                                                                         {input: _fused_op_7, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [14, 2], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fc2c620]]}
  e2e_max_pool2d_28.dc.reduce_max.6_0:                                                       {input: max_pool2d_28.dc.reduce_max.6, type: queue, entries: 1, grid_size: [1, 1], t: 14, mblock: [1, 3], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fd02b20]]}
  e2e_concatenate_130.dc.sparse_matmul.6.lc2_0:                                              {input: concatenate_130.dc.sparse_matmul.6.lc2, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 14], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fa99a20], [5, 0x3faac6c0], [5, 0x7faac6c0], [0, 0x2f9f69e0], [0, 0x7f9fe3c0], [1, 0x3f9febe0], [1, 0x7fa06de0], [2, 0x3fa14940]]}
  e2e__fused_op_2_0:                                                                         {input: _fused_op_2, type: queue, entries: 1, grid_size: [1, 1], t: 14, mblock: [1, 1], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fa7aa60]]}
  e2e__fused_op_4_0:                                                                         {input: _fused_op_4, type: queue, entries: 1, grid_size: [1, 1], t: 14, mblock: [1, 1], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7fa49e80]]}
  e2e__fused_op_6_0:                                                                         {input: _fused_op_6, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [7, 1], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f884520], [3, 0x3f88df80]]}
  e2e__fused_op_8_0:                                                                         {input: _fused_op_8, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [14, 1], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fa465a0]]}
  e2e_conv2d_131.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_131.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 2], t: 98, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fcf0ea0], [5, 0x7fcf0ea0]]}
  e2e_conv2d_131.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_131.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 2], t: 98, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3fc9f280], [4, 0x7fce12c0]]}
  e2e_conv2d_161.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_161.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f701120], [1, 0x3f7041e0], [1, 0x7f712d80], [2, 0x3f722c80], [2, 0x7f72a460]]}
  e2e_conv2d_161.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_161.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f6fe860], [1, 0x3f701920], [1, 0x7f7104c0], [2, 0x3f7203c0], [2, 0x7f727ba0]]}
  e2e_conv2d_161.dc.conv2d.5.dc.matmul.11_0:                                                 {input: conv2d_161.dc.conv2d.5.dc.matmul.11, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f6f24a0], [1, 0x7f701040], [2, 0x3f710f40], [2, 0x7f718720], [3, 0x3f722ba0]]}
  e2e_max_pool2d_160.dc.reduce_max.6_0:                                                      {input: max_pool2d_160.dc.reduce_max.6, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 1], ublock: [1, 6], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f8338a0], [3, 0x7f8340c0], [4, 0x3f840be0], [4, 0x7f886d20], [5, 0x3f89ca80]]}
  e2e__fused_op_16_0:                                                                        {input: _fused_op_16, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7a8da0]]}
  e2e__fused_op_15_0:                                                                        {input: _fused_op_15, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f8f79a0]]}
  e2e__fused_op_13_0:                                                                        {input: _fused_op_13, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f97fc40]]}
  e2e__fused_op_10_0:                                                                        {input: _fused_op_10, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7cf7a0]]}
  e2e__fused_op_12_0:                                                                        {input: _fused_op_12, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f7a95e0]]}
  e2e__fused_op_14_0:                                                                        {input: _fused_op_14, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f839140]]}
  e2e__fused_op_18_0:                                                                        {input: _fused_op_18, type: queue, entries: 1, grid_size: [1, 1], t: 5, mblock: [1, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7f3000]]}
  e2e__fused_op_20_0:                                                                        {input: _fused_op_20, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f7e64e0]]}
  e2e_concatenate_349.dc.concatenate.2_0:                                                    {input: concatenate_349.dc.concatenate.2, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3fade320]]}
  e2e__fused_op_21_0:                                                                        {input: _fused_op_21, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fa2b6a0]]}
  e2e__fused_op_17_0:                                                                        {input: _fused_op_17, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 4], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7fade320]]}
  e2e__fused_op_22_0:                                                                        {input: _fused_op_22, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7d3040]]}
  e2e_conv2d_394.dc.matmul.8_0:                                                              {input: conv2d_394.dc.matmul.8, type: queue, entries: 1, grid_size: [1, 2], t: 5, mblock: [1, 4], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2fa28640], [0, 0x7fa30020]]}
  e2e_conv2d_408.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_408.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 5, mblock: [1, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f85da60]]}
  e2e_concatenate_451.dc.sparse_matmul.5.lc2_0:                                              {input: concatenate_451.dc.sparse_matmul.5.lc2, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [11, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f9af7a0], [0, 0x2f8fa2e0], [0, 0x7f903d40], [1, 0x3f905dc0], [1, 0x7f9120c0]]}
  e2e_conv2d_452.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_452.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 5, mblock: [1, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f9d20a0]]}
  e2e_concatenate_509.dc.sparse_matmul.6.lc2_0:                                              {input: concatenate_509.dc.sparse_matmul.6.lc2, type: queue, entries: 1, grid_size: [1, 5], t: 1, mblock: [13, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f954980], [0, 0x7f95e3e0], [1, 0x3f95ec00], [1, 0x7f96af00], [2, 0x3f978a60]]}
  e2e_conv2d_510.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_510.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 5], t: 25, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f80be80], [3, 0x7f80c6a0], [4, 0x3f8191c0], [4, 0x7f85f300], [5, 0x3f875060]]}
  e2e__fused_op_33_0:                                                                        {input: _fused_op_33, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7eb6a0], [1, 0x7f7f9200], [2, 0x3f8085c0], [2, 0x7f80e740], [3, 0x3f8189c0]]}
  e2e_conv2d_524.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_524.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f73d2c0], [4, 0x7f785980], [5, 0x3f79cf40], [5, 0x7f79f7e0], [0, 0x2f6ec3a0]]}
  e2e__fused_op_26_0:                                                                        {input: _fused_op_26, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f825d00]]}
  e2e__fused_op_28_0:                                                                        {input: _fused_op_28, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f883c20]]}
  e2e__fused_op_30_0:                                                                        {input: _fused_op_30, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f84eea0]]}
  e2e__fused_op_32_0:                                                                        {input: _fused_op_32, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f801c00]]}
  e2e__fused_op_34_0:                                                                        {input: _fused_op_34, type: queue, entries: 1, grid_size: [1, 1], t: 5, mblock: [1, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7c6500]]}
  e2e__fused_op_36_0:                                                                        {input: _fused_op_36, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7b89a0]]}
  e2e__fused_op_40_0:                                                                        {input: _fused_op_40, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f7f2800]]}
  e2e__fused_op_39_0:                                                                        {input: _fused_op_39, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f88a6c0]]}
  e2e__fused_op_37_0:                                                                        {input: _fused_op_37, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f9be3c0]]}
  e2e__fused_op_38_0:                                                                        {input: _fused_op_38, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [5, 1], ublock: [5, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7c54e0]]}
  e2e_conv2d_641.dc.matmul.8_0:                                                              {input: conv2d_641.dc.matmul.8, type: queue, entries: 1, grid_size: [1, 5], t: 5, mblock: [1, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f8b69c0], [3, 0x3f8c0420], [3, 0x7f8c0420], [4, 0x3f8cbf00], [4, 0x7f911000]]}
  e2e_conv2d_700.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_700.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f88e6e0]]}
  e2e_conv2d_700.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_700.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f878980]]}
  e2e_conv2d_700.dc.conv2d.5.dc.matmul.11_0:                                                 {input: conv2d_700.dc.conv2d.5.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f81b280]]}
  e2e_max_pool2d_655.dc.reduce_max.6_0:                                                      {input: max_pool2d_655.dc.reduce_max.6, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 2], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f725b60], [1, 0x7f734700], [2, 0x3f743ac0], [2, 0x7f74ac80], [3, 0x3f754f00], [3, 0x7f755720], [4, 0x3f762240]]}
  e2e_conv2d_758.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_758.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 2], t: 7, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f7c33c0], [4, 0x7f80a540]]}
  e2e_conv2d_758.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_758.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 2], t: 7, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f8212e0], [5, 0x7f823360]]}
  e2e_conv2d_758.dc.conv2d.5.dc.matmul.11_0:                                                 {input: conv2d_758.dc.conv2d.5.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 2], t: 7, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f76eee0], [0, 0x7f77b1e0]]}
  e2e__fused_op_42_0:                                                                        {input: _fused_op_42, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f7622c0]]}
  e2e__fused_op_44_0:                                                                        {input: _fused_op_44, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f75b100]]}
  e2e__fused_op_46_0:                                                                        {input: _fused_op_46, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f74bd40]]}
  e2e__fused_op_48_0:                                                                        {input: _fused_op_48, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f73d1a0]]}
  e2e__fused_op_50_0:                                                                        {input: _fused_op_50, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f73a0e0]]}
  e2e__fused_op_52_0:                                                                        {input: _fused_op_52, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f72d5c0]]}
  e2e_conv2d_816.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_816.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f825500]]}
  e2e_conv2d_816.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_816.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7f5120]]}
  e2e_conv2d_816.dc.conv2d.5.dc.matmul.11_0:                                                 {input: conv2d_816.dc.conv2d.5.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7f81e0]]}
  e2e__fused_op_49_0:                                                                        {input: _fused_op_49, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f82a6c0]]}
  e2e_concatenate_873.dc.sparse_matmul.7.lc2_0:                                              {input: concatenate_873.dc.sparse_matmul.7.lc2, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [11, 1], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f9a55a0], [1, 0x3f9a5dc0]]}
  e2e_concatenate_888.dc.concatenate.1.dc.concatenate.4_transpose_nop_38993_0:               {input: concatenate_888.dc.concatenate.1.dc.concatenate.4_transpose_nop_38993, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 7], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f87fc00]]}
  e2e__fused_op_54_0:                                                                        {input: _fused_op_54, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7c0a00]]}
  e2e__fused_op_56_0:                                                                        {input: _fused_op_56, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7d7fc0]]}
  e2e_conv2d_874.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_874.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [7, 2], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f732020], [3, 0x7f732b80], [4, 0x3f73fb80], [4, 0x7f788240], [5, 0x3f79f800], [5, 0x7f7a20a0], [0, 0x2f6eec60], [0, 0x7f6fbfa0], [1, 0x3f6ff060], [1, 0x7f70dc00], [2, 0x3f71db00], [2, 0x7f7252e0], [3, 0x3f72f760], [3, 0x7f7302c0]]}
  e2e_conv2d_874.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_874.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [7, 2], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f6f96e0], [1, 0x3f6fc7a0], [1, 0x7f70b340], [2, 0x3f71b240], [2, 0x7f722a20], [3, 0x3f72cea0], [3, 0x7f72da00], [4, 0x3f73aa00], [4, 0x7f7830c0], [5, 0x3f79a680], [5, 0x7f79cf20], [0, 0x2f6e9ae0], [0, 0x7f6f6e20], [1, 0x3f6f9ee0]]}
  e2e_conv2d_889.dc.matmul.8_transpose_nop_39114_0:                                          {input: conv2d_889.dc.matmul.8_transpose_nop_39114, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f78ce40], [2, 0x3f79c200], [2, 0x7f7a2380], [3, 0x3f7ac600], [3, 0x7f7ace20], [4, 0x3f7b9940], [4, 0x7f800ac0]]}
  e2e_conv2d_889.dc.matmul.8_0:                                                              {input: conv2d_889.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 3], t: 1, mblock: [1, 1], ublock: [1, 5], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f71d8a0], [3, 0x3f727d20], [3, 0x7f728880], [4, 0x3f735880], [4, 0x7f77df40], [5, 0x3f795500], [5, 0x7f797da0], [0, 0x2f6e4960], [0, 0x7f6f1ca0], [1, 0x3f6f4d60], [1, 0x7f703900], [2, 0x3f713800], [2, 0x7f71afe0], [3, 0x3f725460], [3, 0x7f725fc0], [4, 0x3f732fc0], [4, 0x7f77b680], [5, 0x3f792c40], [5, 0x7f7954e0], [0, 0x2f6e20a0], [0, 0x7f6ef3e0]]}
  e2e_conv2d_903.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_903.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x2f6af960], [0, 0x7f6bce00], [1, 0x3f6bfec0], [1, 0x7f6cede0], [2, 0x3f6dece0], [2, 0x7f6e64c0], [3, 0x3f6f0a40]]}
  e2e_conv2d_903.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_903.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f6e1e00], [2, 0x7f6e95e0], [3, 0x3f6f3b60], [3, 0x7f6f46c0], [4, 0x3f702060], [4, 0x7f74aa40], [5, 0x3f762260]]}
  e2e_concatenate_946.dc.sparse_matmul.5.lc2_0:                                              {input: concatenate_946.dc.sparse_matmul.5.lc2, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [5, 1], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f841c60], [2, 0x7f847de0], [3, 0x3f852060], [3, 0x7f852880]]}
  e2e_conv2d_947.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_947.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 5], t: 7, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f718fe0], [1, 0x7f727b80], [2, 0x3f737a80], [2, 0x7f73ec40], [3, 0x3f748ec0]]}
  e2e_conv2d_947.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_947.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 5], t: 7, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f7a21c0], [5, 0x3f7b9780], [5, 0x7f7bc020], [0, 0x2f708be0], [0, 0x7f715f20]]}
  e2e_concatenate_1033.dc.sparse_matmul.5.lc2_0:                                             {input: concatenate_1033.dc.sparse_matmul.5.lc2, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [9, 1], ublock: [1, 7], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f93e3e0]]}
  e2e_conv2d_1034.dc.conv2d.1.dc.matmul.11_0:                                                {input: conv2d_1034.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f788d60]]}
  e2e_conv2d_1034.dc.conv2d.3.dc.matmul.11_0:                                                {input: conv2d_1034.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 1], t: 7, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f785ca0]]}
  e2e__fused_op_58_0:                                                                        {input: _fused_op_58, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f74b560]]}
  e2e__fused_op_60_0:                                                                        {input: _fused_op_60, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7f7484a0]]}
  e2e__fused_op_62_0:                                                                        {input: _fused_op_62, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f7612e0]]}
  e2e__fused_op_64_0:                                                                        {input: _fused_op_64, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7f7efe00]]}
  e2e__fused_op_66_0:                                                                        {input: _fused_op_66, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7ed560]]}
  e2e_concatenate_1135.dc.concatenate.0.dc.concatenate.6_0:                                  {input: concatenate_1135.dc.concatenate.0.dc.concatenate.6, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fa3d320]]}
  e2e__fused_op_65_0:                                                                        {input: _fused_op_65, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f999a40]]}
  e2e_concatenate_1120.dc.concatenate.4_0:                                                   {input: concatenate_1120.dc.concatenate.4, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 33], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7fd44b60]]}
  e2e__fused_op_70_0:                                                                        {input: _fused_op_70, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f777860]]}
  e2e__fused_op_72_0:                                                                        {input: _fused_op_72, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [7, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f7706a0]]}
  e2e_max_pool2d_1150.dc.reduce_max.6_0:                                                     {input: max_pool2d_1150.dc.reduce_max.6, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 23], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f84c740]]}
  e2e_conv2d_1151.dc.conv2d.5.dc.matmul.11_0:                                                {input: conv2d_1151.dc.conv2d.5.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 1], ublock: [2, 2], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7f6eaa00], [2, 0x3f6fa900], [2, 0x7f7020e0], [3, 0x3f70c560]]}
  e2e_concatenate_1194.dc.sparse_matmul.5.lc2_0:                                             {input: concatenate_1194.dc.sparse_matmul.5.lc2, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [9, 2], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x3f9f8220]]}
  e2e__fused_op_74_0:                                                                        {input: _fused_op_74, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x3f788cc0]]}
  e2e__fused_op_76_0:                                                                        {input: _fused_op_76, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 8], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f77ea40]]}
  e2e_conv2d_1195.dc.conv2d.1.dc.matmul.11_0:                                                {input: conv2d_1195.dc.conv2d.1.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 8], t: 2, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f715f00], [1, 0x7f724aa0], [2, 0x3f7349a0], [2, 0x7f73bb60], [3, 0x3f745de0], [3, 0x7f746940], [4, 0x3f753940], [4, 0x7f79c000]]}
  e2e_conv2d_1195.dc.conv2d.3.dc.matmul.11_0:                                                {input: conv2d_1195.dc.conv2d.3.dc.matmul.11, type: queue, entries: 1, grid_size: [1, 8], t: 2, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7b35c0], [5, 0x7f7b5e60], [0, 0x2f702a20], [0, 0x7f70fd60], [1, 0x3f712e20], [1, 0x7f7219c0], [2, 0x3f7318c0], [2, 0x7f738a80]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 1
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, input_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [4, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.0.conv.weight],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 56, hstack: 2, vstack: 28],
         attributes: {kernel_broadcast: {input_1: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, input_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [14, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [4, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.0.conv.weight_fork_clone4676],
         t: 2, mblock: [7, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 392, hstack: 2, vstack: 196],
         attributes: {kernel_broadcast: {input_1: 4}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 4], grid_size: [2, 2], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11, conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11, input_1_add_1_fork_clone1886], overlay_size: 131072,
         t: 1, mblock: [28, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 392}], input_1_tms: [vstack: 2], input_0_tms: [vstack: 14],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 1}}}
    conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1], overlay_size: 131072,
         t: 49, mblock: [2, 1], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_14.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [4, 1], inputs: [conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.1.conv.weight_fork_clone3935],
         t: 49, mblock: [1, 1], ublock: [2, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 24, hstack: 3, vstack: 8],
         attributes: {kernel_broadcast: {input_1: 18}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1], overlay_size: 131072,
         t: 49, mblock: [2, 1], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_14.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [4, 1], inputs: [conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.1.conv.weight],
         t: 49, mblock: [1, 1], ublock: [2, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 24, hstack: 3, vstack: 8],
         attributes: {kernel_broadcast: {input_1: 18}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [4, 1], inputs: [lc.input_tensor.conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1], overlay_size: 131072,
         t: 49, mblock: [2, 1], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_14.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [4, 1], inputs: [conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.1.conv.weight_fork_clone3933],
         t: 49, mblock: [1, 1], ublock: [2, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 24, hstack: 3, vstack: 8],
         attributes: {kernel_broadcast: {input_1: 18}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_1: {type: fused_op, grid_loc: [4, 2], grid_size: [4, 1], inputs: [conv2d_14.dc.conv2d.1.dc.matmul.11, conv2d_14.dc.conv2d.3.dc.matmul.11, conv2d_14.dc.conv2d.5.dc.matmul.11, input_1_add_15_fork_clone1830],
         t: 49, mblock: [1, 1], ublock: [2, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 392}, vslice: 49],
         attributes: {fused_op_id: 1, kernel_broadcast: {input_3: 3}}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 1
    max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.0, e2e__fused_op_1_0, lc.input_tensor.max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.1],
         t: 14, mblock: [9, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 42, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 49],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 28, num_index_tiles: 2, num_sparse_tiles: 39, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 14}}
    max_pool2d_28.dc.reduce_max.6: {type: reduce, grid_loc: [0, 1], grid_size: [1, 1], inputs: [max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 14, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 63, hstack: 9, vstack: 7, hslice: 9],
         attributes: {dim: z, type: max, z: 9}}
    conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_28.dc.reduce_max.6, lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 7}}
    conv2d_29.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.0.conv.weight_fork_clone3954],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 9}, m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_28.dc.reduce_max.6, lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 7}}
    conv2d_29.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.0.conv.weight],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 9}, m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_28.dc.reduce_max.6, lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 14],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 7}}
    conv2d_29.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.0.conv.weight_fork_clone3952],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 9}, m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_2: {type: fused_op, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.1.dc.matmul.11, conv2d_29.dc.conv2d.3.dc.matmul.11, conv2d_29.dc.conv2d.5.dc.matmul.11, input_1_add_30_fork_clone1718],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}, vslice: 14],
         attributes: {fused_op_id: 2, kernel_broadcast: {input_3: 1}}}
    concatenate_43.dc.concatenate.1: {type: splice, grid_loc: [1, 1], grid_size: [1, 1], inputs: [_fused_op_2, max_pool2d_28.dc.reduce_max.6],
         t: 14, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 399], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 1, 1], input1: [0, 3, 3]}}
    concatenate_43.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 7], inputs: [lc.input_tensor.concatenate_43.dc.sparse_matmul.4.0, concatenate_43.dc.concatenate.1, lc.input_tensor.concatenate_43.dc.sparse_matmul.4.1],
         t: 1, mblock: [4, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 7}], input_1_tms: [transpose, hstack: 14], input_0_tms: [broadcast: {c: 7}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_43.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_44.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.1.conv.weight_fork_clone3962],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 24}, m_k: 1, min_buffer_input: 0, u_kt: 12}}
    conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_43.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_44.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.1.conv.weight],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 24}, m_k: 1, min_buffer_input: 0, u_kt: 12}}
    conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_43.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_44.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.1.conv.weight_fork_clone3960],
         t: 14, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 24}, m_k: 1, min_buffer_input: 0, u_kt: 12}}
    _fused_op_3: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_44.dc.conv2d.1.dc.matmul.11, conv2d_44.dc.conv2d.3.dc.matmul.11, conv2d_44.dc.conv2d.5.dc.matmul.11, input_1_add_45_fork_clone1811],
         t: 1, mblock: [14, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}], input_2_tms: [vstack: 14], input_1_tms: [vstack: 14], input_0_tms: [vstack: 14],
         attributes: {fused_op_id: 3, kernel_broadcast: {input_3: 14}}}
    conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_3, lc.input_tensor.conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_58.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.2.conv.weight_fork_clone3970],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 6}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_3, lc.input_tensor.conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_58.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.2.conv.weight],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 6}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_3, lc.input_tensor.conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 98}}
    conv2d_58.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.2.conv.weight_fork_clone3968],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 6}, m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_4: {type: fused_op, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_58.dc.conv2d.1.dc.matmul.11, conv2d_58.dc.conv2d.3.dc.matmul.11, conv2d_58.dc.conv2d.5.dc.matmul.11, input_1_add_59_fork_clone1723],
         t: 14, mblock: [1, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}, vslice: 14],
         attributes: {fused_op_id: 2, kernel_broadcast: {input_3: 1}}}
    buffer_4_14437_14546: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [max_pool2d_28.dc.reduce_max.6],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_14437_14546: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_4_14437_14546],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_14437_14546: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [buffer_3_14437_14546],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_14437_14546: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_2_14437_14546],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_14437_14546: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [buffer_1_14437_14546],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_72.dc.concatenate.2: {type: splice, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_4, _fused_op_3, buffer_0_14437_14546],
         t: 14, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 357, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 14],
         attributes: {input0: [0, 1, 1], input1: [0, 2, 2], input2: [0, 3, 3]}}
    concatenate_72.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 7], inputs: [lc.input_tensor.concatenate_72.dc.sparse_matmul.5.0, concatenate_72.dc.concatenate.2, lc.input_tensor.concatenate_72.dc.sparse_matmul.5.1],
         t: 1, mblock: [5, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 7}], input_1_tms: [transpose, hstack: 14], input_0_tms: [broadcast: {c: 7}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 6}}
    conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_72.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_73.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.3.conv.weight_fork_clone3978],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 45}, m_k: 1, min_buffer_input: 0, u_kt: 15}}
    conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_72.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_73.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.3.conv.weight],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 45}, m_k: 1, min_buffer_input: 0, u_kt: 15}}
    conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_72.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 14, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_73.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.3.conv.weight_fork_clone3976],
         t: 14, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 14}, hslice: 14], input_0_tms: [vslice: 21, hstack: 3, vstack: 7],
         attributes: {kernel_broadcast: {input_1: 45}, m_k: 1, min_buffer_input: 0, u_kt: 15}}
    _fused_op_5: {type: fused_op, grid_loc: [6, 4], grid_size: [1, 3], inputs: [conv2d_73.dc.conv2d.1.dc.matmul.11, conv2d_73.dc.conv2d.3.dc.matmul.11, conv2d_73.dc.conv2d.5.dc.matmul.11, input_1_add_74_fork_clone1817],
         t: 1, mblock: [14, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}], input_2_tms: [vstack: 14], input_1_tms: [vstack: 14], input_0_tms: [vstack: 14],
         attributes: {fused_op_id: 5, kernel_broadcast: {input_3: 1}}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 1
    conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_5_0, lc.input_tensor.conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 42, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_87.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.4.conv.weight_fork_clone3986],
         t: 1, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_5_0, lc.input_tensor.conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 42, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_87.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.4.conv.weight],
         t: 1, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_5_0, lc.input_tensor.conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 42, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_87.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.4.conv.weight_fork_clone3984],
         t: 1, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_6: {type: fused_op, grid_loc: [0, 6], grid_size: [2, 1], inputs: [conv2d_87.dc.conv2d.1.dc.matmul.11, conv2d_87.dc.conv2d.3.dc.matmul.11, conv2d_87.dc.conv2d.5.dc.matmul.11, input_1_add_88_fork_clone1728],
         t: 1, mblock: [7, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}],
         attributes: {fused_op_id: 6, kernel_broadcast: {input_3: 1}}}
    concatenate_101.dc.concatenate.2: {type: splice, grid_loc: [0, 7], grid_size: [2, 1], inputs: [_fused_op_6, e2e__fused_op_5_0],
         t: 1, mblock: [7, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 7, 7], input1: [0, 21, 21]}}
    concatenate_101.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_101.dc.sparse_matmul.5.0, concatenate_101.dc.concatenate.2, lc.input_tensor.concatenate_101.dc.sparse_matmul.5.1],
         t: 1, mblock: [1, 14], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_101.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 49}}
    conv2d_102.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 3], grid_size: [2, 1], inputs: [conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.5.conv.weight_fork_clone3994],
         t: 7, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 42, hstack: 3, vstack: 14],
         attributes: {kernel_broadcast: {input_1: 18}, m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_101.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 49}}
    conv2d_102.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.5.conv.weight],
         t: 7, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 42, hstack: 3, vstack: 14],
         attributes: {kernel_broadcast: {input_1: 18}, m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_101.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 49}}
    conv2d_102.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [2, 1], inputs: [conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.5.conv.weight_fork_clone3992],
         t: 7, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 42, hstack: 3, vstack: 14],
         attributes: {kernel_broadcast: {input_1: 18}, m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_7: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_102.dc.conv2d.1.dc.matmul.11, conv2d_102.dc.conv2d.3.dc.matmul.11, conv2d_102.dc.conv2d.5.dc.matmul.11, input_1_add_103_fork_clone1823],
         t: 1, mblock: [14, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}], input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 3, kernel_broadcast: {input_3: 14}}}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 1
    conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_7_0, lc.input_tensor.conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 49}}
    conv2d_116.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.6.conv.weight_fork_clone4002],
         t: 1, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_7_0, lc.input_tensor.conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 49}}
    conv2d_116.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.6.conv.weight],
         t: 1, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_7_0, lc.input_tensor.conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 49}}
    conv2d_116.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.6.conv.weight_fork_clone4000],
         t: 1, mblock: [2, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_8: {type: fused_op, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_116.dc.conv2d.1.dc.matmul.11, conv2d_116.dc.conv2d.3.dc.matmul.11, conv2d_116.dc.conv2d.5.dc.matmul.11, input_1_add_117_fork_clone1733],
         t: 1, mblock: [14, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}],
         attributes: {fused_op_id: 5, kernel_broadcast: {input_3: 1}}}
    concatenate_130.dc.concatenate.3: {type: splice, grid_loc: [0, 7], grid_size: [7, 1], inputs: [_fused_op_8, e2e__fused_op_7_0, e2e__fused_op_5_0, e2e_max_pool2d_28.dc.reduce_max.6_0],
         t: 1, mblock: [2, 9], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 14, 14, 14], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [vstack: 14],
         attributes: {input0: [0, 2, 2], input1: [0, 4, 4], input2: [0, 6, 6], input3: [0, 6, 6]}}
    concatenate_130.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [8, 1], inputs: [lc.input_tensor.concatenate_130.dc.sparse_matmul.6.0, concatenate_130.dc.concatenate.3, lc.input_tensor.concatenate_130.dc.sparse_matmul.6.1],
         t: 1, mblock: [1, 14], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 9, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_130.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1], overlay_size: 131072,
         t: 98, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_131.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 2], inputs: [conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.7.conv.weight],
         t: 98, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 98}, hslice: 98], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 48}, m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_130.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1], overlay_size: 131072,
         t: 98, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_131.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 4], grid_size: [1, 2], inputs: [conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.7.conv.weight_fork_clone4008],
         t: 98, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 98}, hslice: 98], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 48}, m_k: 1, min_buffer_input: 0, u_kt: 24}}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 1
    conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_130.dc.sparse_matmul.6.lc2_0, lc.input_tensor.conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 49, mblock: [3, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_131.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [2, 2], inputs: [conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.3.layers.7.conv.weight_fork_clone4010],
         t: 49, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 6, hstack: 3, vstack: 2],
         attributes: {kernel_broadcast: {input_1: 48}, m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_9: {type: fused_op, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_conv2d_131.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_131.dc.conv2d.3.dc.matmul.11_0, conv2d_131.dc.conv2d.5.dc.matmul.11, input_1_add_132_fork_clone1738],
         t: 49, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 98}, vslice: 49], input_1_tms: [vstack: 2], input_0_tms: [vstack: 2],
         attributes: {fused_op_id: 9, kernel_broadcast: {input_3: 4}}}
    concatenate_145.dc.concatenate.5: {type: splice, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e__fused_op_2_0, e2e__fused_op_4_0, e2e__fused_op_6_0, e2e__fused_op_8_0, _fused_op_9],
         t: 49, mblock: [1, 8], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [12, 12, 12, 12, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [vslice: 49], input_2_tms: [vslice: 49], input_1_tms: [vstack: 14, vslice: 49], input_0_tms: [vstack: 14, vslice: 49],
         attributes: {input0: [0, 1, 1], input1: [0, 1, 1], input2: [0, 1, 1], input3: [0, 1, 1], input4: [0, 4, 4]}}
    concatenate_145.dc.sparse_matmul.8.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_145.dc.sparse_matmul.8.0, concatenate_145.dc.concatenate.5, lc.input_tensor.concatenate_145.dc.sparse_matmul.8.1],
         t: 49, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 49, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 4}}
    conv2d_146.dc.matmul.8_transpose_nop_37985: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [concatenate_145.dc.sparse_matmul.8.lc2],
         t: 49, mblock: [1, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    conv2d_146.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [conv2d_146.dc.matmul.8_transpose_nop_37985, base.4.conv.weight, input_1_add_147_fork_clone1574],
         t: 49, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 49], input_1_tms: [broadcast: {c: 49}, hslice: 49],
         attributes: {bias: true, kernel_broadcast: {input_2: 3, input_1: 21}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 7}}
    max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [5, 1], inputs: [lc.input_tensor.max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.0, conv2d_146.dc.matmul.8, lc.input_tensor.max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.1],
         t: 1, mblock: [20, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 49],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 49, num_index_tiles: 1, num_sparse_tiles: 45, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 2}}
    max_pool2d_160.dc.reduce_max.6: {type: reduce, grid_loc: [2, 3], grid_size: [5, 1], inputs: [max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2],
         t: 1, mblock: [5, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 4],
         attributes: {dim: z, type: max, z: 4}}
    conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_160.dc.reduce_max.6, lc.input_tensor.conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_161.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [5, 1], inputs: [conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.0.conv.weight_fork_clone4042],
         t: 1, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_160.dc.reduce_max.6, lc.input_tensor.conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_161.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [5, 1], inputs: [conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.0.conv.weight],
         t: 1, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_160.dc.reduce_max.6, lc.input_tensor.conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_161.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [5, 1], inputs: [conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.0.conv.weight_fork_clone4040],
         t: 1, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 1
    _fused_op_10: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_161.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_161.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_161.dc.conv2d.5.dc.matmul.11_0, input_1_add_162, input_1_add_162_fork_clone1348],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [15, 15, 15, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    concatenate_175.dc.concatenate.1: {type: splice, grid_loc: [0, 1], grid_size: [1, 1], inputs: [_fused_op_10, e2e_max_pool2d_160.dc.reduce_max.6_0],
         t: 1, mblock: [5, 7], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 45], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 30, 30]}}
    concatenate_175.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_175.dc.sparse_matmul.4.0, concatenate_175.dc.concatenate.1, lc.input_tensor.concatenate_175.dc.sparse_matmul.4.1],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_175.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 245, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_176.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.1.conv.weight_fork_clone4050],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [385, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 42}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    buffer_1_14799_14816: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [concatenate_175.dc.sparse_matmul.4.lc2],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [119], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    buffer_0_14799_14816: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [buffer_1_14799_14816],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [119], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_14799_14816, lc.input_tensor.conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 245, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_176.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.1.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [385, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 42}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_175.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_176.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [5, 1], inputs: [conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.1.conv.weight_fork_clone4048],
         t: 1, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    _fused_op_11: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_176.dc.conv2d.1.dc.matmul.11, conv2d_176.dc.conv2d.3.dc.matmul.11, conv2d_176.dc.conv2d.5.dc.matmul.11, input_1_add_177, input_1_add_177_fork_clone1531],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 225, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 11}}
    conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_11, lc.input_tensor.conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_190.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.2.conv.weight_fork_clone4058],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_11, lc.input_tensor.conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_190.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.2.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_11, lc.input_tensor.conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_190.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.2.conv.weight_fork_clone4056],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_12: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_190.dc.conv2d.1.dc.matmul.11, conv2d_190.dc.conv2d.3.dc.matmul.11, conv2d_190.dc.conv2d.5.dc.matmul.11, input_1_add_191, input_1_add_191_fork_clone1353],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17766_14879: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [_fused_op_11],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [365], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_204.dc.concatenate.2: {type: splice, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_12, buffer_0_17766_14879, e2e_max_pool2d_160.dc.reduce_max.6_0],
         t: 1, mblock: [5, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 45], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 10, 10], input2: [0, 30, 30]}}
    concatenate_204.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_204.dc.sparse_matmul.5.0, concatenate_204.dc.concatenate.2, lc.input_tensor.concatenate_204.dc.sparse_matmul.5.1],
         t: 1, mblock: [1, 25], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 9, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    buffer_0_14883_14890: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [concatenate_204.dc.sparse_matmul.5.lc2],
         t: 1, mblock: [1, 25], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [64], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_14883_14890, lc.input_tensor.conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 200, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_205.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.3.conv.weight_fork_clone4066],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [360, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 72}, m_k: 1, min_buffer_input: 0, u_kt: 24}}
    buffer_0_14883_17890: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [concatenate_204.dc.sparse_matmul.5.lc2],
         t: 1, mblock: [1, 25], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [64], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    buffer_0_14883_14900: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_0_14883_17890],
         t: 1, mblock: [1, 25], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [64], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_14883_14900, lc.input_tensor.conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 200, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_205.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.3.conv.weight],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [360, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 72}, m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_204.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_205.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 7], grid_size: [5, 1], inputs: [conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.3.conv.weight_fork_clone4064],
         t: 1, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_13: {type: fused_op, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_205.dc.conv2d.1.dc.matmul.11, conv2d_205.dc.conv2d.3.dc.matmul.11, conv2d_205.dc.conv2d.5.dc.matmul.11, input_1_add_206, input_1_add_206_fork_clone1537],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 125, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 13}}
    conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_13, lc.input_tensor.conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_219.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.4.conv.weight_fork_clone4074],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_13, lc.input_tensor.conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_219.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.4.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_13, lc.input_tensor.conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_219.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.4.conv.weight_fork_clone4072],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_14: {type: fused_op, grid_loc: [6, 2], grid_size: [1, 1], inputs: [conv2d_219.dc.conv2d.1.dc.matmul.11, conv2d_219.dc.conv2d.3.dc.matmul.11, conv2d_219.dc.conv2d.5.dc.matmul.11, input_1_add_220, input_1_add_220_fork_clone1358],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17768_14963: {type: nop, grid_loc: [6, 3], grid_size: [1, 1], inputs: [_fused_op_13],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_233.dc.concatenate.2: {type: splice, grid_loc: [6, 4], grid_size: [1, 1], inputs: [_fused_op_14, buffer_0_17768_14963],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 255], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 15, 15]}}
    concatenate_233.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_233.dc.sparse_matmul.5.0, concatenate_233.dc.concatenate.2, lc.input_tensor.concatenate_233.dc.sparse_matmul.5.1],
         t: 1, mblock: [3, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_233.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_234.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.5.conv.weight_fork_clone4082],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_233.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_234.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.5.conv.weight],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_233.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_234.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.5.conv.weight_fork_clone4080],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_15: {type: fused_op, grid_loc: [8, 0], grid_size: [1, 1], inputs: [conv2d_234.dc.conv2d.1.dc.matmul.11, conv2d_234.dc.conv2d.3.dc.matmul.11, conv2d_234.dc.conv2d.5.dc.matmul.11, input_1_add_235, input_1_add_235_fork_clone1543],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 11}}
    conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_15, lc.input_tensor.conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_248.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.6.conv.weight_fork_clone4090],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_15, lc.input_tensor.conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_248.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.6.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_15, lc.input_tensor.conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_248.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.6.conv.weight_fork_clone4088],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_16: {type: fused_op, grid_loc: [8, 7], grid_size: [1, 1], inputs: [conv2d_248.dc.conv2d.1.dc.matmul.11, conv2d_248.dc.conv2d.3.dc.matmul.11, conv2d_248.dc.conv2d.5.dc.matmul.11, input_1_add_249, input_1_add_249_fork_clone1363],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 1
    concatenate_262.dc.concatenate.3: {type: splice, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e__fused_op_16_0, e2e__fused_op_15_0, e2e__fused_op_13_0, e2e_max_pool2d_160.dc.reduce_max.6_0],
         t: 5, mblock: [1, 12], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [10, 10, 10, 10], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [vslice: 5], input_2_tms: [vslice: 5], input_1_tms: [vslice: 5], input_0_tms: [vslice: 5],
         attributes: {input0: [0, 1, 1], input1: [0, 2, 2], input2: [0, 3, 3], input3: [0, 6, 6]}}
    concatenate_262.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 5], inputs: [lc.input_tensor.concatenate_262.dc.sparse_matmul.6.0, concatenate_262.dc.concatenate.3, lc.input_tensor.concatenate_262.dc.sparse_matmul.6.1],
         t: 1, mblock: [11, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 5}], input_1_tms: [transpose, hstack: 5], input_0_tms: [broadcast: {c: 5}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 12}}
    conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_262.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 11], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_263.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 4], grid_size: [1, 2], inputs: [conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.7.conv.weight_fork_clone4098],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 66}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 11}}
    conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_262.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 11], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_263.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 2], inputs: [conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.7.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 66}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 11}}
    conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_262.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [1, 11], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_263.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [5, 1], inputs: [conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.7.conv.weight_fork_clone4096],
         t: 5, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 132}, m_k: 1, min_buffer_input: 0, u_kt: 33}}
    _fused_op_17: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_263.dc.conv2d.1.dc.matmul.11, conv2d_263.dc.conv2d.3.dc.matmul.11, conv2d_263.dc.conv2d.5.dc.matmul.11, input_1_add_264, input_1_add_264_fork_clone1549],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 17}}
    conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_17, lc.input_tensor.conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_277.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.8.conv.weight_fork_clone4106],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 12}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_17, lc.input_tensor.conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_277.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.8.conv.weight],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 12}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_17, lc.input_tensor.conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_277.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.8.conv.weight_fork_clone4104],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 12}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_18: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_277.dc.conv2d.1.dc.matmul.11, conv2d_277.dc.conv2d.3.dc.matmul.11, conv2d_277.dc.conv2d.5.dc.matmul.11, input_1_add_278, input_1_add_278_fork_clone1368],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 5], input_3_tms: [vslice: 5],
         attributes: {fused_op_id: 18}}
    concatenate_291.dc.concatenate.2: {type: splice, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_18, _fused_op_17],
         t: 5, mblock: [1, 5], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 405], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 5],
         attributes: {input0: [0, 1, 1], input1: [0, 4, 4]}}
    concatenate_291.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [3, 3], grid_size: [5, 1], inputs: [lc.input_tensor.concatenate_291.dc.sparse_matmul.5.0, concatenate_291.dc.concatenate.2, lc.input_tensor.concatenate_291.dc.sparse_matmul.5.1],
         t: 1, mblock: [1, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose, hstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 5}}
    conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_291.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_292.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.9.conv.weight_fork_clone4114],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 30}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_291.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_292.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.9.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 30}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_291.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_292.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.9.conv.weight_fork_clone4112],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 30}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    _fused_op_19: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_292.dc.conv2d.1.dc.matmul.11, conv2d_292.dc.conv2d.3.dc.matmul.11, conv2d_292.dc.conv2d.5.dc.matmul.11, input_1_add_293, input_1_add_293_fork_clone1555],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 11}}
    conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_19, lc.input_tensor.conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_306.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.10.conv.weight_fork_clone4122],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_19, lc.input_tensor.conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_306.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.10.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_19, lc.input_tensor.conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_306.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.10.conv.weight_fork_clone4120],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_20: {type: fused_op, grid_loc: [5, 2], grid_size: [1, 1], inputs: [conv2d_306.dc.conv2d.1.dc.matmul.11, conv2d_306.dc.conv2d.3.dc.matmul.11, conv2d_306.dc.conv2d.5.dc.matmul.11, input_1_add_307, input_1_add_307_fork_clone1373],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17774_15225: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_19],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [365], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_17772_15225: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [_fused_op_17],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [265], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_17772_15225: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_2_17772_15225],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [265], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17772_15225: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [buffer_1_17772_15225],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [265], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_320.dc.concatenate.3: {type: splice, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_20, buffer_0_17774_15225, buffer_0_17772_15225],
         t: 1, mblock: [5, 7], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 95, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 10, 10], input2: [0, 20, 20]}}
    concatenate_320.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_320.dc.sparse_matmul.6.0, concatenate_320.dc.concatenate.3, lc.input_tensor.concatenate_320.dc.sparse_matmul.6.1],
         t: 1, mblock: [1, 25], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_320.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_321.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.11.conv.weight_fork_clone4130],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 54}, m_k: 1, min_buffer_input: 0, u_kt: 18}}
    conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_320.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_321.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.11.conv.weight],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 54}, m_k: 1, min_buffer_input: 0, u_kt: 18}}
    conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_320.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_321.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.11.conv.weight_fork_clone4128],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 54}, m_k: 1, min_buffer_input: 0, u_kt: 18}}
    _fused_op_21: {type: fused_op, grid_loc: [6, 7], grid_size: [1, 1], inputs: [conv2d_321.dc.conv2d.1.dc.matmul.11, conv2d_321.dc.conv2d.3.dc.matmul.11, conv2d_321.dc.conv2d.5.dc.matmul.11, input_1_add_322, input_1_add_322_fork_clone1561],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 13}}
    conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_21, lc.input_tensor.conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_335.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.12.conv.weight_fork_clone4138],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_21, lc.input_tensor.conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_335.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.12.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_21, lc.input_tensor.conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_335.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.12.conv.weight_fork_clone4136],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_22: {type: fused_op, grid_loc: [7, 2], grid_size: [1, 1], inputs: [conv2d_335.dc.conv2d.1.dc.matmul.11, conv2d_335.dc.conv2d.3.dc.matmul.11, conv2d_335.dc.conv2d.5.dc.matmul.11, input_1_add_336, input_1_add_336_fork_clone1378],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17776_15309: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [_fused_op_21],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_349.dc.concatenate.2: {type: splice, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_22, buffer_0_17776_15309],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 255], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 15, 15]}}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 1
    concatenate_393.dc.concatenate.0.dc.concatenate.6: {type: splice, grid_loc: [2, 2], grid_size: [1, 1], inputs: [e2e__fused_op_10_0, e2e__fused_op_12_0, e2e__fused_op_14_0, e2e__fused_op_16_0, e2e__fused_op_18_0, e2e__fused_op_20_0],
         t: 1, mblock: [5, 6], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [5, 5, 5, 5, 5, 5], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vstack: 5],
         attributes: {input0: [0, 5, 5], input1: [0, 5, 5], input2: [0, 5, 5], input3: [0, 5, 5], input4: [0, 5, 5], input5: [0, 5, 5]}}
    concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.0, concatenate_393.dc.concatenate.0.dc.concatenate.6, lc.input_tensor.concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.1],
         t: 5, mblock: [1, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 6}}
    concatenate_349.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_349.dc.sparse_matmul.5.0, e2e_concatenate_349.dc.concatenate.2_0, lc.input_tensor.concatenate_349.dc.sparse_matmul.5.1],
         t: 1, mblock: [1, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_349.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_350.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.13.conv.weight_fork_clone4146],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_349.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_350.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.13.conv.weight],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_349.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_350.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.13.conv.weight_fork_clone4144],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_23: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_350.dc.conv2d.1.dc.matmul.11, conv2d_350.dc.conv2d.3.dc.matmul.11, conv2d_350.dc.conv2d.5.dc.matmul.11, input_1_add_351, input_1_add_351_fork_clone1567],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 11}}
    conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_23, lc.input_tensor.conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_364.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.14.conv.weight_fork_clone4154],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_23, lc.input_tensor.conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_364.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.14.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_23, lc.input_tensor.conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_364.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.14.conv.weight_fork_clone4152],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_24: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_364.dc.conv2d.1.dc.matmul.11, conv2d_364.dc.conv2d.3.dc.matmul.11, conv2d_364.dc.conv2d.5.dc.matmul.11, input_1_add_365, input_1_add_365_fork_clone1383],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    concatenate_378.dc.concatenate.4: {type: splice, grid_loc: [2, 1], grid_size: [5, 1], inputs: [_fused_op_24, _fused_op_23, e2e__fused_op_21_0, e2e__fused_op_17_0, e2e_max_pool2d_160.dc.reduce_max.6_0],
         t: 1, mblock: [1, 16], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 265, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 15, 15, 15], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 1, 1], input1: [0, 2, 2], input2: [0, 3, 3], input3: [0, 4, 4], input4: [0, 6, 6]}}
    concatenate_378.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [lc.input_tensor.concatenate_378.dc.sparse_matmul.7.0, concatenate_378.dc.concatenate.4, lc.input_tensor.concatenate_378.dc.sparse_matmul.7.1],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 8, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_378.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_379.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [5, 1], inputs: [conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.15.conv.weight_fork_clone4162],
         t: 1, mblock: [5, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 14}}
    conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_378.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_379.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [5, 1], inputs: [conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.15.conv.weight],
         t: 1, mblock: [5, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 14}}
    conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 2], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_378.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_379.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 3], grid_size: [5, 1], inputs: [conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.6.layers.15.conv.weight_fork_clone4160],
         t: 1, mblock: [5, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 14}}
    concatenate_393.dc.concatenate.1.dc.concatenate.4_transpose_nop_38241: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.lc2],
         t: 5, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    _fused_op_25: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_379.dc.conv2d.1.dc.matmul.11, conv2d_379.dc.conv2d.3.dc.matmul.11, conv2d_379.dc.conv2d.5.dc.matmul.11, input_1_add_380, input_1_add_380_fork_clone1388],
         t: 1, mblock: [25, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 21, 21], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 25}}
    concatenate_393.dc.concatenate.1.dc.concatenate.4: {type: splice, grid_loc: [4, 0], grid_size: [5, 1], inputs: [concatenate_393.dc.concatenate.1.dc.concatenate.4_transpose_nop_38241, e2e__fused_op_22_0, _fused_op_24, _fused_op_25],
         t: 1, mblock: [1, 14], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 295, 0], input_dram_io_buf_size_tiles: [0, 45, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 5],
         attributes: {input0: [0, 5, 5], input1: [0, 1, 1], input2: [0, 1, 1], input3: [0, 7, 7]}}
    concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.0, concatenate_393.dc.concatenate.1.dc.concatenate.4, lc.input_tensor.concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.1],
         t: 5, mblock: [13, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose, hslice: 5],
         attributes: {act_t: 5, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 4, u_kt: 7}}
    conv2d_394.dc.matmul.8_transpose_nop_38362: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.lc2],
         t: 5, mblock: [1, 13], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    conv2d_394.dc.matmul.8: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [conv2d_394.dc.matmul.8_transpose_nop_38362, base.7.conv.weight, input_1_add_395_fork_clone1146],
         t: 5, mblock: [1, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 20, input_1: 52}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 13}}
    conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_394.dc.matmul.8, lc.input_tensor.conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_408.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.0.conv.weight],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 1
    conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_394.dc.matmul.8_0, lc.input_tensor.conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 40, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_408.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [5, 1], inputs: [conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.0.conv.weight_fork_clone4183],
         t: 1, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_394.dc.matmul.8_0, lc.input_tensor.conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 40, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_408.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [5, 1], inputs: [conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.0.conv.weight_fork_clone4181],
         t: 1, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    _fused_op_26: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_conv2d_408.dc.conv2d.1.dc.matmul.11_0, conv2d_408.dc.conv2d.3.dc.matmul.11, conv2d_408.dc.conv2d.5.dc.matmul.11, input_1_add_409, input_1_add_409_fork_clone979],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [45, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 10}}
    concatenate_422.dc.concatenate.1: {type: splice, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_26, e2e_conv2d_394.dc.matmul.8_0],
         t: 1, mblock: [5, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 45], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 5],
         attributes: {input0: [0, 5, 5], input1: [0, 40, 40]}}
    concatenate_422.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_422.dc.sparse_matmul.4.0, concatenate_422.dc.concatenate.1, lc.input_tensor.concatenate_422.dc.sparse_matmul.4.1],
         t: 1, mblock: [3, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 9, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_422.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_423.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.1.conv.weight_fork_clone4191],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 54}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 9}}
    conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_422.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_423.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.1.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 54}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 9}}
    conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_422.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_423.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [5, 1], inputs: [conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.1.conv.weight_fork_clone4189],
         t: 5, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 54}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 9}}
    _fused_op_27: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_423.dc.conv2d.1.dc.matmul.11, conv2d_423.dc.conv2d.3.dc.matmul.11, conv2d_423.dc.conv2d.5.dc.matmul.11, input_1_add_424, input_1_add_424_fork_clone1152],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 11}}
    conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_27, lc.input_tensor.conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_437.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.2.conv.weight_fork_clone4199],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_27, lc.input_tensor.conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_437.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.2.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_27, lc.input_tensor.conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_437.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.2.conv.weight_fork_clone4197],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_28: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_437.dc.conv2d.1.dc.matmul.11, conv2d_437.dc.conv2d.3.dc.matmul.11, conv2d_437.dc.conv2d.5.dc.matmul.11, input_1_add_438, input_1_add_438_fork_clone984],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    concatenate_451.dc.concatenate.2: {type: splice, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_28, _fused_op_27, e2e_conv2d_394.dc.matmul.8_0],
         t: 5, mblock: [1, 11], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 335, 0], input_dram_io_buf_size_tiles: [0, 0, 45], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 5], input_0_tms: [vslice: 5],
         attributes: {input0: [0, 1, 1], input1: [0, 2, 2], input2: [0, 8, 8]}}
    concatenate_451.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 5], inputs: [lc.input_tensor.concatenate_451.dc.sparse_matmul.5.0, concatenate_451.dc.concatenate.2, lc.input_tensor.concatenate_451.dc.sparse_matmul.5.1],
         t: 1, mblock: [11, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 5}], input_1_tms: [transpose, hstack: 5], input_0_tms: [broadcast: {c: 5}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 4, u_kt: 11}}
    conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_451.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 11], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_452.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.3.conv.weight],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 99}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 11}}

  fwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 1
    conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_451.dc.sparse_matmul.5.lc2_0, lc.input_tensor.conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 11], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_452.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.3.conv.weight_fork_clone4207],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 99}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 11}}
    conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_451.dc.sparse_matmul.5.lc2_0, lc.input_tensor.conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [1, 11], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_452.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [5, 1], inputs: [conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.3.conv.weight_fork_clone4205],
         t: 5, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 99}, m_k: 1, min_buffer_input: 0, u_kt: 33}}
    _fused_op_29: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_conv2d_452.dc.conv2d.1.dc.matmul.11_0, conv2d_452.dc.conv2d.3.dc.matmul.11, conv2d_452.dc.conv2d.5.dc.matmul.11, input_1_add_453, input_1_add_453_fork_clone1158],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [45, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 13}}
    conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_29, lc.input_tensor.conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_466.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.4.conv.weight_fork_clone4215],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_29, lc.input_tensor.conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_466.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.4.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_29, lc.input_tensor.conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_466.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.4.conv.weight_fork_clone4213],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_30: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_466.dc.conv2d.1.dc.matmul.11, conv2d_466.dc.conv2d.3.dc.matmul.11, conv2d_466.dc.conv2d.5.dc.matmul.11, input_1_add_467, input_1_add_467_fork_clone989],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17784_15661: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [_fused_op_29],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_480.dc.concatenate.2: {type: splice, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_30, buffer_0_17784_15661],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 255], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 15, 15]}}
    concatenate_480.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [lc.input_tensor.concatenate_480.dc.sparse_matmul.5.0, concatenate_480.dc.concatenate.2, lc.input_tensor.concatenate_480.dc.sparse_matmul.5.1],
         t: 1, mblock: [2, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_480.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_481.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.5.conv.weight_fork_clone4223],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_480.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_481.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.5.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_480.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_481.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.5.conv.weight_fork_clone4221],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_31: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_481.dc.conv2d.1.dc.matmul.11, conv2d_481.dc.conv2d.3.dc.matmul.11, conv2d_481.dc.conv2d.5.dc.matmul.11, input_1_add_482, input_1_add_482_fork_clone1164],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 11}}
    conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_31, lc.input_tensor.conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_495.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.6.conv.weight_fork_clone4231],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_31, lc.input_tensor.conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_495.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.6.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_31, lc.input_tensor.conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_495.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.6.conv.weight_fork_clone4229],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_32: {type: fused_op, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_495.dc.conv2d.1.dc.matmul.11, conv2d_495.dc.conv2d.3.dc.matmul.11, conv2d_495.dc.conv2d.5.dc.matmul.11, input_1_add_496, input_1_add_496_fork_clone994],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_3_17784_15746: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [_fused_op_29],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_17784_15746: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [buffer_3_17784_15746],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_17784_15746: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_2_17784_15746],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17784_15746: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [buffer_1_17784_15746],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_509.dc.concatenate.3: {type: splice, grid_loc: [5, 2], grid_size: [1, 1], inputs: [_fused_op_32, _fused_op_31, buffer_0_17784_15746, e2e_conv2d_394.dc.matmul.8_0],
         t: 5, mblock: [1, 14], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 295, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 45], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vslice: 5], input_1_tms: [vslice: 5], input_0_tms: [vslice: 5],
         attributes: {input0: [0, 1, 1], input1: [0, 2, 2], input2: [0, 3, 3], input3: [0, 8, 8]}}
    concatenate_509.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [5, 3], grid_size: [1, 5], inputs: [lc.input_tensor.concatenate_509.dc.sparse_matmul.6.0, concatenate_509.dc.concatenate.3, lc.input_tensor.concatenate_509.dc.sparse_matmul.6.1],
         t: 1, mblock: [13, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 5}], input_1_tms: [transpose, hstack: 5], input_0_tms: [broadcast: {c: 5}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 14}}
    conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_509.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 13], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_510.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 1], grid_size: [1, 5], inputs: [conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.7.conv.weight],
         t: 25, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 39}, m_k: 1, min_buffer_input: 0, u_kt: 39}}

  fwd_0_10_temporal_epoch_10:
    target_device: 0
    input_count: 1
    conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_509.dc.sparse_matmul.6.lc2_0, lc.input_tensor.conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [1, 13], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 17, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_510.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [5, 1], inputs: [conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.7.conv.weight_fork_clone4239],
         t: 5, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 195}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 13}}
    conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_509.dc.sparse_matmul.6.lc2_0, lc.input_tensor.conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [1, 13], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_510.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [5, 1], inputs: [conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.7.conv.weight_fork_clone4237],
         t: 5, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 195}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 13}}
    _fused_op_33: {type: fused_op, grid_loc: [0, 4], grid_size: [5, 1], inputs: [e2e_conv2d_510.dc.conv2d.1.dc.matmul.11_0, conv2d_510.dc.conv2d.3.dc.matmul.11, conv2d_510.dc.conv2d.5.dc.matmul.11, input_1_add_511, input_1_add_511_fork_clone1170],
         t: 1, mblock: [5, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [45, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 25],
         attributes: {fused_op_id: 33}}
    conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_33, lc.input_tensor.conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_524.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [5, 1], inputs: [conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.8.conv.weight],
         t: 1, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}

  fwd_0_11_temporal_epoch_11:
    target_device: 0
    input_count: 1
    conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_33_0, lc.input_tensor.conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_524.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.8.conv.weight_fork_clone4247],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 15}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e__fused_op_33_0, lc.input_tensor.conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_524.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.8.conv.weight_fork_clone4245],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 15}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    _fused_op_34: {type: fused_op, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_conv2d_524.dc.conv2d.1.dc.matmul.11_0, conv2d_524.dc.conv2d.3.dc.matmul.11, conv2d_524.dc.conv2d.5.dc.matmul.11, input_1_add_525, input_1_add_525_fork_clone999],
         t: 5, mblock: [1, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [45, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 5], input_3_tms: [vslice: 5], input_0_tms: [vslice: 5],
         attributes: {fused_op_id: 18}}
    concatenate_538.dc.concatenate.2: {type: splice, grid_loc: [0, 5], grid_size: [1, 1], inputs: [_fused_op_34, e2e__fused_op_33_0],
         t: 5, mblock: [1, 6], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 45], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vslice: 5],
         attributes: {input0: [0, 1, 1], input1: [0, 5, 5]}}
    concatenate_538.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_538.dc.sparse_matmul.5.0, concatenate_538.dc.concatenate.2, lc.input_tensor.concatenate_538.dc.sparse_matmul.5.1],
         t: 1, mblock: [2, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose, hstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 6}}
    conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_538.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_539.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.9.conv.weight_fork_clone4255],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 36}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_538.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_539.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.9.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 36}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_538.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_539.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.9.conv.weight_fork_clone4253],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 36}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    _fused_op_35: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_539.dc.conv2d.1.dc.matmul.11, conv2d_539.dc.conv2d.3.dc.matmul.11, conv2d_539.dc.conv2d.5.dc.matmul.11, input_1_add_540, input_1_add_540_fork_clone1176],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 11}}
    conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_35, lc.input_tensor.conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_553.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.10.conv.weight_fork_clone4263],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_35, lc.input_tensor.conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_553.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.10.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_35, lc.input_tensor.conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_553.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.10.conv.weight_fork_clone4261],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_36: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_553.dc.conv2d.1.dc.matmul.11, conv2d_553.dc.conv2d.3.dc.matmul.11, conv2d_553.dc.conv2d.5.dc.matmul.11, input_1_add_554, input_1_add_554_fork_clone1004],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17790_15923: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_35],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [365], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_567.dc.concatenate.3: {type: splice, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_36, buffer_0_17790_15923, e2e__fused_op_33_0],
         t: 1, mblock: [5, 8], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 45, 0], input_dram_io_buf_size_tiles: [0, 0, 45], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 10, 10], input2: [0, 25, 25]}}
    concatenate_567.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_567.dc.sparse_matmul.6.0, concatenate_567.dc.concatenate.3, lc.input_tensor.concatenate_567.dc.sparse_matmul.6.1],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    buffer_0_15927_15934: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [concatenate_567.dc.sparse_matmul.6.lc2],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [119], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_15927_15934, lc.input_tensor.conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 245, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_568.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.11.conv.weight_fork_clone4271],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 63}, m_k: 1, min_buffer_input: 0, u_kt: 21}}
    buffer_1_15927_15944: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [concatenate_567.dc.sparse_matmul.6.lc2],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [119], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    buffer_0_15927_15944: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_1_15927_15944],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [119], input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2}
    conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_15927_15944, lc.input_tensor.conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [15, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 245, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_568.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.11.conv.weight],
         t: 5, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 63}, m_k: 1, min_buffer_input: 0, u_kt: 21}}
    conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_567.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_568.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 7], grid_size: [5, 1], inputs: [conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.11.conv.weight_fork_clone4269],
         t: 1, mblock: [1, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 21}}
    _fused_op_37: {type: fused_op, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_568.dc.conv2d.1.dc.matmul.11, conv2d_568.dc.conv2d.3.dc.matmul.11, conv2d_568.dc.conv2d.5.dc.matmul.11, input_1_add_569, input_1_add_569_fork_clone1182],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 125, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 13}}
    conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_37, lc.input_tensor.conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_582.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.12.conv.weight_fork_clone4279],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_37, lc.input_tensor.conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_582.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.12.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_37, lc.input_tensor.conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_582.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.12.conv.weight_fork_clone4277],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_38: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [conv2d_582.dc.conv2d.1.dc.matmul.11, conv2d_582.dc.conv2d.3.dc.matmul.11, conv2d_582.dc.conv2d.5.dc.matmul.11, input_1_add_583, input_1_add_583_fork_clone1009],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}
    buffer_0_17792_16007: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_37],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [315], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_596.dc.concatenate.2: {type: splice, grid_loc: [6, 0], grid_size: [1, 1], inputs: [_fused_op_38, buffer_0_17792_16007],
         t: 1, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 255], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 5, 5], input1: [0, 15, 15]}}
    concatenate_596.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [lc.input_tensor.concatenate_596.dc.sparse_matmul.5.0, concatenate_596.dc.concatenate.2, lc.input_tensor.concatenate_596.dc.sparse_matmul.5.1],
         t: 1, mblock: [2, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_596.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_597.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.13.conv.weight_fork_clone4287],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_596.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_597.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.13.conv.weight],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_596.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_597.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.13.conv.weight_fork_clone4285],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 24}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_39: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [conv2d_597.dc.conv2d.1.dc.matmul.11, conv2d_597.dc.conv2d.3.dc.matmul.11, conv2d_597.dc.conv2d.5.dc.matmul.11, input_1_add_598, input_1_add_598_fork_clone1188],
         t: 1, mblock: [5, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 5], input_1_tms: [vstack: 5], input_0_tms: [vstack: 5],
         attributes: {fused_op_id: 11}}
    conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_39, lc.input_tensor.conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_611.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.14.conv.weight_fork_clone4295],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_39, lc.input_tensor.conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_611.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.14.conv.weight],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_39, lc.input_tensor.conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 25}}
    conv2d_611.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.14.conv.weight_fork_clone4293],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_40: {type: fused_op, grid_loc: [8, 4], grid_size: [1, 1], inputs: [conv2d_611.dc.conv2d.1.dc.matmul.11, conv2d_611.dc.conv2d.3.dc.matmul.11, conv2d_611.dc.conv2d.5.dc.matmul.11, input_1_add_612, input_1_add_612_fork_clone1014],
         t: 1, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 10}}

  fwd_0_12_temporal_epoch_12:
    target_device: 0
    input_count: 1
    concatenate_640.dc.concatenate.0.dc.concatenate.6: {type: splice, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e__fused_op_26_0, e2e__fused_op_28_0, e2e__fused_op_30_0, e2e__fused_op_32_0, e2e__fused_op_34_0, e2e__fused_op_36_0],
         t: 1, mblock: [5, 6], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [5, 5, 5, 5, 5, 5], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vstack: 5],
         attributes: {input0: [0, 5, 5], input1: [0, 5, 5], input2: [0, 5, 5], input3: [0, 5, 5], input4: [0, 5, 5], input5: [0, 5, 5]}}
    concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.0, concatenate_640.dc.concatenate.0.dc.concatenate.6, lc.input_tensor.concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.1],
         t: 1, mblock: [2, 5], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 6}}
    concatenate_625.dc.concatenate.4: {type: splice, grid_loc: [0, 0], grid_size: [5, 1], inputs: [e2e__fused_op_40_0, e2e__fused_op_39_0, e2e__fused_op_37_0, e2e__fused_op_33_0, e2e_conv2d_394.dc.matmul.8_0],
         t: 1, mblock: [1, 19], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [5, 5, 5, 5, 5], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vstack: 5],
         attributes: {input0: [0, 1, 1], input1: [0, 2, 2], input2: [0, 3, 3], input3: [0, 5, 5], input4: [0, 8, 8]}}
    concatenate_625.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_625.dc.sparse_matmul.7.0, concatenate_625.dc.concatenate.4, lc.input_tensor.concatenate_625.dc.sparse_matmul.7.1],
         t: 1, mblock: [1, 25], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 19, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 3], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_625.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 3], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 17, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_626.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 4], grid_size: [5, 1], inputs: [conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.15.conv.weight_fork_clone4303],
         t: 5, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 432}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 18}}
    conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_625.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 3], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 17, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_626.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [5, 1], inputs: [conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.15.conv.weight],
         t: 5, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 432}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 18}}
    conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 1], grid_size: [5, 1], inputs: [lc.input_tensor.conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_625.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 5, mblock: [3, 3], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 5}}
    conv2d_626.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 2], grid_size: [5, 1], inputs: [conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.8.layers.15.conv.weight_fork_clone4301],
         t: 5, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 15, hstack: 3, vstack: 5],
         attributes: {kernel_broadcast: {input_1: 432}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 18}}
    concatenate_640.dc.concatenate.1.dc.concatenate.4_transpose_nop_38615: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.lc2],
         t: 1, mblock: [25, 1], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    _fused_op_41: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_626.dc.conv2d.1.dc.matmul.11, conv2d_626.dc.conv2d.3.dc.matmul.11, conv2d_626.dc.conv2d.5.dc.matmul.11, input_1_add_627, input_1_add_627_fork_clone1019],
         t: 5, mblock: [5, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 48, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_4_tms: [vslice: 5], input_3_tms: [vslice: 5],
         attributes: {fused_op_id: 41}}
    concatenate_640.dc.concatenate.1.dc.concatenate.4: {type: splice, grid_loc: [1, 1], grid_size: [1, 1], inputs: [concatenate_640.dc.concatenate.1.dc.concatenate.4_transpose_nop_38615, e2e__fused_op_38_0, e2e__fused_op_40_0, _fused_op_41],
         t: 5, mblock: [1, 16], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 20, 20, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vslice: 5], input_1_tms: [vslice: 5], input_0_tms: [vslice: 5],
         attributes: {input0: [0, 6, 6], input1: [0, 1, 1], input2: [0, 1, 1], input3: [0, 8, 8]}}
    concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [1, 7], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.0, concatenate_640.dc.concatenate.1.dc.concatenate.4, lc.input_tensor.concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.1],
         t: 5, mblock: [5, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 5, fracture_factor: 1, identity: true, l1_acc: true, m_k: 4, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_641.dc.matmul.8_transpose_nop_38736: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.lc2],
         t: 5, mblock: [5, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    conv2d_641.dc.matmul.8: {type: matmul, grid_loc: [8, 0], grid_size: [1, 5], inputs: [conv2d_641.dc.matmul.8_transpose_nop_38736, base.9.conv.weight, input_1_add_642_fork_clone807],
         t: 5, mblock: [1, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 10, input_1: 30}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 5}}

  fwd_0_13_temporal_epoch_13:
    target_device: 0
    input_count: 1
    max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.0, e2e_conv2d_641.dc.matmul.8_0, lc.input_tensor.max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.1],
         t: 1, mblock: [4, 2], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 5}}
    max_pool2d_655.dc.reduce_max.6: {type: reduce, grid_loc: [0, 1], grid_size: [7, 1], inputs: [max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2],
         t: 1, mblock: [1, 2], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 4],
         attributes: {dim: z, type: max, z: 4}}
    conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_655.dc.reduce_max.6, lc.input_tensor.conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_656.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.0.conv.weight_fork_clone4335],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 5}}
    conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_655.dc.reduce_max.6, lc.input_tensor.conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_656.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.0.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 5}}
    conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_655.dc.reduce_max.6, lc.input_tensor.conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_656.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.0.conv.weight_fork_clone4333],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 5}}
    _fused_op_42: {type: fused_op, grid_loc: [1, 2], grid_size: [1, 1], inputs: [conv2d_656.dc.conv2d.1.dc.matmul.11, conv2d_656.dc.conv2d.3.dc.matmul.11, conv2d_656.dc.conv2d.5.dc.matmul.11, input_1_add_657, input_1_add_657_fork_clone581],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_0_16158_16199: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [max_pool2d_655.dc.reduce_max.6],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [329], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_670.dc.concatenate.1: {type: splice, grid_loc: [1, 4], grid_size: [1, 1], inputs: [_fused_op_42, buffer_0_16158_16199],
         t: 1, mblock: [1, 12], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 287], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 10, 10]}}
    concatenate_670.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [1, 5], grid_size: [4, 1], inputs: [lc.input_tensor.concatenate_670.dc.sparse_matmul.4.0, concatenate_670.dc.concatenate.1, lc.input_tensor.concatenate_670.dc.sparse_matmul.4.1],
         t: 1, mblock: [3, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 3, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_670.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_671.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.1.conv.weight_fork_clone4343],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 72}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_670.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_671.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.1.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 72}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_670.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_671.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.1.conv.weight_fork_clone4341],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 72}, l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 4}}
    _fused_op_43: {type: fused_op, grid_loc: [2, 4], grid_size: [7, 1], inputs: [conv2d_671.dc.conv2d.1.dc.matmul.11, conv2d_671.dc.conv2d.3.dc.matmul.11, conv2d_671.dc.conv2d.5.dc.matmul.11, input_1_add_672, input_1_add_672_fork_clone764],
         t: 1, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 43}}
    conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_43, lc.input_tensor.conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_685.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.2.conv.weight_fork_clone4351],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_43, lc.input_tensor.conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_685.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.2.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_43, lc.input_tensor.conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_685.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.2.conv.weight_fork_clone4349],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_44: {type: fused_op, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_685.dc.conv2d.1.dc.matmul.11, conv2d_685.dc.conv2d.3.dc.matmul.11, conv2d_685.dc.conv2d.5.dc.matmul.11, input_1_add_686, input_1_add_686_fork_clone586],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_0_16158_17963: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [max_pool2d_655.dc.reduce_max.6],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [329], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_16158_17962: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [buffer_0_16158_17963],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [329], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_16158_17961: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [buffer_0_16158_17962],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [329], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_16158_16283: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [buffer_0_16158_17961],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [329], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_699.dc.concatenate.2: {type: splice, grid_loc: [5, 6], grid_size: [1, 1], inputs: [_fused_op_44, _fused_op_43, buffer_0_16158_16283],
         t: 1, mblock: [1, 14], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 245, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 10, 10]}}
    concatenate_699.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [lc.input_tensor.concatenate_699.dc.sparse_matmul.5.0, concatenate_699.dc.concatenate.2, lc.input_tensor.concatenate_699.dc.sparse_matmul.5.1],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 7}}
    conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_699.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_700.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.3.conv.weight_fork_clone4359],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 168}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 21}}
    conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_699.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_700.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.3.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 168}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 21}}
    conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_699.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_700.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.3.conv.weight_fork_clone4357],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 168}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 21}}

  fwd_0_14_temporal_epoch_14:
    target_device: 0
    input_count: 1
    _fused_op_45: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_700.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_700.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_700.dc.conv2d.5.dc.matmul.11_0, input_1_add_701, input_1_add_701_fork_clone770],
         t: 1, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [14, 14, 14, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 45}}
    conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_45, lc.input_tensor.conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_714.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.4.conv.weight_fork_clone4367],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_45, lc.input_tensor.conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_714.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.4.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_45, lc.input_tensor.conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_714.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.4.conv.weight_fork_clone4365],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_46: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_714.dc.conv2d.1.dc.matmul.11, conv2d_714.dc.conv2d.3.dc.matmul.11, conv2d_714.dc.conv2d.5.dc.matmul.11, input_1_add_715, input_1_add_715_fork_clone591],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_728.dc.concatenate.2: {type: splice, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_46, _fused_op_45],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 371], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 4, 4]}}
    concatenate_728.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [1, 1], grid_size: [5, 1], inputs: [lc.input_tensor.concatenate_728.dc.sparse_matmul.5.0, concatenate_728.dc.concatenate.2, lc.input_tensor.concatenate_728.dc.sparse_matmul.5.1],
         t: 1, mblock: [1, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 6}}
    conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_728.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_729.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.5.conv.weight_fork_clone4375],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_728.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_729.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.5.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_728.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_729.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.5.conv.weight_fork_clone4373],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    _fused_op_47: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_729.dc.conv2d.1.dc.matmul.11, conv2d_729.dc.conv2d.3.dc.matmul.11, conv2d_729.dc.conv2d.5.dc.matmul.11, input_1_add_730, input_1_add_730_fork_clone776],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_47, lc.input_tensor.conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_743.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.6.conv.weight_fork_clone4383],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_47, lc.input_tensor.conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_743.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.6.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_47, lc.input_tensor.conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_743.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.6.conv.weight_fork_clone4381],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_48: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_743.dc.conv2d.1.dc.matmul.11, conv2d_743.dc.conv2d.3.dc.matmul.11, conv2d_743.dc.conv2d.5.dc.matmul.11, input_1_add_744, input_1_add_744_fork_clone596],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_1_17800_16452: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_45],
         t: 1, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [413], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17800_16452: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_1_17800_16452],
         t: 1, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [413], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_757.dc.concatenate.3: {type: splice, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_48, _fused_op_47, buffer_0_17800_16452, e2e_max_pool2d_655.dc.reduce_max.6_0],
         t: 1, mblock: [1, 18], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 175, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 49], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 4, 4], input3: [0, 10, 10]}}
    concatenate_757.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_757.dc.sparse_matmul.6.0, concatenate_757.dc.concatenate.3, lc.input_tensor.concatenate_757.dc.sparse_matmul.6.1],
         t: 1, mblock: [17, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 5, u_kt: 9}}
    conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_757.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 17], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_758.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 3], grid_size: [1, 2], inputs: [conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.7.conv.weight_fork_clone4391],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 153}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 17}}
    conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_757.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 17], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_758.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 3], grid_size: [1, 2], inputs: [conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.7.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 153}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 17}}
    conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_757.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 17], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_758.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.7.conv.weight_fork_clone4389],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 153}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 17}}

  fwd_0_15_temporal_epoch_15:
    target_device: 0
    input_count: 1
    _fused_op_49: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_758.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_758.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_758.dc.conv2d.5.dc.matmul.11_0, input_1_add_759, input_1_add_759_fork_clone782],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [14, 14, 14, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 49}}
    conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_49, lc.input_tensor.conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_772.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.8.conv.weight_fork_clone4399],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_49, lc.input_tensor.conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_772.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.8.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_49, lc.input_tensor.conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_772.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.8.conv.weight_fork_clone4397],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 6}}
    _fused_op_50: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_772.dc.conv2d.1.dc.matmul.11, conv2d_772.dc.conv2d.3.dc.matmul.11, conv2d_772.dc.conv2d.5.dc.matmul.11, input_1_add_773, input_1_add_773_fork_clone601],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_786.dc.concatenate.2: {type: splice, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_50, _fused_op_49],
         t: 1, mblock: [1, 8], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 343], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 6, 6]}}
    concatenate_786.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_786.dc.sparse_matmul.5.0, concatenate_786.dc.concatenate.2, lc.input_tensor.concatenate_786.dc.sparse_matmul.5.1],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 4}}
    conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_786.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_787.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.9.conv.weight_fork_clone4407],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_786.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_787.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.9.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_786.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_787.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.9.conv.weight_fork_clone4405],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 7}}
    _fused_op_51: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_787.dc.conv2d.1.dc.matmul.11, conv2d_787.dc.conv2d.3.dc.matmul.11, conv2d_787.dc.conv2d.5.dc.matmul.11, input_1_add_788, input_1_add_788_fork_clone788],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_51, lc.input_tensor.conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_801.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.10.conv.weight_fork_clone4415],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_51, lc.input_tensor.conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_801.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.10.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_51, lc.input_tensor.conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_801.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.10.conv.weight_fork_clone4413],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_52: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_801.dc.conv2d.1.dc.matmul.11, conv2d_801.dc.conv2d.3.dc.matmul.11, conv2d_801.dc.conv2d.5.dc.matmul.11, input_1_add_802, input_1_add_802_fork_clone606],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_1_17804_16629: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [_fused_op_49],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [385], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17804_16629: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [buffer_1_17804_16629],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [385], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_815.dc.concatenate.3: {type: splice, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_52, _fused_op_51, buffer_0_17804_16629],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 301, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 6, 6]}}
    concatenate_815.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [3, 3], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_815.dc.sparse_matmul.6.0, concatenate_815.dc.concatenate.3, lc.input_tensor.concatenate_815.dc.sparse_matmul.6.1],
         t: 1, mblock: [3, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 5}}
    conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_815.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_816.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.11.conv.weight_fork_clone4423],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 108}, m_k: 1, min_buffer_input: 0, u_kt: 27}}
    conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_815.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_816.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.11.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 108}, m_k: 1, min_buffer_input: 0, u_kt: 27}}
    conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_815.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_816.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.11.conv.weight_fork_clone4421],
         t: 7, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 108}, m_k: 1, min_buffer_input: 0, u_kt: 27}}

  fwd_0_16_temporal_epoch_16:
    target_device: 0
    input_count: 1
    concatenate_888.dc.concatenate.0.dc.concatenate.6: {type: splice, grid_loc: [3, 5], grid_size: [1, 1], inputs: [e2e__fused_op_42_0, e2e__fused_op_44_0, e2e__fused_op_46_0, e2e__fused_op_48_0, e2e__fused_op_50_0, e2e__fused_op_52_0],
         t: 1, mblock: [1, 12], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [7, 7, 7, 7, 7, 7], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 2, 2], input3: [0, 2, 2], input4: [0, 2, 2], input5: [0, 2, 2]}}
    concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.0, concatenate_888.dc.concatenate.0.dc.concatenate.6, lc.input_tensor.concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.1],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 3, num_index_tiles: 1, num_sparse_tiles: 18, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 4}}
    _fused_op_53: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_816.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_816.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_816.dc.conv2d.5.dc.matmul.11_0, input_1_add_817, input_1_add_817_fork_clone794],
         t: 1, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [14, 14, 14, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 45}}
    conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_53, lc.input_tensor.conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_830.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.12.conv.weight_fork_clone4431],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_53, lc.input_tensor.conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_830.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.12.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_53, lc.input_tensor.conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_830.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.12.conv.weight_fork_clone4429],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_54: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_830.dc.conv2d.1.dc.matmul.11, conv2d_830.dc.conv2d.3.dc.matmul.11, conv2d_830.dc.conv2d.5.dc.matmul.11, input_1_add_831, input_1_add_831_fork_clone611],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_844.dc.concatenate.2: {type: splice, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_54, _fused_op_53],
         t: 1, mblock: [1, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 371], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 4, 4]}}
    concatenate_844.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [1, 1], grid_size: [5, 1], inputs: [lc.input_tensor.concatenate_844.dc.sparse_matmul.5.0, concatenate_844.dc.concatenate.2, lc.input_tensor.concatenate_844.dc.sparse_matmul.5.1],
         t: 1, mblock: [1, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 6}}
    conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_844.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_845.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.13.conv.weight_fork_clone4439],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_844.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_845.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.13.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_844.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_845.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.13.conv.weight_fork_clone4437],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    _fused_op_55: {type: fused_op, grid_loc: [2, 0], grid_size: [1, 1], inputs: [conv2d_845.dc.conv2d.1.dc.matmul.11, conv2d_845.dc.conv2d.3.dc.matmul.11, conv2d_845.dc.conv2d.5.dc.matmul.11, input_1_add_846, input_1_add_846_fork_clone800],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_55, lc.input_tensor.conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_859.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.14.conv.weight_fork_clone4447],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_55, lc.input_tensor.conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_859.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.14.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_55, lc.input_tensor.conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_859.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.14.conv.weight_fork_clone4445],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 6}}
    _fused_op_56: {type: fused_op, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_859.dc.conv2d.1.dc.matmul.11, conv2d_859.dc.conv2d.3.dc.matmul.11, conv2d_859.dc.conv2d.5.dc.matmul.11, input_1_add_860, input_1_add_860_fork_clone616],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_1_17808_16799: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [_fused_op_53],
         t: 1, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [413], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17808_16799: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_1_17808_16799],
         t: 1, mblock: [1, 4], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [413], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_873.dc.concatenate.4: {type: splice, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_56, _fused_op_55, buffer_0_17808_16799, e2e__fused_op_49_0, e2e_max_pool2d_655.dc.reduce_max.6_0],
         t: 1, mblock: [1, 24], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 77, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 21, 21], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 4, 4], input3: [0, 6, 6], input4: [0, 10, 10]}}
    concatenate_873.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [lc.input_tensor.concatenate_873.dc.sparse_matmul.7.0, concatenate_873.dc.concatenate.4, lc.input_tensor.concatenate_873.dc.sparse_matmul.7.1],
         t: 1, mblock: [11, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 6, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 4}}
    concatenate_888.dc.concatenate.1.dc.concatenate.4_transpose_nop_38993: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.lc2],
         t: 1, mblock: [7, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  fwd_0_17_temporal_epoch_17:
    target_device: 0
    input_count: 1
    conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_873.dc.sparse_matmul.7.lc2_0, lc.input_tensor.conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 11], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_874.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 2], inputs: [conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.15.conv.weight],
         t: 1, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 11}}
    conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_873.dc.sparse_matmul.7.lc2_0, lc.input_tensor.conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 11], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_874.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [7, 2], inputs: [conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.15.conv.weight_fork_clone4453],
         t: 1, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 11}}

  fwd_0_18_temporal_epoch_18:
    target_device: 0
    input_count: 1
    conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_873.dc.sparse_matmul.7.lc2_0, lc.input_tensor.conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 11], ublock: [3, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_874.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 2], inputs: [conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.11.layers.15.conv.weight_fork_clone4455],
         t: 1, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 11}}
    _fused_op_57: {type: fused_op, grid_loc: [0, 3], grid_size: [7, 1], inputs: [e2e_conv2d_874.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_874.dc.conv2d.3.dc.matmul.11_0, conv2d_874.dc.conv2d.5.dc.matmul.11, input_1_add_875, input_1_add_875_fork_clone621],
         t: 1, mblock: [1, 2], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [20, 20, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 57}}
    concatenate_888.dc.concatenate.1.dc.concatenate.4: {type: splice, grid_loc: [0, 4], grid_size: [7, 1], inputs: [e2e_concatenate_888.dc.concatenate.1.dc.concatenate.4_transpose_nop_38993_0, e2e__fused_op_54_0, e2e__fused_op_56_0, _fused_op_57],
         t: 1, mblock: [1, 21], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [16, 16, 16, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 7, 7], input1: [0, 2, 2], input2: [0, 2, 2], input3: [0, 10, 10]}}
    concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.0, concatenate_888.dc.concatenate.1.dc.concatenate.4, lc.input_tensor.concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.1],
         t: 1, mblock: [19, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 3, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_889.dc.matmul.8_transpose_nop_39114: {type: nop, grid_loc: [0, 6], grid_size: [7, 1], inputs: [concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.lc2],
         t: 1, mblock: [1, 19], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  fwd_0_19_temporal_epoch_19:
    target_device: 0
    input_count: 1
    conv2d_889.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [7, 3], inputs: [e2e_conv2d_889.dc.matmul.8_transpose_nop_39114_0, base.12.conv.weight, input_1_add_890_fork_clone379],
         t: 1, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [38, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 5}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 19}}
    conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_889.dc.matmul.8, lc.input_tensor.conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_903.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.0.conv.weight],
         t: 1, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 5}}
    conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_889.dc.matmul.8, lc.input_tensor.conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_903.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.0.conv.weight_fork_clone4474],
         t: 1, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 5}}

  fwd_0_20_temporal_epoch_20:
    target_device: 0
    input_count: 1
    conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_889.dc.matmul.8_0, lc.input_tensor.conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_903.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.0.conv.weight_fork_clone4476],
         t: 1, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 5}}
    _fused_op_58: {type: fused_op, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_conv2d_903.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_903.dc.conv2d.3.dc.matmul.11_0, conv2d_903.dc.conv2d.5.dc.matmul.11, input_1_add_904, input_1_add_904_fork_clone212],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [21, 21, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_917.dc.concatenate.1: {type: splice, grid_loc: [0, 3], grid_size: [1, 1], inputs: [_fused_op_58, e2e_conv2d_889.dc.matmul.8_0],
         t: 1, mblock: [1, 17], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 15, 15]}}
    concatenate_917.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_917.dc.sparse_matmul.4.0, concatenate_917.dc.concatenate.1, lc.input_tensor.concatenate_917.dc.sparse_matmul.4.1],
         t: 1, mblock: [17, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 5, u_kt: 17}}
    conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_917.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 17], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_918.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.1.conv.weight_fork_clone4484],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 153}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 17}}
    conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_917.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 17], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_918.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.1.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 153}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 17}}
    conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_917.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [1, 17], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_918.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.1.conv.weight_fork_clone4482],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 153}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 17}}
    _fused_op_59: {type: fused_op, grid_loc: [0, 7], grid_size: [7, 1], inputs: [conv2d_918.dc.conv2d.1.dc.matmul.11, conv2d_918.dc.conv2d.3.dc.matmul.11, conv2d_918.dc.conv2d.5.dc.matmul.11, input_1_add_919, input_1_add_919_fork_clone385],
         t: 1, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 59}}
    conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_59, lc.input_tensor.conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_932.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.2.conv.weight_fork_clone4492],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_59, lc.input_tensor.conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_932.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.2.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_59, lc.input_tensor.conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_932.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.2.conv.weight_fork_clone4490],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_60: {type: fused_op, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_932.dc.conv2d.1.dc.matmul.11, conv2d_932.dc.conv2d.3.dc.matmul.11, conv2d_932.dc.conv2d.5.dc.matmul.11, input_1_add_933, input_1_add_933_fork_clone217],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_946.dc.concatenate.2: {type: splice, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_60, _fused_op_59, e2e_conv2d_889.dc.matmul.8_0],
         t: 1, mblock: [1, 20], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 161, 0], input_dram_io_buf_size_tiles: [0, 0, 49], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 3, 3], input2: [0, 15, 15]}}
    concatenate_946.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [4, 1], inputs: [lc.input_tensor.concatenate_946.dc.sparse_matmul.5.0, concatenate_946.dc.concatenate.2, lc.input_tensor.concatenate_946.dc.sparse_matmul.5.1],
         t: 1, mblock: [5, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 4}}
    conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_946.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 4], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_947.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 5], inputs: [conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.3.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 60}, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 4}}
    conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_946.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 4], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_947.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 1], grid_size: [1, 5], inputs: [conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.3.conv.weight_fork_clone4498],
         t: 7, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 60}, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 4}}

  fwd_0_21_temporal_epoch_21:
    target_device: 0
    input_count: 1
    conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_946.dc.sparse_matmul.5.lc2_0, lc.input_tensor.conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 4], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_947.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 5], inputs: [conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.3.conv.weight_fork_clone4500],
         t: 7, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 60}, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 4}}
    _fused_op_61: {type: fused_op, grid_loc: [0, 6], grid_size: [7, 1], inputs: [e2e_conv2d_947.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_947.dc.conv2d.3.dc.matmul.11_0, conv2d_947.dc.conv2d.5.dc.matmul.11, input_1_add_948, input_1_add_948_fork_clone391],
         t: 1, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [20, 20, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 61}}
    conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_61, lc.input_tensor.conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_961.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.4.conv.weight_fork_clone4508],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_61, lc.input_tensor.conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_961.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.4.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_61, lc.input_tensor.conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_961.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.4.conv.weight_fork_clone4506],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    _fused_op_62: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_961.dc.conv2d.1.dc.matmul.11, conv2d_961.dc.conv2d.3.dc.matmul.11, conv2d_961.dc.conv2d.5.dc.matmul.11, input_1_add_962, input_1_add_962_fork_clone222],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_975.dc.concatenate.2: {type: splice, grid_loc: [1, 7], grid_size: [1, 1], inputs: [_fused_op_62, _fused_op_61],
         t: 1, mblock: [1, 7], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 357], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 5, 5]}}
    concatenate_975.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_975.dc.sparse_matmul.5.0, concatenate_975.dc.concatenate.2, lc.input_tensor.concatenate_975.dc.sparse_matmul.5.1],
         t: 1, mblock: [6, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 7}}
    conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_975.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_976.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.5.conv.weight_fork_clone4516],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 18}}
    conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_975.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_976.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.5.conv.weight],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 18}}
    conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_975.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_976.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.5.conv.weight_fork_clone4514],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 18}}
    _fused_op_63: {type: fused_op, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_976.dc.conv2d.1.dc.matmul.11, conv2d_976.dc.conv2d.3.dc.matmul.11, conv2d_976.dc.conv2d.5.dc.matmul.11, input_1_add_977, input_1_add_977_fork_clone397],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 63}}
    conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_63, lc.input_tensor.conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_990.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.6.conv.weight_fork_clone4524],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_63, lc.input_tensor.conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_990.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.6.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_63, lc.input_tensor.conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_990.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.6.conv.weight_fork_clone4522],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_64: {type: fused_op, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_990.dc.conv2d.1.dc.matmul.11, conv2d_990.dc.conv2d.3.dc.matmul.11, conv2d_990.dc.conv2d.5.dc.matmul.11, input_1_add_991, input_1_add_991_fork_clone227],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_0_17818_17150: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [_fused_op_63],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17816_18012: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [_fused_op_61],
         t: 1, mblock: [1, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [399], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17816_17150: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [buffer_0_17816_18012],
         t: 1, mblock: [1, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [399], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_1004.dc.concatenate.3: {type: splice, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_64, buffer_0_17818_17150, buffer_0_17816_17150, e2e_conv2d_889.dc.matmul.8_0],
         t: 1, mblock: [1, 25], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 77, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 49], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 3, 3], input2: [0, 5, 5], input3: [0, 15, 15]}}
    concatenate_1004.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1004.dc.sparse_matmul.6.0, concatenate_1004.dc.concatenate.3, lc.input_tensor.concatenate_1004.dc.sparse_matmul.6.1],
         t: 1, mblock: [3, 7], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 5}}
    conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1004.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1005.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 4], inputs: [conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.7.conv.weight_fork_clone4532],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 144}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1004.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1005.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 4], inputs: [conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.7.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 144}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1004.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1005.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [6, 1], grid_size: [1, 4], inputs: [conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.7.conv.weight_fork_clone4530],
         t: 7, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 144}, l1_acc: true, m_k: 18, min_buffer_input: 0, u_kt: 4}}
    _fused_op_65: {type: fused_op, grid_loc: [4, 7], grid_size: [1, 1], inputs: [conv2d_1005.dc.conv2d.1.dc.matmul.11, conv2d_1005.dc.conv2d.3.dc.matmul.11, conv2d_1005.dc.conv2d.5.dc.matmul.11, input_1_add_1006, input_1_add_1006_fork_clone403],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 65}}
    conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_65, lc.input_tensor.conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1019.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.8.conv.weight_fork_clone4540],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_65, lc.input_tensor.conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1019.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.8.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_65, lc.input_tensor.conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1019.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.8.conv.weight_fork_clone4538],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 4}}
    _fused_op_66: {type: fused_op, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_1019.dc.conv2d.1.dc.matmul.11, conv2d_1019.dc.conv2d.3.dc.matmul.11, conv2d_1019.dc.conv2d.5.dc.matmul.11, input_1_add_1020, input_1_add_1020_fork_clone232],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_0_17820_17234: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [_fused_op_65],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [352], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_1033.dc.concatenate.2: {type: splice, grid_loc: [6, 5], grid_size: [1, 1], inputs: [_fused_op_66, buffer_0_17820_17234],
         t: 1, mblock: [1, 10], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 315], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 8, 8]}}
    concatenate_1033.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1033.dc.sparse_matmul.5.0, concatenate_1033.dc.concatenate.2, lc.input_tensor.concatenate_1033.dc.sparse_matmul.5.1],
         t: 1, mblock: [9, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 4, u_kt: 5}}
    conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1033.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1034.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.9.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 81}, m_k: 1, min_buffer_input: 0, u_kt: 27}}
    conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1033.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1034.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.9.conv.weight_fork_clone4546],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 81}, m_k: 1, min_buffer_input: 0, u_kt: 27}}

  fwd_0_22_temporal_epoch_22:
    target_device: 0
    input_count: 1
    conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_1033.dc.sparse_matmul.5.lc2_0, lc.input_tensor.conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 3], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1034.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.9.conv.weight_fork_clone4548],
         t: 7, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 81}, m_k: 1, min_buffer_input: 0, u_kt: 27}}
    _fused_op_67: {type: fused_op, grid_loc: [0, 2], grid_size: [7, 1], inputs: [e2e_conv2d_1034.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_1034.dc.conv2d.3.dc.matmul.11_0, conv2d_1034.dc.conv2d.5.dc.matmul.11, input_1_add_1035, input_1_add_1035_fork_clone409],
         t: 1, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 59}}
    conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_67, lc.input_tensor.conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1048.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.10.conv.weight_fork_clone4556],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_67, lc.input_tensor.conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1048.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.10.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_67, lc.input_tensor.conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1048.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.10.conv.weight_fork_clone4554],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_68: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_1048.dc.conv2d.1.dc.matmul.11, conv2d_1048.dc.conv2d.3.dc.matmul.11, conv2d_1048.dc.conv2d.5.dc.matmul.11, input_1_add_1049, input_1_add_1049_fork_clone237],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_1135.dc.concatenate.0.dc.concatenate.6: {type: splice, grid_loc: [7, 6], grid_size: [1, 1], inputs: [e2e__fused_op_58_0, e2e__fused_op_60_0, e2e__fused_op_62_0, e2e__fused_op_64_0, e2e__fused_op_66_0, _fused_op_68],
         t: 1, mblock: [1, 12], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [7, 7, 7, 7, 7, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 2, 2], input2: [0, 2, 2], input3: [0, 2, 2], input4: [0, 2, 2], input5: [0, 2, 2]}}
    concatenate_1062.dc.concatenate.3: {type: splice, grid_loc: [1, 3], grid_size: [1, 1], inputs: [_fused_op_68, _fused_op_67, e2e__fused_op_65_0],
         t: 1, mblock: [1, 13], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 259, 0], input_dram_io_buf_size_tiles: [0, 0, 49], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 3, 3], input2: [0, 8, 8]}}
    concatenate_1062.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [4, 1], inputs: [lc.input_tensor.concatenate_1062.dc.sparse_matmul.6.0, concatenate_1062.dc.concatenate.3, lc.input_tensor.concatenate_1062.dc.sparse_matmul.6.1],
         t: 1, mblock: [3, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 13}}
    conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1062.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1063.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.11.conv.weight_fork_clone4564],
         t: 7, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 180}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 18}}
    conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1062.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1063.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.11.conv.weight],
         t: 7, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 180}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 18}}
    conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1062.dc.sparse_matmul.6.lc2, lc.input_tensor.conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [3, 2], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1063.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.11.conv.weight_fork_clone4562],
         t: 7, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 180}, l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 18}}
    _fused_op_69: {type: fused_op, grid_loc: [1, 7], grid_size: [7, 1], inputs: [conv2d_1063.dc.conv2d.1.dc.matmul.11, conv2d_1063.dc.conv2d.3.dc.matmul.11, conv2d_1063.dc.conv2d.5.dc.matmul.11, input_1_add_1064, input_1_add_1064_fork_clone415],
         t: 1, mblock: [1, 1], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [vstack: 7], input_1_tms: [vstack: 7], input_0_tms: [vstack: 7],
         attributes: {fused_op_id: 61}}
    conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_69, lc.input_tensor.conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1077.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.12.conv.weight_fork_clone4572],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_69, lc.input_tensor.conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1077.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.12.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_69, lc.input_tensor.conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1077.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.12.conv.weight_fork_clone4570],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 5}}
    _fused_op_70: {type: fused_op, grid_loc: [2, 3], grid_size: [1, 1], inputs: [conv2d_1077.dc.conv2d.1.dc.matmul.11, conv2d_1077.dc.conv2d.3.dc.matmul.11, conv2d_1077.dc.conv2d.5.dc.matmul.11, input_1_add_1078, input_1_add_1078_fork_clone242],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    concatenate_1091.dc.concatenate.2: {type: splice, grid_loc: [3, 3], grid_size: [1, 1], inputs: [_fused_op_70, _fused_op_69],
         t: 1, mblock: [1, 7], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 357], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 2, 2], input1: [0, 5, 5]}}
    concatenate_1091.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [4, 3], grid_size: [3, 1], inputs: [lc.input_tensor.concatenate_1091.dc.sparse_matmul.5.0, concatenate_1091.dc.concatenate.2, lc.input_tensor.concatenate_1091.dc.sparse_matmul.5.1],
         t: 1, mblock: [2, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 7}}
    conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1091.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1092.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.13.conv.weight_fork_clone4580],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 18}}
    conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1091.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1092.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.13.conv.weight],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 18}}
    conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1091.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 6], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1092.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.13.conv.weight_fork_clone4578],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 18}}
    _fused_op_71: {type: fused_op, grid_loc: [5, 6], grid_size: [1, 1], inputs: [conv2d_1092.dc.conv2d.1.dc.matmul.11, conv2d_1092.dc.conv2d.3.dc.matmul.11, conv2d_1092.dc.conv2d.5.dc.matmul.11, input_1_add_1093, input_1_add_1093_fork_clone421],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 63}}
    conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_71, lc.input_tensor.conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1106.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.14.conv.weight_fork_clone4588],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_71, lc.input_tensor.conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1106.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.14.conv.weight],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_71, lc.input_tensor.conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 7}}
    conv2d_1106.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.14.conv.weight_fork_clone4586],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 9}}
    _fused_op_72: {type: fused_op, grid_loc: [6, 6], grid_size: [1, 1], inputs: [conv2d_1106.dc.conv2d.1.dc.matmul.11, conv2d_1106.dc.conv2d.3.dc.matmul.11, conv2d_1106.dc.conv2d.5.dc.matmul.11, input_1_add_1107, input_1_add_1107_fork_clone247],
         t: 1, mblock: [1, 2], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 42}}
    buffer_0_17826_17496: {type: nop, grid_loc: [7, 2], grid_size: [1, 1], inputs: [_fused_op_71],
         t: 1, mblock: [1, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [427], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17824_18023: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [_fused_op_69],
         t: 1, mblock: [1, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [399], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_17824_17496: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [buffer_0_17824_18023],
         t: 1, mblock: [1, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [399], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_1120.dc.concatenate.4: {type: splice, grid_loc: [7, 5], grid_size: [1, 1], inputs: [_fused_op_72, buffer_0_17826_17496, buffer_0_17824_17496, e2e__fused_op_65_0, e2e_conv2d_889.dc.matmul.8_0],
         t: 1, mblock: [7, 33], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 24, 24], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 14, 14], input1: [0, 21, 21], input2: [0, 35, 35], input3: [0, 56, 56], input4: [0, 105, 105]}}

  fwd_0_23_temporal_epoch_23:
    target_device: 0
    input_count: 1
    concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.0, e2e_concatenate_1135.dc.concatenate.0.dc.concatenate.6_0, lc.input_tensor.concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.1],
         t: 1, mblock: [9, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 28, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 3, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 4, u_kt: 4}}
    concatenate_1120.dc.sparse_matmul.7.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1120.dc.sparse_matmul.7.0, e2e_concatenate_1120.dc.concatenate.4_0, lc.input_tensor.concatenate_1120.dc.sparse_matmul.7.1], overlay_size: 131072,
         t: 1, mblock: [31, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 33, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1120.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 31], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_1121.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [7, 1], inputs: [conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.15.conv.weight_fork_clone4596],
         t: 1, mblock: [1, 13], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 93, min_buffer_input: 0, u_kt: 1}}
    conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1120.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 31], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_1121.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.15.conv.weight],
         t: 1, mblock: [1, 13], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 93, min_buffer_input: 0, u_kt: 1}}
    conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1120.dc.sparse_matmul.7.lc2, lc.input_tensor.conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 31], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_1121.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.13.layers.15.conv.weight_fork_clone4594],
         t: 1, mblock: [1, 13], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 93, min_buffer_input: 0, u_kt: 1}}
    concatenate_1135.dc.concatenate.1.dc.concatenate.3_transpose_nop_39366: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.lc2],
         t: 1, mblock: [1, 9], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    _fused_op_73: {type: fused_op, grid_loc: [0, 7], grid_size: [1, 1], inputs: [conv2d_1121.dc.conv2d.1.dc.matmul.11, conv2d_1121.dc.conv2d.3.dc.matmul.11, conv2d_1121.dc.conv2d.5.dc.matmul.11, input_1_add_1122, input_1_add_1122_fork_clone252],
         t: 1, mblock: [1, 13], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 73}}
    concatenate_1135.dc.concatenate.1.dc.concatenate.3: {type: splice, grid_loc: [1, 2], grid_size: [1, 1], inputs: [concatenate_1135.dc.concatenate.1.dc.concatenate.3_transpose_nop_39366, e2e__fused_op_70_0, e2e__fused_op_72_0, _fused_op_73],
         t: 1, mblock: [1, 26], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 21, 21, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 9, 9], input1: [0, 2, 2], input2: [0, 2, 2], input3: [0, 13, 13]}}
    concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.lc2: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.0, concatenate_1135.dc.concatenate.1.dc.concatenate.3, lc.input_tensor.concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.1],
         t: 5, mblock: [5, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 13}}
    conv2d_1136.dc.matmul.8_transpose_nop_39487: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.lc2],
         t: 5, mblock: [1, 5], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    conv2d_1136.dc.matmul.8: {type: matmul, grid_loc: [2, 7], grid_size: [7, 1], inputs: [conv2d_1136.dc.matmul.8_transpose_nop_39487, base.14.conv.weight, input_1_add_1137_fork_clone118],
         t: 1, mblock: [1, 23], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 23}, m_k: 5, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 5}}
    max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.0, conv2d_1136.dc.matmul.8, lc.input_tensor.max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 23], ublock: [8, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 31, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    max_pool2d_1150.dc.reduce_max.6: {type: reduce, grid_loc: [7, 3], grid_size: [1, 1], inputs: [max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2],
         t: 1, mblock: [1, 23], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 4],
         attributes: {dim: z, type: max, z: 4}}
    conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, max_pool2d_1150.dc.reduce_max.6, lc.input_tensor.conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 23], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    conv2d_1151.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [8, 1], grid_size: [1, 4], inputs: [conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.0.conv.weight_fork_clone4628],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 23}}

  fwd_0_24_temporal_epoch_24:
    target_device: 0
    input_count: 1
    conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_max_pool2d_1150.dc.reduce_max.6_0, lc.input_tensor.conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 23], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 46, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    conv2d_1151.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 4], inputs: [conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.0.conv.weight],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 23}}
    conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_max_pool2d_1150.dc.reduce_max.6_0, lc.input_tensor.conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 23], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 46, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    conv2d_1151.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 1], grid_size: [1, 4], inputs: [conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.0.conv.weight_fork_clone4626],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 23}}
    _fused_op_74: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [conv2d_1151.dc.conv2d.1.dc.matmul.11, conv2d_1151.dc.conv2d.3.dc.matmul.11, e2e_conv2d_1151.dc.conv2d.5.dc.matmul.11_0, input_1_add_1152, input_1_add_1152_fork_clone58],
         t: 1, mblock: [2, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 74}}
    concatenate_1165.dc.concatenate.1: {type: splice, grid_loc: [0, 6], grid_size: [1, 1], inputs: [_fused_op_74, e2e_max_pool2d_1150.dc.reduce_max.6_0],
         t: 1, mblock: [1, 31], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 48], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 8, 8], input1: [0, 23, 23]}}
    concatenate_1165.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1165.dc.sparse_matmul.4.0, concatenate_1165.dc.concatenate.1, lc.input_tensor.concatenate_1165.dc.sparse_matmul.4.1],
         t: 1, mblock: [31, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 2, sparse_ublock_idx_bits: 5, u_kt: 31}}
    conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1165.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 31], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1166.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 7], inputs: [conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.1.conv.weight_fork_clone4636],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 31}}
    conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1165.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 31], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1166.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 1], grid_size: [1, 7], inputs: [conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.1.conv.weight],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 31}}
    conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1165.dc.sparse_matmul.4.lc2, lc.input_tensor.conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 31], ublock: [6, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1166.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 7], inputs: [conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.1.conv.weight_fork_clone4634],
         t: 1, mblock: [1, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 31}}
    _fused_op_75: {type: fused_op, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_1166.dc.conv2d.1.dc.matmul.11, conv2d_1166.dc.conv2d.3.dc.matmul.11, conv2d_1166.dc.conv2d.5.dc.matmul.11, input_1_add_1167, input_1_add_1167_fork_clone111],
         t: 1, mblock: [2, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 75}}
    conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_75, lc.input_tensor.conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1180.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.2.conv.weight_fork_clone4644],
         t: 1, mblock: [2, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 7}}
    conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_75, lc.input_tensor.conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1180.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.2.conv.weight],
         t: 1, mblock: [2, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 7}}
    conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, _fused_op_75, lc.input_tensor.conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1180.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.2.conv.weight_fork_clone4642],
         t: 1, mblock: [2, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 6, min_buffer_input: 0, u_kt: 7}}
    _fused_op_76: {type: fused_op, grid_loc: [5, 4], grid_size: [1, 1], inputs: [conv2d_1180.dc.conv2d.1.dc.matmul.11, conv2d_1180.dc.conv2d.3.dc.matmul.11, conv2d_1180.dc.conv2d.5.dc.matmul.11, input_1_add_1181, input_1_add_1181_fork_clone63],
         t: 1, mblock: [2, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {fused_op_id: 74}}
    concatenate_1194.dc.concatenate.2: {type: splice, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_76, _fused_op_75, e2e_max_pool2d_1150.dc.reduce_max.6_0],
         t: 1, mblock: [1, 45], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 280, 0], input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 8, 8], input1: [0, 14, 14], input2: [0, 23, 23]}}
    concatenate_1194.dc.sparse_matmul.5.lc2: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.concatenate_1194.dc.sparse_matmul.5.0, concatenate_1194.dc.concatenate.2, lc.input_tensor.concatenate_1194.dc.sparse_matmul.5.1],
         t: 1, mblock: [9, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 4, u_kt: 45}}
    conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1194.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [3, 9], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_1195.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.3.conv.weight],
         t: 2, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 405}, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 9}}
    conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, concatenate_1194.dc.sparse_matmul.5.lc2, lc.input_tensor.conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [3, 9], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    conv2d_1195.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.3.conv.weight_fork_clone4650],
         t: 2, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {kernel_broadcast: {input_1: 405}, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 9}}

  fwd_0_25_temporal_epoch_25:
    target_device: 0
    input_count: 1
    conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_concatenate_1194.dc.sparse_matmul.5.lc2_0, lc.input_tensor.conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 9], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 49, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 2}}
    conv2d_1195.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, base.16.layers.3.conv.weight_fork_clone4652],
         t: 1, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 9}}
    _fused_op_77: {type: fused_op, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_conv2d_1195.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_1195.dc.conv2d.3.dc.matmul.11_0, conv2d_1195.dc.conv2d.5.dc.matmul.11, input_1_add_1196, input_1_add_1196_fork_clone68],
         t: 1, mblock: [1, 3], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24, 0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [vstack: 2], input_0_tms: [vstack: 2],
         attributes: {fused_op_id: 77}}
    concatenate_1209.dc.concatenate.1: {type: splice, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e__fused_op_74_0, e2e__fused_op_76_0, _fused_op_77],
         t: 1, mblock: [1, 40], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 24, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {input0: [0, 8, 8], input1: [0, 8, 8], input2: [0, 24, 24]}}
    concatenate_1209.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [4, 1], inputs: [lc.input_tensor.concatenate_1209.dc.sparse_matmul.4.0, concatenate_1209.dc.concatenate.1, lc.input_tensor.concatenate_1209.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 2], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 10, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 2, sparse_ublock_idx_bits: 2, u_kt: 4}}
    conv2d_1211.dc.matmul.8_transpose_nop_39587: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [concatenate_1209.dc.sparse_matmul.4.lc2],
         t: 1, mblock: [1, 5], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    conv2d_1211.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [conv2d_1211.dc.matmul.8_transpose_nop_39587, base.18.conv.weight, input_1_add_1212_fork_clone20],
         t: 1, mblock: [1, 2], ublock: [1, 5], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}],
         attributes: {bias: true, kernel_broadcast: {input_2: 10}, m_k: 8, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 5}}
    avg_pool2d_1225.dc.reduce_avg.2.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.avg_pool2d_1225.dc.reduce_avg.2.0, conv2d_1211.dc.matmul.8],
         t: 1, mblock: [1, 5], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    matmul_1229: {type: matmul, grid_loc: [4, 2], grid_size: [1, 4], inputs: [avg_pool2d_1225.dc.reduce_avg.2.lc1, base.19.3.weight],
         t: 1, mblock: [1, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {l1_acc: true, m_k: 10, min_buffer_input: 0, u_kt: 4}}
    add_1230: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [matmul_1229, base.19.3.bias], untilize_output: true,
         t: 1, mblock: [1, 4], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q12_shadow: 0, $gptr_q10_shadow: 0, $gptr_q7_shadow: 0, $gptr_q2_shadow: 0, $gptr_q26_shadow: 0, $gptr_q28_shadow: 0, $gptr_q17: 0, $gptr_q15: 0, $gptr_q16: 0, $lptr_q16: 0, $gptr_q14: 0, $gptr_q12: 0, $gptr_q20: 0, $lptr_q8: 0, $lptr_q21: 0, $gptr_q3: 0, $lptr_q15: 0, $gptr_q23: 0, $lptr_q23: 0, $gptr_q9: 0, $lptr_q24: 0, $gptr_q25: 0, $gptr_q1: 0, $lptr_q19: 0, $gptr_q22: 0, $lptr_q1: 0, $lptr_q26: 0, $lptr_q33: 0, $lptr_q7: 0, $gptr_q19_shadow: 0, $gptr_q7: 0, $gptr_q21: 0, $lptr_q5: 0, $gptr_q32: 0, $gptr_q31: 0, $lptr_q31: 0, $gptr_q29: 0, $gptr_q33: 0, $lptr_q17: 0, $lptr_q14: 0, $gptr_q13: 0, $gptr_q27: 0, $lptr_q32: 0, $lptr_q25: 0, $gptr_q30: 0, $gptr_q28: 0, $lptr_q30: 0, $gptr_q5_shadow: 0, $gptr_q19: 0, $lptr_q20: 0, $gptr_q24: 0, $lptr_q18: 0, $gptr_q18: 0, $lptr_q29: 0, $lptr_q0: 0, $lptr_q3: 0, $lptr_q9: 0, $gptr_q26: 0, $lptr_q10: 0, $gptr_q0: 0, $lptr_q2: 0, $gptr_q2: 0, $lptr_q4: 0, $gptr_q4: 0, $gptr_q6: 0, $lptr_q6: 0, $gptr_q5: 0, $gptr_q10: 0, $gptr_q8: 0, $lptr_q28: 0, $lptr_q27: 0, $lptr_q11: 0, $gptr_q11: 0, $lptr_q13: 0, $gptr_q15_shadow: 0, $gptr_q23_shadow: 0, $lptr_q22: 0, $lptr_q12: 0}
    - loop: $p_loop_count
    -   varinst: [$gptr_q28, set, $gptr_q28_shadow]
    -   varinst: [$gptr_q26, set, $gptr_q26_shadow]
    -   varinst: [$gptr_q23, set, $gptr_q23_shadow]
    -   varinst: [$gptr_q19, set, $gptr_q19_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   varinst: [$gptr_q5, set, $gptr_q5_shadow]
    -   varinst: [$gptr_q7, set, $gptr_q7_shadow]
    -   varinst: [$gptr_q10, set, $gptr_q10_shadow]
    -   varinst: [$gptr_q12, set, $gptr_q12_shadow]
    -   varinst: [$gptr_q15, set, $gptr_q15_shadow]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.0.conv.weight_fork_clone4676: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1_fork_clone1886: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_14.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.1.conv.weight_fork_clone3935: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_14.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_14.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.1.conv.weight_fork_clone3933: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_15_fork_clone1830: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e__fused_op_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_28.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.0.conv.weight_fork_clone3954: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.0.conv.weight_fork_clone3952: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_30_fork_clone1718: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_43.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_43.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_44.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.1.conv.weight_fork_clone3962: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_44.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_44.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.1.conv.weight_fork_clone3960: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_45_fork_clone1811: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_58.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.2.conv.weight_fork_clone3970: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_58.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.2.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_58.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.2.conv.weight_fork_clone3968: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_59_fork_clone1723: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_72.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_72.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.3.conv.weight_fork_clone3978: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.3.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_73.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.3.conv.weight_fork_clone3976: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_74_fork_clone1817: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_87.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.4.conv.weight_fork_clone3986: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_87.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.4.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_87.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.4.conv.weight_fork_clone3984: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_88_fork_clone1728: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_101.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_101.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_102.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.5.conv.weight_fork_clone3994: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_102.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.5.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_102.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.5.conv.weight_fork_clone3992: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_103_fork_clone1823: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e_max_pool2d_28.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e__fused_op_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               lc.input_tensor.conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_116.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.6.conv.weight_fork_clone4002: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_116.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.6.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_116.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.6.conv.weight_fork_clone4000: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_117_fork_clone1733: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_130.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_130.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_131.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.7.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_131.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.7.conv.weight_fork_clone4008: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e__fused_op_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_concatenate_130.dc.sparse_matmul.6.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_conv2d_131.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_conv2d_131.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_131.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.3.layers.7.conv.weight_fork_clone4010: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_132_fork_clone1738: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_145.dc.sparse_matmul.8.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_145.dc.sparse_matmul.8.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.4.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_147_fork_clone1574: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_160.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_161.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.0.conv.weight_fork_clone4042: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_161.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_161.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.0.conv.weight_fork_clone4040: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e_max_pool2d_160.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_161.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_161.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_161.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_add_162: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_162_fork_clone1348: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_175.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_175.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_176.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.1.conv.weight_fork_clone4050: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_176.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_176.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.1.conv.weight_fork_clone4048: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_177: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_177_fork_clone1531: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_190.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.2.conv.weight_fork_clone4058: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_190.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.2.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_190.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.2.conv.weight_fork_clone4056: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_191: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_191_fork_clone1353: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_204.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_204.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_205.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.3.conv.weight_fork_clone4066: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_205.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.3.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_205.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.3.conv.weight_fork_clone4064: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_206: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_206_fork_clone1537: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_219.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.4.conv.weight_fork_clone4074: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_219.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.4.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_219.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.4.conv.weight_fork_clone4072: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_220: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_220_fork_clone1358: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_233.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_233.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_234.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.5.conv.weight_fork_clone4082: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_234.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.5.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_234.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.5.conv.weight_fork_clone4080: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_235: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_235_fork_clone1543: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_248.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.6.conv.weight_fork_clone4090: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_248.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.6.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_248.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.6.conv.weight_fork_clone4088: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_249: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_249_fork_clone1363: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_max_pool2d_160.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e__fused_op_13_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_16_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.concatenate_262.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_262.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_263.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.7.conv.weight_fork_clone4098: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_263.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.7.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_263.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.7.conv.weight_fork_clone4096: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_264: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_264_fork_clone1549: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_277.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.8.conv.weight_fork_clone4106: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_277.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.8.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_277.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.8.conv.weight_fork_clone4104: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_278: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_278_fork_clone1368: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_291.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_291.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_292.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.9.conv.weight_fork_clone4114: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_292.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.9.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_292.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.9.conv.weight_fork_clone4112: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_293: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_293_fork_clone1555: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_306.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.10.conv.weight_fork_clone4122: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_306.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.10.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_306.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.10.conv.weight_fork_clone4120: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_307: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_307_fork_clone1373: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_320.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_320.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_321.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.11.conv.weight_fork_clone4130: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_321.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.11.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_321.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.11.conv.weight_fork_clone4128: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_322: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_322_fork_clone1561: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_335.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.12.conv.weight_fork_clone4138: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_335.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.12.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_335.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.12.conv.weight_fork_clone4136: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_336: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_336_fork_clone1378: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
               e2e_max_pool2d_160.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_16_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_18_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_20_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e__fused_op_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_concatenate_349.dc.concatenate.2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               lc.input_tensor.concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_393.dc.concatenate.0.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_349.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_349.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_350.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.13.conv.weight_fork_clone4146: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_350.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.13.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_350.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.13.conv.weight_fork_clone4144: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_351: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_351_fork_clone1567: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_364.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.14.conv.weight_fork_clone4154: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_364.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.14.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_364.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.14.conv.weight_fork_clone4152: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_365: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_365_fork_clone1383: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_378.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_378.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_379.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.15.conv.weight_fork_clone4162: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_379.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.15.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_379.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.6.layers.15.conv.weight_fork_clone4160: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_380: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_380_fork_clone1388: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_393.dc.concatenate.1.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.7.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_395_fork_clone1146: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_408.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
               e2e_conv2d_394.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_conv2d_408.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               lc.input_tensor.conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_408.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.0.conv.weight_fork_clone4183: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_408.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.0.conv.weight_fork_clone4181: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_409: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_409_fork_clone979: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_422.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_422.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_423.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.1.conv.weight_fork_clone4191: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_423.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_423.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.1.conv.weight_fork_clone4189: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_424: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_424_fork_clone1152: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_437.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.2.conv.weight_fork_clone4199: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_437.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.2.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_437.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.2.conv.weight_fork_clone4197: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_438: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_438_fork_clone984: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_451.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_451.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_452.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.3.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_9_temporal_epoch_9, queue_settings: {
               e2e_conv2d_394.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_concatenate_451.dc.sparse_matmul.5.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_conv2d_452.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               lc.input_tensor.conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_452.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.3.conv.weight_fork_clone4207: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_452.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.3.conv.weight_fork_clone4205: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_453: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_453_fork_clone1158: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_466.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.4.conv.weight_fork_clone4215: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_466.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.4.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_466.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.4.conv.weight_fork_clone4213: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_467: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_467_fork_clone989: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_480.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_480.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_481.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.5.conv.weight_fork_clone4223: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_481.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.5.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_481.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.5.conv.weight_fork_clone4221: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_482: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_482_fork_clone1164: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_495.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.6.conv.weight_fork_clone4231: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_495.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.6.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_495.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.6.conv.weight_fork_clone4229: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_496: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_496_fork_clone994: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_509.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_509.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_510.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.7.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_10_temporal_epoch_10, queue_settings: {
               e2e_concatenate_509.dc.sparse_matmul.6.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_conv2d_510.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               lc.input_tensor.conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_510.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.7.conv.weight_fork_clone4239: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_510.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.7.conv.weight_fork_clone4237: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_511: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_511_fork_clone1170: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_524.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.8.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_11_temporal_epoch_11, queue_settings: {
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_conv2d_524.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               lc.input_tensor.conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_524.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.8.conv.weight_fork_clone4247: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_524.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.8.conv.weight_fork_clone4245: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_525: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_525_fork_clone999: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_538.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_538.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_539.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.9.conv.weight_fork_clone4255: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_539.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.9.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_539.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.9.conv.weight_fork_clone4253: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_540: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_540_fork_clone1176: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_553.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.10.conv.weight_fork_clone4263: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_553.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.10.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_553.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.10.conv.weight_fork_clone4261: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_554: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_554_fork_clone1004: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_567.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_567.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.11.conv.weight_fork_clone4271: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.11.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.11.conv.weight_fork_clone4269: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_569: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_569_fork_clone1182: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_582.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.12.conv.weight_fork_clone4279: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_582.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.12.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_582.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.12.conv.weight_fork_clone4277: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_583: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_583_fork_clone1009: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_596.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_596.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_597.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.13.conv.weight_fork_clone4287: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_597.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.13.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_597.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.13.conv.weight_fork_clone4285: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_598: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_598_fork_clone1188: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.14.conv.weight_fork_clone4295: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.14.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.14.conv.weight_fork_clone4293: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_612: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_612_fork_clone1014: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q15_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_12_temporal_epoch_12, queue_settings: {
               e2e_conv2d_394.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_28_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_30_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_32_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_33_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_34_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_36_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_37_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_39_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e__fused_op_40_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               lc.input_tensor.concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_640.dc.concatenate.0.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_625.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_625.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_626.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.15.conv.weight_fork_clone4303: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_626.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.15.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_626.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.8.layers.15.conv.weight_fork_clone4301: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_627: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_627_fork_clone1019: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_640.dc.concatenate.1.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.9.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_642_fork_clone807: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_13_temporal_epoch_13, queue_settings: {
               e2e_conv2d_641.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               lc.input_tensor.max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_655.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_656.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.0.conv.weight_fork_clone4335: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_656.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_656.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.0.conv.weight_fork_clone4333: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_657: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_657_fork_clone581: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_670.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_670.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.1.conv.weight_fork_clone4343: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_671.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.1.conv.weight_fork_clone4341: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_672: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_672_fork_clone764: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_685.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.2.conv.weight_fork_clone4351: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_685.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.2.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_685.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.2.conv.weight_fork_clone4349: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_686: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_686_fork_clone586: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_699.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_699.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_700.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.3.conv.weight_fork_clone4359: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_700.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.3.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_700.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.3.conv.weight_fork_clone4357: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_14_temporal_epoch_14, queue_settings: {
               e2e_max_pool2d_655.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_conv2d_700.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_conv2d_700.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_conv2d_700.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               input_1_add_701: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_701_fork_clone770: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_714.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.4.conv.weight_fork_clone4367: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_714.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.4.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_714.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.4.conv.weight_fork_clone4365: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_715: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_715_fork_clone591: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_728.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_728.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.5.conv.weight_fork_clone4375: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.5.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_729.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.5.conv.weight_fork_clone4373: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_730: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_730_fork_clone776: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_743.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.6.conv.weight_fork_clone4383: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_743.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.6.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_743.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.6.conv.weight_fork_clone4381: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_744: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_744_fork_clone596: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_757.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_757.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_758.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.7.conv.weight_fork_clone4391: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_758.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.7.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_758.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.7.conv.weight_fork_clone4389: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q19_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_15_temporal_epoch_15, queue_settings: {
               e2e_conv2d_758.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_conv2d_758.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_conv2d_758.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               input_1_add_759: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_759_fork_clone782: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_772.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.8.conv.weight_fork_clone4399: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_772.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.8.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_772.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.8.conv.weight_fork_clone4397: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_773: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_773_fork_clone601: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_786.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_786.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_787.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.9.conv.weight_fork_clone4407: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_787.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.9.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_787.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.9.conv.weight_fork_clone4405: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_788: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_788_fork_clone788: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_801.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.10.conv.weight_fork_clone4415: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_801.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.10.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_801.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.10.conv.weight_fork_clone4413: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_802: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_802_fork_clone606: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_815.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_815.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_816.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.11.conv.weight_fork_clone4423: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_816.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.11.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_816.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.11.conv.weight_fork_clone4421: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_16_temporal_epoch_16, queue_settings: {
               e2e_max_pool2d_655.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_42_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_46_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_48_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_49_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_50_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_52_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_conv2d_816.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_conv2d_816.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_conv2d_816.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               lc.input_tensor.concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_888.dc.concatenate.0.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_817: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_817_fork_clone794: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_830.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.12.conv.weight_fork_clone4431: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_830.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.12.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_830.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.12.conv.weight_fork_clone4429: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_831: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_831_fork_clone611: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_844.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_844.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_845.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.13.conv.weight_fork_clone4439: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_845.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.13.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_845.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.13.conv.weight_fork_clone4437: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_846: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_846_fork_clone800: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_859.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.14.conv.weight_fork_clone4447: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_859.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.14.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_859.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.14.conv.weight_fork_clone4445: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_860: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_860_fork_clone616: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_873.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_873.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_17_temporal_epoch_17, queue_settings: {
               e2e_concatenate_873.dc.sparse_matmul.7.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               lc.input_tensor.conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_874.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.15.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_874.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.15.conv.weight_fork_clone4453: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q23_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_18_temporal_epoch_18, queue_settings: {
               e2e__fused_op_54_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_56_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_concatenate_873.dc.sparse_matmul.7.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_conv2d_874.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_conv2d_874.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_concatenate_888.dc.concatenate.1.dc.concatenate.4_transpose_nop_38993_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               lc.input_tensor.conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_874.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.11.layers.15.conv.weight_fork_clone4455: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_875: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_875_fork_clone621: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_888.dc.concatenate.1.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q24, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_19_temporal_epoch_19, queue_settings: {
               e2e_conv2d_889.dc.matmul.8_transpose_nop_39114_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               base.12.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_890_fork_clone379: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_903.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_903.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.0.conv.weight_fork_clone4474: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_20_temporal_epoch_20, queue_settings: {
               e2e_conv2d_889.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_conv2d_903.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_conv2d_903.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               lc.input_tensor.conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_903.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.0.conv.weight_fork_clone4476: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_904: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_904_fork_clone212: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_917.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_917.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_918.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.1.conv.weight_fork_clone4484: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_918.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_918.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.1.conv.weight_fork_clone4482: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_919: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_919_fork_clone385: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_932.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.2.conv.weight_fork_clone4492: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_932.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.2.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_932.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.2.conv.weight_fork_clone4490: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_933: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_933_fork_clone217: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_946.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_946.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_947.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.3.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_947.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.3.conv.weight_fork_clone4498: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q26_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_21_temporal_epoch_21, queue_settings: {
               e2e_conv2d_889.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_concatenate_946.dc.sparse_matmul.5.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_conv2d_947.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_conv2d_947.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               lc.input_tensor.conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_947.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.3.conv.weight_fork_clone4500: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_948: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_948_fork_clone391: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_961.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.4.conv.weight_fork_clone4508: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_961.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.4.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_961.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.4.conv.weight_fork_clone4506: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_962: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_962_fork_clone222: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_975.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_975.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_976.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.5.conv.weight_fork_clone4516: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_976.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.5.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_976.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.5.conv.weight_fork_clone4514: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_977: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_977_fork_clone397: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_990.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.6.conv.weight_fork_clone4524: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_990.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.6.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_990.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.6.conv.weight_fork_clone4522: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_991: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_991_fork_clone227: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1004.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1004.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1005.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.7.conv.weight_fork_clone4532: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1005.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.7.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1005.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.7.conv.weight_fork_clone4530: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1006: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1006_fork_clone403: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1019.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.8.conv.weight_fork_clone4540: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1019.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.8.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1019.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.8.conv.weight_fork_clone4538: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1020: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1020_fork_clone232: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1033.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1033.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1034.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.9.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1034.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.9.conv.weight_fork_clone4546: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q28_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_22_temporal_epoch_22, queue_settings: {
               e2e_conv2d_889.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_58_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_60_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_64_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_65_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_66_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_concatenate_1033.dc.sparse_matmul.5.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_conv2d_1034.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_conv2d_1034.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               lc.input_tensor.conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1034.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.9.conv.weight_fork_clone4548: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1035: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1035_fork_clone409: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1048.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.10.conv.weight_fork_clone4556: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1048.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.10.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1048.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.10.conv.weight_fork_clone4554: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1049: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1049_fork_clone237: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1062.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1062.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1063.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.11.conv.weight_fork_clone4564: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1063.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.11.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1063.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.11.conv.weight_fork_clone4562: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1064: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1064_fork_clone415: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1077.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.12.conv.weight_fork_clone4572: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1077.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.12.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1077.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.12.conv.weight_fork_clone4570: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1078: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1078_fork_clone242: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1091.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1091.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1092.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.13.conv.weight_fork_clone4580: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1092.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.13.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1092.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.13.conv.weight_fork_clone4578: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1093: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1093_fork_clone421: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1106.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.14.conv.weight_fork_clone4588: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1106.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.14.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1106.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.14.conv.weight_fork_clone4586: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1107: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1107_fork_clone247: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q30, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_23_temporal_epoch_23, queue_settings: {
               e2e_concatenate_1135.dc.concatenate.0.dc.concatenate.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e__fused_op_70_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e__fused_op_72_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_concatenate_1120.dc.concatenate.4_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               lc.input_tensor.concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1135.dc.concatenate.0.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1120.dc.sparse_matmul.7.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1120.dc.sparse_matmul.7.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1121.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.15.conv.weight_fork_clone4596: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1121.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.15.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1121.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.13.layers.15.conv.weight_fork_clone4594: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1122_fork_clone252: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1135.dc.concatenate.1.dc.sparse_matmul.6.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.14.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1137_fork_clone118: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_1150.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1151.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.0.conv.weight_fork_clone4628: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_24_temporal_epoch_24, queue_settings: {
               e2e_max_pool2d_1150.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_conv2d_1151.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               lc.input_tensor.conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1151.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.0.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1151.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.0.conv.weight_fork_clone4626: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1152: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1152_fork_clone58: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1165.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1165.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1166.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.1.conv.weight_fork_clone4636: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1166.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.1.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1166.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.1.conv.weight_fork_clone4634: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1167: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1167_fork_clone111: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1180.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.2.conv.weight_fork_clone4644: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1180.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.2.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1180.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.2.conv.weight_fork_clone4642: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1181: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1181_fork_clone63: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1194.dc.sparse_matmul.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1194.dc.sparse_matmul.5.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1195.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.3.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1195.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.3.conv.weight_fork_clone4650: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q32, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_25_temporal_epoch_25, queue_settings: {
               e2e__fused_op_74_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e__fused_op_76_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_concatenate_1194.dc.sparse_matmul.5.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_conv2d_1195.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_conv2d_1195.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               lc.input_tensor.conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_1195.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.16.layers.3.conv.weight_fork_clone4652: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1196: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1196_fork_clone68: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1209.dc.sparse_matmul.4.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_1209.dc.sparse_matmul.4.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.18.conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1212_fork_clone20: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.avg_pool2d_1225.dc.reduce_avg.2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               base.19.3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               base.19.3.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 2]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - conv2d_0.dc.conv2d.3.dc.add.4.0: { type: add, inputs: [input0, input1], mblock: [28, 1], ublock: [7, 1], output: dest}
        - add_12.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [28, 1], ublock: [7, 1], output: output}
  1: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_14.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [2, 3], output: dest}
        - conv2d_14.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [2, 3], output: dest}
        - add_26.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [2, 3], output: output}
  2: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_29.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [7, 1], output: dest}
        - conv2d_29.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [7, 1], output: dest}
        - add_41.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [7, 1], output: output}
  3: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_44.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [14, 2], ublock: [7, 1], output: dest}
        - conv2d_44.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [14, 2], ublock: [7, 1], output: dest}
        - add_56.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [14, 2], ublock: [7, 1], output: output}
  5: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_73.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [14, 1], ublock: [7, 1], output: dest}
        - conv2d_73.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [14, 1], ublock: [7, 1], output: dest}
        - add_85.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [14, 1], ublock: [7, 1], output: output}
  6: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_87.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [7, 1], output: dest}
        - conv2d_87.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 1], ublock: [7, 1], output: dest}
        - add_99.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 1], ublock: [7, 1], output: output}
  9: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - conv2d_131.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 4], output: dest}
        - conv2d_131.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 4], output: dest}
        - add_143.0: { type: add, inputs: [dest, input3], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 4], output: output}
  10: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_161.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 1], ublock: [5, 1], output: dest}
        - conv2d_161.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 1], ublock: [5, 1], output: dest}
        - multiply_168.0: { type: multiply, inputs: [dest, input3], mblock: [5, 1], ublock: [5, 1], output: dest}
        - add_173.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [5, 1], ublock: [5, 1], output: output}
  11: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_176.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 2], ublock: [5, 1], output: dest}
        - conv2d_176.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 2], ublock: [5, 1], output: dest}
        - multiply_183.0: { type: multiply, inputs: [dest, input3], mblock: [5, 2], ublock: [5, 1], output: dest}
        - add_188.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [5, 2], ublock: [5, 1], output: output}
  13: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_205.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 3], ublock: [5, 1], output: dest}
        - conv2d_205.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 3], ublock: [5, 1], output: dest}
        - multiply_212.0: { type: multiply, inputs: [dest, input3], mblock: [5, 3], ublock: [5, 1], output: dest}
        - add_217.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [5, 3], ublock: [5, 1], output: output}
  17: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_263.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 4], ublock: [5, 1], output: dest}
        - conv2d_263.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 4], ublock: [5, 1], output: dest}
        - multiply_270.0: { type: multiply, inputs: [dest, input3], mblock: [5, 4], ublock: [5, 1], output: dest}
        - add_275.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [5, 4], ublock: [5, 1], output: output}
  18: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_277.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [5, 1], output: dest}
        - conv2d_277.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [5, 1], output: dest}
        - multiply_284.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [5, 1], output: dest}
        - add_289.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [5, 1], output: output}
  25: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_379.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [25, 1], ublock: [1, 7], output: dest}
        - conv2d_379.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [25, 1], ublock: [1, 7], output: dest}
        - multiply_386.0: { type: multiply, inputs: [dest, input3], mblock: [25, 1], ublock: [1, 7], output: dest}
        - add_391.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [25, 1], ublock: [1, 7], output: output}
  33: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_510.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 1], ublock: [1, 5], output: dest}
        - conv2d_510.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 1], ublock: [1, 5], output: dest}
        - multiply_517.0: { type: multiply, inputs: [dest, input3], mblock: [5, 1], ublock: [1, 5], output: dest}
        - add_522.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [5, 1], ublock: [1, 5], output: output}
  41: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_626.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [5, 1], ublock: [1, 8], output: dest}
        - conv2d_626.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [5, 1], ublock: [1, 8], output: dest}
        - multiply_633.0: { type: multiply, inputs: [dest, input3], mblock: [5, 1], ublock: [1, 8], output: dest}
        - add_638.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [5, 1], ublock: [1, 8], output: output}
  42: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_656.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 2], ublock: [7, 1], output: dest}
        - conv2d_656.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 2], ublock: [7, 1], output: dest}
        - multiply_663.0: { type: multiply, inputs: [dest, input3], mblock: [1, 2], ublock: [7, 1], output: dest}
        - add_668.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 2], ublock: [7, 1], output: output}
  43: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_671.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 2], output: dest}
        - conv2d_671.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 2], output: dest}
        - multiply_678.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 2], output: dest}
        - add_683.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 2], output: output}
  45: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_700.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 4], ublock: [7, 1], output: dest}
        - conv2d_700.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 4], ublock: [7, 1], output: dest}
        - multiply_707.0: { type: multiply, inputs: [dest, input3], mblock: [1, 4], ublock: [7, 1], output: dest}
        - add_712.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 4], ublock: [7, 1], output: output}
  49: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_758.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 6], ublock: [7, 1], output: dest}
        - conv2d_758.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 6], ublock: [7, 1], output: dest}
        - multiply_765.0: { type: multiply, inputs: [dest, input3], mblock: [1, 6], ublock: [7, 1], output: dest}
        - add_770.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 6], ublock: [7, 1], output: output}
  57: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_874.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 2], ublock: [1, 5], output: dest}
        - conv2d_874.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 2], ublock: [1, 5], output: dest}
        - multiply_881.0: { type: multiply, inputs: [dest, input3], mblock: [1, 2], ublock: [1, 5], output: dest}
        - add_886.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 2], ublock: [1, 5], output: output}
  59: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_918.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 3], output: dest}
        - conv2d_918.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 3], output: dest}
        - multiply_925.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 3], output: dest}
        - add_930.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 3], output: output}
  61: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_947.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 5], output: dest}
        - conv2d_947.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 5], output: dest}
        - multiply_954.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 5], output: dest}
        - add_959.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 5], output: output}
  63: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_976.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 3], ublock: [7, 1], output: dest}
        - conv2d_976.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 3], ublock: [7, 1], output: dest}
        - multiply_983.0: { type: multiply, inputs: [dest, input3], mblock: [1, 3], ublock: [7, 1], output: dest}
        - add_988.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 3], ublock: [7, 1], output: output}
  65: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_1005.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [1, 8], output: dest}
        - conv2d_1005.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [7, 1], ublock: [1, 8], output: dest}
        - multiply_1012.0: { type: multiply, inputs: [dest, input3], mblock: [7, 1], ublock: [1, 8], output: dest}
        - add_1017.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 1], ublock: [1, 8], output: output}
  73: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_1121.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 13], ublock: [7, 1], output: dest}
        - conv2d_1121.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 13], ublock: [7, 1], output: dest}
        - multiply_1128.0: { type: multiply, inputs: [dest, input3], mblock: [1, 13], ublock: [7, 1], output: dest}
        - add_1133.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 13], ublock: [7, 1], output: output}
  74: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_1151.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [2, 1], ublock: [1, 8], output: dest}
        - conv2d_1151.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [2, 1], ublock: [1, 8], output: dest}
        - multiply_1158.0: { type: multiply, inputs: [dest, input3], mblock: [2, 1], ublock: [1, 8], output: dest}
        - add_1163.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [2, 1], ublock: [1, 8], output: output}
  75: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_1166.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [2, 2], ublock: [1, 7], output: dest}
        - conv2d_1166.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [2, 2], ublock: [1, 7], output: dest}
        - multiply_1173.0: { type: multiply, inputs: [dest, input3], mblock: [2, 2], ublock: [1, 7], output: dest}
        - add_1178.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [2, 2], ublock: [1, 7], output: output}
  77: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_1195.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 3], ublock: [1, 8], output: dest}
        - conv2d_1195.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 3], ublock: [1, 8], output: dest}
        - multiply_1202.0: { type: multiply, inputs: [dest, input3], mblock: [1, 3], ublock: [1, 8], output: dest}
        - add_1207.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 3], ublock: [1, 8], output: output}
